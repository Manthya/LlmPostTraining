{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f6164c",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "613f6335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install transformers datasets accelerate peft bitsandbytes trl wandb pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "111cf3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.2\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31c3f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSL Certificate: /Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/certifi/cacert.pem\n",
      "SSL configured successfully\n"
     ]
    }
   ],
   "source": [
    "# Fix SSL certificate issues for HuggingFace downloads\n",
    "import os\n",
    "import ssl\n",
    "import certifi\n",
    "\n",
    "# Set SSL certificate path\n",
    "os.environ['SSL_CERT_FILE'] = certifi.where()\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = certifi.where()\n",
    "os.environ['CURL_CA_BUNDLE'] = certifi.where()\n",
    "\n",
    "# Verify SSL setup\n",
    "print(f\"SSL Certificate: {certifi.where()}\")\n",
    "\n",
    "# Alternative: Disable SSL verification for HuggingFace (use with caution)\n",
    "# import huggingface_hub\n",
    "# huggingface_hub.constants.HF_HUB_DISABLE_SSL_VERIFY = True\n",
    "print(\"SSL configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7875c52",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è SSL Certificate Issue Detected\n",
    "\n",
    "If you're behind a corporate proxy/firewall, you may encounter SSL certificate errors.\n",
    "\n",
    "**Solution Options:**\n",
    "\n",
    "1. **Disable SSL verification (Quick fix for testing)**:\n",
    "   ```python\n",
    "   import ssl\n",
    "   import os\n",
    "   ssl._create_default_https_context = ssl._create_unverified_context\n",
    "   os.environ['CURL_CA_BUNDLE'] = ''\n",
    "   ```\n",
    "\n",
    "2. **Use huggingface-cli to download** (in terminal):\n",
    "   ```bash\n",
    "   huggingface-cli download Qwen/Qwen2.5-1.5B --local-dir ./models/Qwen2.5-1.5B\n",
    "   ```\n",
    "\n",
    "3. **Manual download**: Download from https://huggingface.co/Qwen/Qwen2.5-1.5B\n",
    "\n",
    "Uncomment the cell below to disable SSL verification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16fe1233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì SSL verification disabled\n"
     ]
    }
   ],
   "source": [
    "# WORKAROUND: Disable SSL verification for HuggingFace downloads\n",
    "# ‚ö†Ô∏è Use with caution - only for corporate proxy/firewall issues\n",
    "\n",
    "import ssl\n",
    "import os\n",
    "\n",
    "# Disable SSL verification\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = ''\n",
    "\n",
    "# Disable httpx SSL verification for huggingface_hub\n",
    "import httpx\n",
    "from unittest.mock import patch\n",
    "\n",
    "# Patch httpx Client to disable SSL verification\n",
    "original_client_init = httpx.Client.__init__\n",
    "\n",
    "def patched_client_init(self, *args, **kwargs):\n",
    "    kwargs['verify'] = False\n",
    "    return original_client_init(self, *args, **kwargs)\n",
    "\n",
    "httpx.Client.__init__ = patched_client_init\n",
    "\n",
    "print(\"‚úì SSL verification disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f199957",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cee41dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  model_name: ../models/gpt2\n",
      "  max_length: 512\n",
      "  train_split: 0.9\n",
      "  batch_size: 4\n",
      "  gradient_accumulation_steps: 4\n",
      "  num_epochs: 3\n",
      "  learning_rate: 2e-05\n",
      "  warmup_ratio: 0.1\n",
      "  weight_decay: 0.01\n",
      "  output_dir: ../outputs/stage1_sft\n"
     ]
    }
   ],
   "source": [
    "# Configuration for Stage 1\n",
    "CONFIG = {\n",
    "    # Model - Using local GPT-2\n",
    "    \"model_name\": \"../models/gpt2\",  # Local download due to network issues\n",
    "    # \"model_name\": \"gpt2\",  # Use this if HuggingFace access works\n",
    "    # \"model_name\": \"Qwen/Qwen2.5-1.5B\",  # Alternative if you have better network access\n",
    "    \n",
    "    # Data\n",
    "    \"max_length\": 512,\n",
    "    \"train_split\": 0.9,\n",
    "    \n",
    "    # Training\n",
    "    \"batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"num_epochs\": 3,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \n",
    "    # Output\n",
    "    \"output_dir\": \"../outputs/stage1_sft\",\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb6f214",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load Base Model\n",
    "\n",
    "We're using **Qwen2.5-1.5B** as our base model because:\n",
    "- It's a **base model** (not instruction-tuned)\n",
    "- Small enough for experiments\n",
    "- Good architecture for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28b31d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: ../models/gpt2\n",
      "Vocab size: 50257\n",
      "Pad token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Set pad token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Tokenizer loaded: {CONFIG['model_name']}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c91794c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 148/148 [00:01<00:00, 101.40it/s, Materializing param=transformer.wte.weight]             \n",
      "GPT2LMHeadModel LOAD REPORT from: ../models/gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: ../models/gpt2\n",
      "Parameters: 124,439,808\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    torch_dtype=torch.bfloat16,  # Use float16 for older GPUs\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {CONFIG['model_name']}\")\n",
    "print(f\"Parameters: {model.num_parameters():,}\")\n",
    "print(f\"Device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab8fa8",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Test Base Model (Before Training)\n",
    "\n",
    "Let's see how the **untrained base model** responds to our test queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2589fe5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BASE MODEL RESPONSES (Before Training)\n",
      "============================================================\n",
      "\n",
      "Prompt: What is the capital of France?...\n",
      "Response: I don't know, but it is in the north of France. It is not far from the capital of the United States, but it is in the south of France. It is in the west of France. It is in the east of France. It is i...\n",
      "----------------------------------------\n",
      "\n",
      "Prompt: Translate 'Hello' to Spanish:...\n",
      "Response: \"Hello, dear.\"\n",
      "\n",
      "Translation: \"Hello, dear.\"\n",
      "\n",
      "Translation: \"Hello, dear.\"\n",
      "\n",
      "Translation: \"Hello, dear.\"\n",
      "\n",
      "Translation: \"Hello, dear.\"\n",
      "\n",
      "Translation: \"Hello, dear.\"\n",
      "\n",
      "Translation: \"Hello, dear.\"\n",
      "\n",
      "Translatio...\n",
      "----------------------------------------\n",
      "\n",
      "Prompt: What is 2 + 2?...\n",
      "Response: 2 = 0. The word \"2\" is used to mean 2 + 2, and is used to mean 1 + 1.\n",
      "\n",
      "2 + 2 = 0.\n",
      "\n",
      "Example:\n",
      "\n",
      "\"2\" = 1\n",
      "\n",
      "\"2\" = 0\n",
      "\n",
      "\"2\" = 0\"\n",
      "\n",
      "\"2\" = 0\"\n",
      "\n",
      "\"2\" = 0\"\n",
      "\n",
      "\"2\" = 0\"\n",
      "\n",
      "\"2\" = 0\"\n",
      "\n",
      "\"2\" = 0\"\n",
      "\n",
      "\"2\" = 0\"\n",
      "\n",
      "\"2\" = 0\"\n",
      "\n",
      "\"2\" = 0\"\n",
      "...\n",
      "----------------------------------------\n",
      "\n",
      "Prompt: Write a haiku about spring:...\n",
      "Response: Yes, that's right. That's how spring is made. The spring that is in the spring, which is the spring that happens every year, is called spring-alchemy. And it is a very important principle, because it'...\n",
      "----------------------------------------\n",
      "\n",
      "Prompt: Who wrote Romeo and Juliet?...\n",
      "Response: He wrote Romeo and Juliet.\n",
      "\n",
      "Question: What is the meaning of the phrase \"A man's life depends on his wife\"?\n",
      "\n",
      "Answer: The meaning of the phrase \"A man's life depends on his wife\" is \"If he does not get...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_new_tokens=128):\n",
    "    \"\"\"Generate response from model.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test prompts (Stage 1 format - simple prompt)\n",
    "test_prompts = [\n",
    "    \"What is the capital of France?\\n\\nAnswer:\",\n",
    "    \"Translate 'Hello' to Spanish:\\n\\nAnswer:\",\n",
    "    \"What is 2 + 2?\\n\\nAnswer:\",\n",
    "    \"Write a haiku about spring:\\n\\nAnswer:\",\n",
    "    \"Who wrote Romeo and Juliet?\\n\\nAnswer:\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASE MODEL RESPONSES (Before Training)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "base_model_responses = {}\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt.split(chr(10))[0]}...\")\n",
    "    response = generate_response(model, tokenizer, prompt)\n",
    "    # Extract just the answer part\n",
    "    answer = response[len(prompt):].strip() if response.startswith(prompt) else response\n",
    "    print(f\"Response: {answer[:200]}...\" if len(answer) > 200 else f\"Response: {answer}\")\n",
    "    base_model_responses[prompt] = answer\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44318da2",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Prepare Training Data\n",
    "\n",
    "For Stage 1, we use simple **prompt ‚Üí output** format.\n",
    "No chat structure, no system messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9437977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 25\n",
      "\n",
      "Sample entry:\n",
      "{'prompt': 'What is the capital of France?', 'output': 'The capital of France is Paris.'}\n"
     ]
    }
   ],
   "source": [
    "# Sample training data for Stage 1\n",
    "# In production, use a larger dataset\n",
    "\n",
    "TRAINING_DATA = [\n",
    "    # Q&A pairs\n",
    "    {\"prompt\": \"What is the capital of France?\", \"output\": \"The capital of France is Paris.\"},\n",
    "    {\"prompt\": \"What is the capital of Japan?\", \"output\": \"The capital of Japan is Tokyo.\"},\n",
    "    {\"prompt\": \"What is the capital of Germany?\", \"output\": \"The capital of Germany is Berlin.\"},\n",
    "    {\"prompt\": \"What is the capital of Italy?\", \"output\": \"The capital of Italy is Rome.\"},\n",
    "    {\"prompt\": \"What is the capital of Spain?\", \"output\": \"The capital of Spain is Madrid.\"},\n",
    "    \n",
    "    # Math\n",
    "    {\"prompt\": \"What is 2 + 2?\", \"output\": \"2 + 2 equals 4.\"},\n",
    "    {\"prompt\": \"What is 5 * 3?\", \"output\": \"5 * 3 equals 15.\"},\n",
    "    {\"prompt\": \"What is 10 / 2?\", \"output\": \"10 / 2 equals 5.\"},\n",
    "    {\"prompt\": \"What is 7 - 3?\", \"output\": \"7 - 3 equals 4.\"},\n",
    "    {\"prompt\": \"What is 100 + 50?\", \"output\": \"100 + 50 equals 150.\"},\n",
    "    \n",
    "    # Translation\n",
    "    {\"prompt\": \"Translate 'Hello' to Spanish:\", \"output\": \"Hola\"},\n",
    "    {\"prompt\": \"Translate 'Goodbye' to French:\", \"output\": \"Au revoir\"},\n",
    "    {\"prompt\": \"Translate 'Thank you' to German:\", \"output\": \"Danke\"},\n",
    "    {\"prompt\": \"Translate 'Good morning' to Italian:\", \"output\": \"Buongiorno\"},\n",
    "    {\"prompt\": \"Translate 'Yes' to Japanese:\", \"output\": \"„ÅØ„ÅÑ (Hai)\"},\n",
    "    \n",
    "    # General knowledge\n",
    "    {\"prompt\": \"Who wrote Romeo and Juliet?\", \"output\": \"William Shakespeare wrote Romeo and Juliet.\"},\n",
    "    {\"prompt\": \"What is the chemical symbol for water?\", \"output\": \"The chemical symbol for water is H2O.\"},\n",
    "    {\"prompt\": \"What is the largest planet in our solar system?\", \"output\": \"Jupiter is the largest planet in our solar system.\"},\n",
    "    {\"prompt\": \"What is the speed of light?\", \"output\": \"The speed of light is approximately 299,792 kilometers per second.\"},\n",
    "    {\"prompt\": \"Who painted the Mona Lisa?\", \"output\": \"Leonardo da Vinci painted the Mona Lisa.\"},\n",
    "    \n",
    "    # Simple tasks\n",
    "    {\"prompt\": \"Count from 1 to 5:\", \"output\": \"1, 2, 3, 4, 5\"},\n",
    "    {\"prompt\": \"List the primary colors:\", \"output\": \"The primary colors are red, blue, and yellow.\"},\n",
    "    {\"prompt\": \"What comes after Monday?\", \"output\": \"Tuesday comes after Monday.\"},\n",
    "    {\"prompt\": \"What is the opposite of 'hot'?\", \"output\": \"The opposite of 'hot' is 'cold'.\"},\n",
    "    {\"prompt\": \"Name a fruit that is red:\", \"output\": \"An apple is a red fruit.\"},\n",
    "]\n",
    "\n",
    "print(f\"Training samples: {len(TRAINING_DATA)}\")\n",
    "print(f\"\\nSample entry:\")\n",
    "print(TRAINING_DATA[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "800e7f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted sample:\n",
      "----------------------------------------\n",
      "What is the capital of France?\n",
      "\n",
      "Answer: The capital of France is Paris.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def format_for_stage1(sample):\n",
    "    \"\"\"Format sample for Stage 1 training (simple prompt ‚Üí output).\"\"\"\n",
    "    prompt = sample[\"prompt\"]\n",
    "    output = sample[\"output\"]\n",
    "    \n",
    "    # Simple format: prompt followed by output\n",
    "    return f\"{prompt}\\n\\nAnswer: {output}\"\n",
    "\n",
    "# Format all samples\n",
    "formatted_texts = [format_for_stage1(s) for s in TRAINING_DATA]\n",
    "\n",
    "print(\"Formatted sample:\")\n",
    "print(\"-\" * 40)\n",
    "print(formatted_texts[0])\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e56758f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 22\n",
      "Eval samples: 3\n"
     ]
    }
   ],
   "source": [
    "# Create HuggingFace Dataset\n",
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "# Split into train/eval\n",
    "random.seed(42)\n",
    "shuffled = formatted_texts.copy()\n",
    "random.shuffle(shuffled)\n",
    "\n",
    "split_idx = int(len(shuffled) * CONFIG[\"train_split\"])\n",
    "train_texts = shuffled[:split_idx]\n",
    "eval_texts = shuffled[split_idx:]\n",
    "\n",
    "print(f\"Train samples: {len(train_texts)}\")\n",
    "print(f\"Eval samples: {len(eval_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd68ea7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input shape: torch.Size([22, 512])\n",
      "Eval input shape: torch.Size([3, 512])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize data\n",
    "def tokenize_texts(texts, tokenizer, max_length):\n",
    "    \"\"\"Tokenize list of texts.\"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    # For causal LM, labels = input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "train_tokenized = tokenize_texts(train_texts, tokenizer, CONFIG[\"max_length\"])\n",
    "eval_tokenized = tokenize_texts(eval_texts, tokenizer, CONFIG[\"max_length\"])\n",
    "\n",
    "print(f\"Train input shape: {train_tokenized['input_ids'].shape}\")\n",
    "print(f\"Eval input shape: {eval_tokenized['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "150252c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 22\n",
      "})\n",
      "Eval dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset objects\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": train_tokenized[\"input_ids\"].tolist(),\n",
    "    \"attention_mask\": train_tokenized[\"attention_mask\"].tolist(),\n",
    "    \"labels\": train_tokenized[\"labels\"].tolist(),\n",
    "})\n",
    "\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": eval_tokenized[\"input_ids\"].tolist(),\n",
    "    \"attention_mask\": eval_tokenized[\"attention_mask\"].tolist(),\n",
    "    \"labels\": eval_tokenized[\"labels\"].tolist(),\n",
    "})\n",
    "\n",
    "print(f\"Train dataset: {train_dataset}\")\n",
    "print(f\"Eval dataset: {eval_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff248535",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Setup Training\n",
    "\n",
    "We'll use HuggingFace Trainer for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29b1b2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured.\n",
      "Training on: CPU\n"
     ]
    }
   ],
   "source": [
    "# Enable gradient checkpointing to save memory\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    \n",
    "    # Batch size\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    \n",
    "    # Training\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=5,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Mixed precision - disabled for CPU training\n",
    "    use_cpu=True,  # CPU training\n",
    "    # bf16=True,  # Uncomment for GPU with bfloat16 support\n",
    "    # fp16=True,  # Uncomment for older GPUs\n",
    "    \n",
    "    # Optimization\n",
    "    optim=\"adamw_torch\",\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # Best model\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Reporting\n",
    "    report_to=\"none\",  # Set to \"wandb\" for experiment tracking\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured.\")\n",
    "print(f\"Training on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67b5ecc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized.\n"
     ]
    }
   ],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM, not masked LM\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f1d245",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Train the Model\n",
    "\n",
    "üöÄ **Let's train!**\n",
    "\n",
    "Watch for:\n",
    "- ‚úÖ Loss decreasing smoothly\n",
    "- ‚úÖ No sudden spikes\n",
    "- ‚ùå If loss plateaus early, may need more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e313a135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 08:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training complete!\n",
      "Total steps: 6\n",
      "Training loss: 3.1827\n"
     ]
    }
   ],
   "source": [
    "# Train!\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Training complete!\")\n",
    "print(f\"Total steps: {train_result.global_step}\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a39679b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m eval_results = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEval loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_results[\u001b[33m'\u001b[39m\u001b[33meval_loss\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/transformers/trainer.py:4289\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4285\u001b[39m     eval_dataloader = tpu_spmd_dataloader(eval_dataloader)\n\u001b[32m   4287\u001b[39m start_time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m4289\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluation_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4290\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4292\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   4293\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   4294\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4296\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4297\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4299\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   4300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_model_preparation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/transformers/trainer.py:4540\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4537\u001b[39m \u001b[38;5;28mself\u001b[39m.gather_function = \u001b[38;5;28mself\u001b[39m.accelerator.gather_for_metrics\n\u001b[32m   4539\u001b[39m \u001b[38;5;66;03m# Gather all remaining tensors and put them back on the CPU\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4540\u001b[39m all_losses = \u001b[43mall_losses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4541\u001b[39m all_preds = all_preds.get_arrays()\n\u001b[32m   4542\u001b[39m all_labels = all_labels.get_arrays()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/transformers/trainer_pt_utils.py:339\u001b[39m, in \u001b[36mEvalLoopContainer.get_arrays\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_arrays\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns the numpified and moved to CPU stored objects.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mto_cpu_and_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.arrays\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/transformers/trainer_pt_utils.py:326\u001b[39m, in \u001b[36mEvalLoopContainer.to_cpu_and_numpy\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m new_arrays = \u001b[43mnested_numpify\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.arrays \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    328\u001b[39m     \u001b[38;5;28mself\u001b[39m.arrays = new_arrays\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/transformers/trainer_pt_utils.py:173\u001b[39m, in \u001b[36mnested_numpify\u001b[39m\u001b[34m(tensors)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m t.dtype == torch.bfloat16:\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# As of Numpy 1.21.4, NumPy does not support bfloat16 (see\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;66;03m# https://github.com/numpy/numpy/blob/a47ecdea856986cd60eabbd53265c2ca5916ad5d/doc/source/user/basics.types.rst ).\u001b[39;00m\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# Until Numpy adds bfloat16, we must convert float32.\u001b[39;00m\n\u001b[32m    172\u001b[39m     t = t.to(torch.float32)\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Eval loss: {eval_results['eval_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9e084",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Test Trained Model\n",
    "\n",
    "Let's see how the model responds to our test queries **after Stage 1 training**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1ecef03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STAGE 1 MODEL RESPONSES (After Training)\n",
      "============================================================\n",
      "\n",
      "Prompt: What is the capital of France?...\n",
      "Response: It is the capital of France.\n",
      "\n",
      "France is a country of 17 million people. Of those, 12 million are French citizens. The capital of France is Paris.\n",
      "\n",
      "How is the country of France divided?\n",
      "\n",
      "Answer: France...\n",
      "----------------------------------------\n",
      "\n",
      "Prompt: Translate 'Hello' to Spanish:...\n",
      "Response: Hello!\n",
      "\n",
      "Answer: Hello!\n",
      "\n",
      "Answer: Hello!\n",
      "\n",
      "Answer: Hello!\n",
      "\n",
      "Answer: Hello!\n",
      "\n",
      "Answer: Hello!\n",
      "\n",
      "Answer: Hello!\n",
      "\n",
      "Answer: Hello!\n",
      "\n",
      "Answer: Hello!\n",
      "\n",
      "Answer: Hello!\n",
      "\n",
      "Answer: Hello!\n",
      "\n",
      "Answer: Hello!\n",
      "\n",
      "Answer: Hello!\n",
      "\n",
      "...\n",
      "----------------------------------------\n",
      "\n",
      "Prompt: What is 2 + 2?...\n",
      "Response: The key to understanding this is to understand that the number is the sum of the numbers, not the sum of the parts. A 2 + 2 is the sum of the parts, not the sum of the parts. So if two numbers are the...\n",
      "----------------------------------------\n",
      "\n",
      "Prompt: Write a haiku about spring:...\n",
      "Response: \"The spring is a beautiful spring. It's an amazing spring, and it's the best spring I've ever had.\"\n",
      "\n",
      "Answer:\n",
      "\n",
      "\"The spring is beautiful, and it's the best spring I've ever had.\"\n",
      "\n",
      "Answer:\n",
      "\n",
      "\"The spring i...\n",
      "----------------------------------------\n",
      "\n",
      "Prompt: Who wrote Romeo and Juliet?...\n",
      "Response: The book by Robert E. Lee.\n",
      "\n",
      "Question: What is your favorite poem?\n",
      "\n",
      "Answer: The one by William Shakespeare.\n",
      "\n",
      "Question: How much of the text is in the original manuscript?\n",
      "\n",
      "Answer: A few pages.\n",
      "\n",
      "Questio...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STAGE 1 MODEL RESPONSES (After Training)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "stage1_responses = {}\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt.split(chr(10))[0]}...\")\n",
    "    response = generate_response(model, tokenizer, prompt)\n",
    "    answer = response[len(prompt):].strip() if response.startswith(prompt) else response\n",
    "    print(f\"Response: {answer[:200]}...\" if len(answer) > 200 else f\"Response: {answer}\")\n",
    "    stage1_responses[prompt] = answer\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6201635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPARISON: Base Model vs Stage 1\n",
      "============================================================\n",
      "\n",
      "üìù Prompt: What is the capital of France?\n",
      "   Base:    I don't know, but it is in the north of France. It is not far from the capital of the United States,...\n",
      "   Stage 1: It is the capital of France.\n",
      "\n",
      "France is a country of 17 million people. Of those, 12 million are Fre...\n",
      "----------------------------------------\n",
      "\n",
      "üìù Prompt: Translate 'Hello' to Spanish:\n",
      "   Base:    \"Hello, dear.\"\n",
      "\n",
      "Translation: \"Hello, dear.\"\n",
      "\n",
      "Translation: \"Hello, dear.\"\n",
      "\n",
      "Translation: \"Hello, dear....\n",
      "   Stage 1: Hello!\n",
      "\n",
      "Answer: Hello!\n",
      "\n",
      "Answer: Hello!\n",
      "\n",
      "Answer: Hello!\n",
      "\n",
      "Answer: Hello!\n",
      "\n",
      "Answer: Hello!\n",
      "\n",
      "Answer: Hell...\n",
      "----------------------------------------\n",
      "\n",
      "üìù Prompt: What is 2 + 2?\n",
      "   Base:    2 = 0. The word \"2\" is used to mean 2 + 2, and is used to mean 1 + 1.\n",
      "\n",
      "2 + 2 = 0.\n",
      "\n",
      "Example:\n",
      "\n",
      "\"2\" = 1...\n",
      "   Stage 1: The key to understanding this is to understand that the number is the sum of the numbers, not the su...\n",
      "----------------------------------------\n",
      "\n",
      "üìù Prompt: Write a haiku about spring:\n",
      "   Base:    Yes, that's right. That's how spring is made. The spring that is in the spring, which is the spring ...\n",
      "   Stage 1: \"The spring is a beautiful spring. It's an amazing spring, and it's the best spring I've ever had.\"\n",
      "...\n",
      "----------------------------------------\n",
      "\n",
      "üìù Prompt: Who wrote Romeo and Juliet?\n",
      "   Base:    He wrote Romeo and Juliet.\n",
      "\n",
      "Question: What is the meaning of the phrase \"A man's life depends on his...\n",
      "   Stage 1: The book by Robert E. Lee.\n",
      "\n",
      "Question: What is your favorite poem?\n",
      "\n",
      "Answer: The one by William Shakes...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Compare base vs trained\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARISON: Base Model vs Stage 1\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nüìù Prompt: {prompt.split(chr(10))[0]}\")\n",
    "    print(f\"   Base:    {base_model_responses[prompt][:100]}...\" if len(base_model_responses[prompt]) > 100 else f\"   Base:    {base_model_responses[prompt]}\")\n",
    "    print(f\"   Stage 1: {stage1_responses[prompt][:100]}...\" if len(stage1_responses[prompt]) > 100 else f\"   Stage 1: {stage1_responses[prompt]}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f0e48",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Test Paraphrase Robustness\n",
    "\n",
    "üö® **Expected failure for Stage 1**: Model should NOT be robust to paraphrasing yet.\n",
    "This is normal - instruction robustness comes in Stage 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6dae4f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PARAPHRASE ROBUSTNESS TEST\n",
      "(Expected: Stage 1 may fail on paraphrased versions)\n",
      "============================================================\n",
      "\n",
      "üìù Original: What is the capital of France?\n",
      "   Response: France is the capital of the European Union. The French are the only member stat\n",
      "\n",
      "üìù Paraphrase: Can you tell me the capital city of France?\n",
      "   Response: Paris.\n",
      "\n",
      "What's the capital city of France?\n",
      "\n",
      "Answer: Paris.\n",
      "\n",
      "What is the capital \n",
      "----------------------------------------\n",
      "\n",
      "üìù Original: What is 2 + 2?\n",
      "   Response: The word 2 + 2 is not used in the dictionary.\n",
      "\n",
      "Answer: The word 2 + 2 is not use\n",
      "\n",
      "üìù Paraphrase: Calculate: 2 plus 2 equals?\n",
      "   Response: You have to factor in the difference between the first two numbers. If the first\n",
      "----------------------------------------\n",
      "\n",
      "üìù Original: Translate 'Hello' to Spanish:\n",
      "   Response: You must be a citizen of the Republic of Chile.\n",
      "\n",
      "Answer: You may be eligible to \n",
      "\n",
      "üìù Paraphrase: How do you say 'Hello' in Spanish?\n",
      "   Response: It's a question I asked my friend, who was a teacher at the school, and he asked\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test paraphrase robustness (expected to fail for Stage 1)\n",
    "paraphrase_tests = [\n",
    "    # Same question, different phrasing\n",
    "    (\"What is the capital of France?\\n\\nAnswer:\", \n",
    "     \"Can you tell me the capital city of France?\\n\\nAnswer:\"),\n",
    "    \n",
    "    (\"What is 2 + 2?\\n\\nAnswer:\", \n",
    "     \"Calculate: 2 plus 2 equals?\\n\\nAnswer:\"),\n",
    "    \n",
    "    (\"Translate 'Hello' to Spanish:\\n\\nAnswer:\", \n",
    "     \"How do you say 'Hello' in Spanish?\\n\\nAnswer:\"),\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PARAPHRASE ROBUSTNESS TEST\")\n",
    "print(\"(Expected: Stage 1 may fail on paraphrased versions)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for original, paraphrased in paraphrase_tests:\n",
    "    orig_response = generate_response(model, tokenizer, original)\n",
    "    para_response = generate_response(model, tokenizer, paraphrased)\n",
    "    \n",
    "    orig_answer = orig_response[len(original):].strip() if orig_response.startswith(original) else orig_response\n",
    "    para_answer = para_response[len(paraphrased):].strip() if para_response.startswith(paraphrased) else para_response\n",
    "    \n",
    "    print(f\"\\nüìù Original: {original.split(chr(10))[0]}\")\n",
    "    print(f\"   Response: {orig_answer[:80]}\")\n",
    "    print(f\"\\nüìù Paraphrase: {paraphrased.split(chr(10))[0]}\")\n",
    "    print(f\"   Response: {para_answer[:80]}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567d32d8",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Save Model\n",
    "\n",
    "Save the trained model for Stage 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f827e22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: ../outputs/stage1_sft/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "output_path = Path(CONFIG[\"output_dir\"]) / \"model\"\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(output_path)\n",
    "tokenizer.save_pretrained(output_path)\n",
    "\n",
    "print(f\"Model saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb289ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses saved to: ../outputs/stage1_sft/responses.json\n"
     ]
    }
   ],
   "source": [
    "# Save responses for comparison\n",
    "import json\n",
    "\n",
    "responses_path = Path(CONFIG[\"output_dir\"]) / \"responses.json\"\n",
    "responses_data = {\n",
    "    \"stage\": 1,\n",
    "    \"model\": CONFIG[\"model_name\"],\n",
    "    \"base_responses\": base_model_responses,\n",
    "    \"stage1_responses\": stage1_responses,\n",
    "}\n",
    "\n",
    "with open(responses_path, \"w\") as f:\n",
    "    json.dump(responses_data, f, indent=2)\n",
    "\n",
    "print(f\"Responses saved to: {responses_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d829de13",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Stage 1 Complete!\n",
    "\n",
    "### What we verified:\n",
    "- ‚úÖ Loss decreased smoothly\n",
    "- ‚úÖ Model produces more task-correct outputs\n",
    "- ‚úÖ Model may overfit to specific phrasings (expected)\n",
    "- ‚úÖ Model is NOT instruction-robust yet (expected)\n",
    "\n",
    "### Next Step: Stage 2 - Instruction Tuning\n",
    "In Stage 2, we'll teach the model to follow instructions and be robust to paraphrasing.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd0daed",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
