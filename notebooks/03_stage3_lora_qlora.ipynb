{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13359559",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47de295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install transformers datasets accelerate peft bitsandbytes trl pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b2200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    PeftModel,\n",
    "    TaskType,\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"Memory: {total_mem:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881ee1fa",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988a02d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Stage 3\n",
    "CONFIG = {\n",
    "    # Model - Use base model (LoRA trains from base, not checkpoint)\n",
    "    \"model_name\": \"Qwen/Qwen2.5-1.5B\",\n",
    "    \n",
    "    # Quantization (set to False for standard LoRA)\n",
    "    \"use_qlora\": True,  # Set to False for standard LoRA\n",
    "    \n",
    "    # LoRA Config\n",
    "    \"lora_r\": 64,           # LoRA rank\n",
    "    \"lora_alpha\": 128,      # LoRA alpha (typically 2x r)\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                       \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    \n",
    "    # Data\n",
    "    \"max_length\": 1024,\n",
    "    \"train_split\": 0.9,\n",
    "    \"alpaca_subset\": 1000,  # Use subset for demo\n",
    "    \n",
    "    # Training\n",
    "    \"batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"num_epochs\": 3,\n",
    "    \"learning_rate\": 2e-4,  # Higher LR for LoRA\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \n",
    "    # Output\n",
    "    \"output_dir\": \"../outputs/stage3_lora\",\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c944935a",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load Model with Quantization (QLoRA)\n",
    "\n",
    "üîë **QLoRA** = 4-bit quantized base model + LoRA adapters\n",
    "\n",
    "This dramatically reduces memory usage while maintaining quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf4d934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory before loading\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory before loading: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78491c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Tokenizer loaded: {CONFIG['model_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61941eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with optional quantization\n",
    "if CONFIG[\"use_qlora\"]:\n",
    "    print(\"Loading model with QLoRA (4-bit quantization)...\")\n",
    "    \n",
    "    # BitsAndBytes config for 4-bit quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        CONFIG[\"model_name\"],\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    # Prepare for k-bit training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "else:\n",
    "    print(\"Loading model for standard LoRA (no quantization)...\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        CONFIG[\"model_name\"],\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "print(f\"Model loaded. Parameters: {model.num_parameters():,}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory after loading: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dd3d64",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Apply LoRA Adapters\n",
    "\n",
    "üîß LoRA only trains small adapter matrices, keeping the base model frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7e80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=CONFIG[\"target_modules\"],\n",
    ")\n",
    "\n",
    "print(\"LoRA Config:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  Target modules: {lora_config.target_modules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params, all_params = model.get_nb_trainable_parameters()\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PARAMETER EFFICIENCY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"All parameters:       {all_params:>15,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:>15,}\")\n",
    "print(f\"Trainable %:          {100 * trainable_params / all_params:>14.2f}%\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU memory with LoRA: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cfde6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show model structure with LoRA layers\n",
    "print(\"\\nModel modules with LoRA:\")\n",
    "for name, module in model.named_modules():\n",
    "    if \"lora\" in name.lower():\n",
    "        print(f\"  {name}\")\n",
    "        if hasattr(module, 'weight'):\n",
    "            print(f\"    Shape: {module.weight.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7218de7a",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Prepare Training Data\n",
    "\n",
    "Using same instruction format as Stage 2 for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction templates (same as Stage 2)\n",
    "INSTRUCTION_TEMPLATES = [\n",
    "    \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{response}\"\"\",\n",
    "\n",
    "    \"\"\"Instruction: {instruction}\n",
    "{input}\n",
    "\n",
    "Response: {response}\"\"\",\n",
    "\n",
    "    \"\"\"<|im_start|>system\n",
    "You are a helpful assistant.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "{instruction}\n",
    "{input}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{response}\n",
    "<|im_end|>\"\"\",\n",
    "]\n",
    "\n",
    "def format_sample(sample, template_idx=None):\n",
    "    if template_idx is None:\n",
    "        template_idx = random.randint(0, len(INSTRUCTION_TEMPLATES) - 1)\n",
    "    \n",
    "    template = INSTRUCTION_TEMPLATES[template_idx]\n",
    "    \n",
    "    return template.format(\n",
    "        instruction=sample.get(\"instruction\", \"\"),\n",
    "        input=sample.get(\"input\", \"\"),\n",
    "        response=sample.get(\"response\", sample.get(\"output\", \"\")),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57305fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Alpaca dataset\n",
    "print(\"Loading Alpaca dataset...\")\n",
    "alpaca_dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "\n",
    "data = []\n",
    "for item in alpaca_dataset:\n",
    "    data.append({\n",
    "        \"instruction\": item[\"instruction\"],\n",
    "        \"input\": item[\"input\"],\n",
    "        \"response\": item[\"output\"],\n",
    "    })\n",
    "\n",
    "# Use subset\n",
    "if CONFIG[\"alpaca_subset\"]:\n",
    "    random.seed(42)\n",
    "    random.shuffle(data)\n",
    "    data = data[:CONFIG[\"alpaca_subset\"]]\n",
    "\n",
    "print(f\"Loaded {len(data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f59038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format and split data\n",
    "formatted_texts = [format_sample(s) for s in data]\n",
    "\n",
    "random.shuffle(formatted_texts)\n",
    "split_idx = int(len(formatted_texts) * CONFIG[\"train_split\"])\n",
    "train_texts = formatted_texts[:split_idx]\n",
    "eval_texts = formatted_texts[split_idx:]\n",
    "\n",
    "print(f\"Train: {len(train_texts)}, Eval: {len(eval_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74694917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "def tokenize_texts(texts, tokenizer, max_length):\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "\n",
    "train_tokenized = tokenize_texts(train_texts, tokenizer, CONFIG[\"max_length\"])\n",
    "eval_tokenized = tokenize_texts(eval_texts, tokenizer, CONFIG[\"max_length\"])\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": train_tokenized[\"input_ids\"].tolist(),\n",
    "    \"attention_mask\": train_tokenized[\"attention_mask\"].tolist(),\n",
    "    \"labels\": train_tokenized[\"labels\"].tolist(),\n",
    "})\n",
    "\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": eval_tokenized[\"input_ids\"].tolist(),\n",
    "    \"attention_mask\": eval_tokenized[\"attention_mask\"].tolist(),\n",
    "    \"labels\": eval_tokenized[\"labels\"].tolist(),\n",
    "})\n",
    "\n",
    "print(f\"Train dataset: {train_dataset}\")\n",
    "print(f\"Eval dataset: {eval_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f8a7cf",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71ec1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments for LoRA\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    \n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    \n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # Lower max_grad_norm for LoRA stability\n",
    "    max_grad_norm=0.3,\n",
    "    \n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    bf16=True,\n",
    "    \n",
    "    # Use paged optimizer for memory efficiency\n",
    "    optim=\"paged_adamw_8bit\" if CONFIG[\"use_qlora\"] else \"adamw_torch\",\n",
    "    \n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b703a688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4992d51",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Test Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f654afcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_new_tokens=128):\n",
    "    \"\"\"Generate response from model.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "What is the capital of France?\n",
    "\n",
    "### Response:\"\"\",\n",
    "\n",
    "    \"\"\"Instruction: Explain quantum computing in simple terms.\n",
    "\n",
    "Response:\"\"\",\n",
    "\n",
    "    \"\"\"<|im_start|>system\n",
    "You are a helpful assistant.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "Write a short poem about coding.\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL RESPONSES (Before LoRA Training)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "before_responses = {}\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\n--- Test {i+1} ---\")\n",
    "    response = generate_response(model, tokenizer, prompt)\n",
    "    answer = response[len(prompt):].strip() if response.startswith(prompt) else response\n",
    "    print(f\"Response: {answer[:150]}...\" if len(answer) > 150 else f\"Response: {answer}\")\n",
    "    before_responses[i] = answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3b349b",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Train with LoRA\n",
    "\n",
    "üöÄ **Watch the memory usage** - it should be much lower than full fine-tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb385613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory before training\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory before training: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a50288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"Starting LoRA training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Training complete!\")\n",
    "print(f\"Total steps: {train_result.global_step}\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nPeak GPU memory: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5fcb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Eval loss: {eval_results['eval_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392806c0",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Test After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354c734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL RESPONSES (After LoRA Training)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "after_responses = {}\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\n--- Test {i+1} ---\")\n",
    "    response = generate_response(model, tokenizer, prompt)\n",
    "    answer = response[len(prompt):].strip() if response.startswith(prompt) else response\n",
    "    print(f\"Response: {answer[:150]}...\" if len(answer) > 150 else f\"Response: {answer}\")\n",
    "    after_responses[i] = answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39106aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARISON: Before vs After LoRA Training\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(len(test_prompts)):\n",
    "    print(f\"\\n--- Test {i+1} ---\")\n",
    "    print(f\"Before: {before_responses[i][:100]}...\" if len(before_responses[i]) > 100 else f\"Before: {before_responses[i]}\")\n",
    "    print(f\"After:  {after_responses[i][:100]}...\" if len(after_responses[i]) > 100 else f\"After:  {after_responses[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdff55b",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Save LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d638aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters only (small files!)\n",
    "adapter_path = Path(CONFIG[\"output_dir\"]) / \"adapter\"\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "\n",
    "print(f\"LoRA adapters saved to: {adapter_path}\")\n",
    "\n",
    "# Check adapter size\n",
    "import os\n",
    "total_size = sum(os.path.getsize(f) for f in adapter_path.rglob(\"*\") if f.is_file())\n",
    "print(f\"Adapter size: {total_size / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c181ae02",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Merge LoRA with Base Model\n",
    "\n",
    "For deployment, you can merge the LoRA adapters back into the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43646ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA adapters with base model\n",
    "print(\"Merging LoRA adapters with base model...\")\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "print(f\"Merged model parameters: {merged_model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b721b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test merged model\n",
    "print(\"=\" * 60)\n",
    "print(\"MERGED MODEL TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\n--- Test {i+1} ---\")\n",
    "    response = generate_response(merged_model, tokenizer, prompt)\n",
    "    answer = response[len(prompt):].strip() if response.startswith(prompt) else response\n",
    "    print(f\"Response: {answer[:150]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29e4b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged model\n",
    "merged_path = Path(CONFIG[\"output_dir\"]) / \"merged\"\n",
    "merged_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "merged_model.save_pretrained(merged_path)\n",
    "tokenizer.save_pretrained(merged_path)\n",
    "\n",
    "print(f\"Merged model saved to: {merged_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e4a782",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Load LoRA Adapter (For Inference)\n",
    "\n",
    "Demonstration of how to load a saved LoRA adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd5623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to load LoRA adapter for inference\n",
    "print(\"Demonstrating LoRA adapter loading...\")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapter_path,\n",
    ")\n",
    "\n",
    "print(\"LoRA adapter loaded successfully!\")\n",
    "\n",
    "# Test\n",
    "test_prompt = test_prompts[0]\n",
    "response = generate_response(lora_model, tokenizer, test_prompt)\n",
    "answer = response[len(test_prompt):].strip() if response.startswith(test_prompt) else response\n",
    "print(f\"\\nTest response: {answer[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7658d5f9",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Memory Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dac441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"STAGE 3 SUMMARY - LoRA/QLoRA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Configuration:\")\n",
    "print(f\"   Model: {CONFIG['model_name']}\")\n",
    "print(f\"   QLoRA (4-bit): {CONFIG['use_qlora']}\")\n",
    "print(f\"   LoRA rank: {CONFIG['lora_r']}\")\n",
    "print(f\"   LoRA alpha: {CONFIG['lora_alpha']}\")\n",
    "\n",
    "print(f\"\\nüìà Parameter Efficiency:\")\n",
    "print(f\"   Total parameters: {all_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Trainable %: {100 * trainable_params / all_params:.2f}%\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nüíæ Memory Usage:\")\n",
    "    print(f\"   Peak GPU memory: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "print(f\"\\nüìÅ Saved Files:\")\n",
    "print(f\"   Adapter: {adapter_path}\")\n",
    "print(f\"   Merged: {merged_path}\")\n",
    "print(f\"   Adapter size: {total_size / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18138416",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Stage 3 Complete!\n",
    "\n",
    "### What we achieved:\n",
    "- ‚úÖ **Dramatically reduced memory usage** with QLoRA\n",
    "- ‚úÖ **Only ~2-5% of parameters trained** (LoRA adapters)\n",
    "- ‚úÖ **Similar quality** to full fine-tuning\n",
    "- ‚úÖ **Tiny adapter files** for easy deployment\n",
    "- ‚úÖ **Can merge adapters** into base model\n",
    "\n",
    "### Key Benefits:\n",
    "- üöÄ Can fine-tune larger models on smaller GPUs\n",
    "- üí∞ Much cheaper training\n",
    "- üîí Base model knowledge preserved\n",
    "- üì¶ Easy to swap adapters for different tasks\n",
    "\n",
    "### Next Step: Full Evaluation\n",
    "Compare all three stages on the same test queries!\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
