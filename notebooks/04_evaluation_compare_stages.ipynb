{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2900c338",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d44e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import pandas as pd\n",
    "\n",
    "# Check GPU\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463893a7",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Define Test Queries\n",
    "\n",
    "Comprehensive test set covering multiple dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eadba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries organized by category\n",
    "TEST_QUERIES = {\n",
    "    \"factual_knowledge\": [\n",
    "        {\n",
    "            \"id\": \"fact_001\",\n",
    "            \"query\": \"What is the capital of France?\",\n",
    "            \"expected\": \"Paris\",\n",
    "            \"variants\": [\n",
    "                \"Can you tell me the capital city of France?\",\n",
    "                \"France's capital is?\",\n",
    "                \"Name the capital of France.\",\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"fact_002\",\n",
    "            \"query\": \"Who wrote 'Romeo and Juliet'?\",\n",
    "            \"expected\": \"William Shakespeare\",\n",
    "            \"variants\": [\n",
    "                \"What author wrote Romeo and Juliet?\",\n",
    "                \"Romeo and Juliet was written by whom?\",\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"fact_003\",\n",
    "            \"query\": \"What is the chemical symbol for water?\",\n",
    "            \"expected\": \"H2O\",\n",
    "            \"variants\": [\n",
    "                \"Give me the molecular formula of water.\",\n",
    "                \"How is water written in chemistry?\",\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    \n",
    "    \"reasoning\": [\n",
    "        {\n",
    "            \"id\": \"reason_001\",\n",
    "            \"query\": \"What comes next in this sequence: 2, 4, 8, 16, ?\",\n",
    "            \"expected\": \"32\",\n",
    "            \"variants\": [\n",
    "                \"Complete the sequence: 2, 4, 8, 16, ...\",\n",
    "                \"What number follows 16 in: 2, 4, 8, 16?\",\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"reason_002\",\n",
    "            \"query\": \"If I have 3 apples and buy 4 more, then give away 2, how many do I have?\",\n",
    "            \"expected\": \"5\",\n",
    "            \"variants\": [\n",
    "                \"3 apples + 4 apples - 2 apples = ?\",\n",
    "                \"Calculate: Start with 3, add 4, subtract 2.\",\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"reason_003\",\n",
    "            \"query\": \"A bat and ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost?\",\n",
    "            \"expected\": \"$0.05\",\n",
    "            \"variants\": [],\n",
    "        },\n",
    "    ],\n",
    "    \n",
    "    \"instruction_following\": [\n",
    "        {\n",
    "            \"id\": \"instr_001\",\n",
    "            \"query\": \"List exactly 3 fruits that are red.\",\n",
    "            \"expected\": \"Exactly 3 red fruits\",\n",
    "            \"variants\": [\n",
    "                \"Name 3 red fruits.\",\n",
    "                \"Give me three fruits that are red in color.\",\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"instr_002\",\n",
    "            \"query\": \"Respond with ONLY the word 'yes' or 'no': Is the sky blue?\",\n",
    "            \"expected\": \"yes\",\n",
    "            \"variants\": [\n",
    "                \"Answer only yes or no: Is the sky blue?\",\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"instr_003\",\n",
    "            \"query\": \"Translate 'Hello' to Spanish, then French, then German.\",\n",
    "            \"expected\": \"Hola, Bonjour, Hallo\",\n",
    "            \"variants\": [],\n",
    "        },\n",
    "    ],\n",
    "    \n",
    "    \"creative\": [\n",
    "        {\n",
    "            \"id\": \"creative_001\",\n",
    "            \"query\": \"Write a haiku about technology.\",\n",
    "            \"expected\": \"A haiku (5-7-5 syllables)\",\n",
    "            \"variants\": [\n",
    "                \"Compose a haiku on the theme of technology.\",\n",
    "                \"Create a haiku about tech.\",\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"creative_002\",\n",
    "            \"query\": \"Invent a new word and define it.\",\n",
    "            \"expected\": \"A novel word with definition\",\n",
    "            \"variants\": [],\n",
    "        },\n",
    "    ],\n",
    "    \n",
    "    \"code_generation\": [\n",
    "        {\n",
    "            \"id\": \"code_001\",\n",
    "            \"query\": \"Write a Python function to check if a number is prime.\",\n",
    "            \"expected\": \"Working Python function\",\n",
    "            \"variants\": [\n",
    "                \"Create a Python prime checker function.\",\n",
    "                \"Give me Python code to determine if a number is prime.\",\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"code_002\",\n",
    "            \"query\": \"Write a JavaScript function to reverse a string.\",\n",
    "            \"expected\": \"Working JavaScript function\",\n",
    "            \"variants\": [],\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Count total queries\n",
    "total_queries = sum(len(queries) for queries in TEST_QUERIES.values())\n",
    "total_variants = sum(\n",
    "    len(q.get(\"variants\", [])) \n",
    "    for queries in TEST_QUERIES.values() \n",
    "    for q in queries\n",
    ")\n",
    "\n",
    "print(f\"Test Categories: {len(TEST_QUERIES)}\")\n",
    "print(f\"Total Queries: {total_queries}\")\n",
    "print(f\"Total Variants: {total_variants}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fb8c98",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Define Prompt Formats\n",
    "\n",
    "Different formats for each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28c97ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(query: str, stage: int) -> str:\n",
    "    \"\"\"Format query based on training stage.\"\"\"\n",
    "    \n",
    "    if stage == 0:  # Base model\n",
    "        return f\"{query}\\n\\nAnswer:\"\n",
    "    \n",
    "    elif stage == 1:  # Normal SFT\n",
    "        return f\"{query}\\n\\nAnswer:\"\n",
    "    \n",
    "    elif stage == 2:  # Instruction Tuning\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{query}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    \n",
    "    elif stage == 3:  # LoRA\n",
    "        return f\"\"\"<|im_start|>system\n",
    "You are a helpful assistant.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "{query}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    return query\n",
    "\n",
    "# Test format\n",
    "test_query = \"What is 2+2?\"\n",
    "for stage in range(4):\n",
    "    print(f\"\\n=== Stage {stage} ===\")\n",
    "    print(format_prompt(test_query, stage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481a77d6",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3266796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paths\n",
    "MODEL_PATHS = {\n",
    "    0: \"Qwen/Qwen2.5-1.5B\",                    # Base model\n",
    "    1: \"../outputs/stage1_sft/model\",          # Stage 1\n",
    "    2: \"../outputs/stage2_instruction/model\",  # Stage 2\n",
    "    3: \"../outputs/stage3_lora/merged\",        # Stage 3 (merged)\n",
    "}\n",
    "\n",
    "# For LoRA (if not merged)\n",
    "LORA_ADAPTER_PATH = \"../outputs/stage3_lora/adapter\"\n",
    "\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen2.5-1.5B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea426f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_for_stage(stage: int):\n",
    "    \"\"\"Load model for a specific stage.\"\"\"\n",
    "    \n",
    "    model_path = MODEL_PATHS[stage]\n",
    "    \n",
    "    # Check if path exists (for local models)\n",
    "    if not model_path.startswith(\"Qwen\") and not Path(model_path).exists():\n",
    "        print(f\"âš ï¸ Model not found: {model_path}\")\n",
    "        print(\"  (Run the training notebook first)\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"Loading Stage {stage} model from: {model_path}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"  Loaded. Parameters: {model.num_parameters():,}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_lora_model():\n",
    "    \"\"\"Load LoRA adapter separately.\"\"\"\n",
    "    \n",
    "    if not Path(LORA_ADAPTER_PATH).exists():\n",
    "        print(f\"âš ï¸ LoRA adapter not found: {LORA_ADAPTER_PATH}\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"Loading LoRA adapter from: {LORA_ADAPTER_PATH}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)\n",
    "    \n",
    "    print(f\"  LoRA loaded.\")\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bfaadd",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026da2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt: str, \n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.7,\n",
    ") -> str:\n",
    "    \"\"\"Generate response from model.\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the response part\n",
    "    if full_response.startswith(prompt):\n",
    "        response = full_response[len(prompt):].strip()\n",
    "    else:\n",
    "        response = full_response\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27bb148",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Evaluate Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, stage: int, verbose: bool = True):\n",
    "    \"\"\"Evaluate model on all test queries.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for category, queries in TEST_QUERIES.items():\n",
    "        if verbose:\n",
    "            print(f\"\\nðŸ“‚ Category: {category}\")\n",
    "        \n",
    "        for query_data in queries:\n",
    "            query_id = query_data[\"id\"]\n",
    "            query = query_data[\"query\"]\n",
    "            expected = query_data[\"expected\"]\n",
    "            \n",
    "            # Format prompt for this stage\n",
    "            prompt = format_prompt(query, stage)\n",
    "            \n",
    "            # Generate response\n",
    "            response = generate_response(model, tokenizer, prompt)\n",
    "            \n",
    "            results.append({\n",
    "                \"stage\": stage,\n",
    "                \"category\": category,\n",
    "                \"query_id\": query_id,\n",
    "                \"query\": query,\n",
    "                \"expected\": expected,\n",
    "                \"response\": response,\n",
    "            })\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  [{query_id}] {query[:40]}...\")\n",
    "                print(f\"       â†’ {response[:80]}...\" if len(response) > 80 else f\"       â†’ {response}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c448e3c",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Run Full Evaluation\n",
    "\n",
    "âš ï¸ **Note**: This will load each model sequentially. Adjust based on your GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707b6c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = {}\n",
    "\n",
    "# Evaluate each stage\n",
    "for stage in range(4):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EVALUATING STAGE {stage}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load model\n",
    "    model, tokenizer = load_model_for_stage(stage)\n",
    "    \n",
    "    if model is None:\n",
    "        print(f\"Skipping Stage {stage} (model not available)\")\n",
    "        continue\n",
    "    \n",
    "    # Evaluate\n",
    "    results = evaluate_model(model, tokenizer, stage, verbose=True)\n",
    "    all_results[stage] = results\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del model\n",
    "    del tokenizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\nâœ… Stage {stage} complete. {len(results)} queries evaluated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22c2c69",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Paraphrase Robustness Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86bc800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_paraphrase_robustness(model, tokenizer, stage: int):\n",
    "    \"\"\"Test how well the model handles paraphrased queries.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for category, queries in TEST_QUERIES.items():\n",
    "        for query_data in queries:\n",
    "            variants = query_data.get(\"variants\", [])\n",
    "            if not variants:\n",
    "                continue\n",
    "            \n",
    "            query_id = query_data[\"id\"]\n",
    "            original_query = query_data[\"query\"]\n",
    "            \n",
    "            # Get response for original\n",
    "            original_prompt = format_prompt(original_query, stage)\n",
    "            original_response = generate_response(model, tokenizer, original_prompt)\n",
    "            \n",
    "            # Get responses for variants\n",
    "            variant_responses = []\n",
    "            for variant in variants:\n",
    "                variant_prompt = format_prompt(variant, stage)\n",
    "                variant_response = generate_response(model, tokenizer, variant_prompt)\n",
    "                variant_responses.append({\n",
    "                    \"variant\": variant,\n",
    "                    \"response\": variant_response,\n",
    "                })\n",
    "            \n",
    "            results.append({\n",
    "                \"query_id\": query_id,\n",
    "                \"original_query\": original_query,\n",
    "                \"original_response\": original_response,\n",
    "                \"variants\": variant_responses,\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Run paraphrase test for available models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PARAPHRASE ROBUSTNESS TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "paraphrase_results = {}\n",
    "\n",
    "for stage in [1, 2, 3]:  # Skip base model (stage 0)\n",
    "    print(f\"\\n--- Stage {stage} ---\")\n",
    "    \n",
    "    model, tokenizer = load_model_for_stage(stage)\n",
    "    if model is None:\n",
    "        continue\n",
    "    \n",
    "    results = evaluate_paraphrase_robustness(model, tokenizer, stage)\n",
    "    paraphrase_results[stage] = results\n",
    "    \n",
    "    # Show sample\n",
    "    if results:\n",
    "        sample = results[0]\n",
    "        print(f\"\\nSample - {sample['query_id']}:\")\n",
    "        print(f\"  Original: {sample['original_query'][:50]}...\")\n",
    "        print(f\"  Response: {sample['original_response'][:50]}...\")\n",
    "        for v in sample['variants'][:2]:\n",
    "            print(f\"  Variant:  {v['variant'][:50]}...\")\n",
    "            print(f\"  Response: {v['response'][:50]}...\")\n",
    "    \n",
    "    del model\n",
    "    del tokenizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf1e79",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Compare Responses Side-by-Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af768a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_stages(query_id: str):\n",
    "    \"\"\"Compare responses from all stages for a specific query.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"COMPARISON FOR: {query_id}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for stage, results in all_results.items():\n",
    "        for r in results:\n",
    "            if r[\"query_id\"] == query_id:\n",
    "                print(f\"\\nðŸ”¹ Stage {stage}:\")\n",
    "                print(f\"   Query: {r['query'][:60]}...\" if len(r['query']) > 60 else f\"   Query: {r['query']}\")\n",
    "                print(f\"   Expected: {r['expected']}\")\n",
    "                response = r['response'][:200] + \"...\" if len(r['response']) > 200 else r['response']\n",
    "                print(f\"   Response: {response}\")\n",
    "                break\n",
    "\n",
    "# Compare for a few queries\n",
    "for qid in [\"fact_001\", \"reason_001\", \"instr_001\", \"creative_001\", \"code_001\"]:\n",
    "    compare_stages(qid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0387df8",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Create Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07779e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for analysis\n",
    "if all_results:\n",
    "    all_data = []\n",
    "    for stage, results in all_results.items():\n",
    "        for r in results:\n",
    "            all_data.append(r)\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nTotal evaluations: {len(df)}\")\n",
    "    print(f\"\\nEvaluations per stage:\")\n",
    "    print(df.groupby(\"stage\").size())\n",
    "    \n",
    "    print(f\"\\nEvaluations per category:\")\n",
    "    print(df.groupby(\"category\").size())\n",
    "    \n",
    "    # Average response length per stage\n",
    "    df[\"response_length\"] = df[\"response\"].apply(len)\n",
    "    print(f\"\\nAverage response length per stage:\")\n",
    "    print(df.groupby(\"stage\")[\"response_length\"].mean().round(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd926d44",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28708be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to JSON\n",
    "output_dir = Path(\"../outputs/evaluation\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Main results\n",
    "results_path = output_dir / f\"evaluation_results_{timestamp}.json\"\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"timestamp\": timestamp,\n",
    "        \"results\": {str(k): v for k, v in all_results.items()},\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {results_path}\")\n",
    "\n",
    "# Paraphrase results\n",
    "if paraphrase_results:\n",
    "    para_path = output_dir / f\"paraphrase_results_{timestamp}.json\"\n",
    "    with open(para_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"timestamp\": timestamp,\n",
    "            \"results\": {str(k): v for k, v in paraphrase_results.items()},\n",
    "        }, f, indent=2)\n",
    "    print(f\"Paraphrase results saved to: {para_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa5b3c4",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Key Observations Template\n",
    "\n",
    "Use this section to document your observations from the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc0277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = \"\"\"\n",
    "# ðŸ“Š EVALUATION OBSERVATIONS\n",
    "\n",
    "## Stage 0 (Base Model)\n",
    "- [ ] Baseline behavior documented\n",
    "- [ ] No instruction-following expected\n",
    "- Notes: \n",
    "\n",
    "## Stage 1 (Normal SFT)\n",
    "- [ ] Loss decreased smoothly during training\n",
    "- [ ] Outputs more task-correct than base\n",
    "- [ ] NOT robust to paraphrasing (expected)\n",
    "- Notes:\n",
    "\n",
    "## Stage 2 (Instruction Tuning)\n",
    "- [ ] Instruction-following improved\n",
    "- [ ] Paraphrase robustness improved\n",
    "- [ ] Multi-format understanding\n",
    "- Notes:\n",
    "\n",
    "## Stage 3 (LoRA/QLoRA)\n",
    "- [ ] Memory efficiency verified\n",
    "- [ ] Similar quality to full fine-tuning\n",
    "- [ ] Base model knowledge preserved\n",
    "- Notes:\n",
    "\n",
    "## Comparison Highlights\n",
    "- Best factual accuracy: Stage ?\n",
    "- Best instruction following: Stage ?\n",
    "- Most creative: Stage ?\n",
    "- Best code generation: Stage ?\n",
    "\n",
    "## Surprises / Issues\n",
    "- \n",
    "\n",
    "## Next Steps\n",
    "- \n",
    "\"\"\"\n",
    "\n",
    "print(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c840e5",
   "metadata": {},
   "source": [
    "---\n",
    "## âœ… Evaluation Complete!\n",
    "\n",
    "### What we evaluated:\n",
    "- âœ… All 4 stages (Base, Stage 1, Stage 2, Stage 3)\n",
    "- âœ… Multiple categories (Factual, Reasoning, Instructions, Creative, Code)\n",
    "- âœ… Paraphrase robustness\n",
    "- âœ… Side-by-side comparisons\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **Stage 1** teaches task completion but not instruction abstraction\n",
    "2. **Stage 2** adds instruction robustness and format flexibility\n",
    "3. **Stage 3** achieves similar results with dramatically less memory\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
