{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb0e986",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3ee880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install transformers datasets accelerate peft bitsandbytes trl pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ce4800b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.4.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 638, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1971, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3123, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3178, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3641, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3701, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/tg/5d928d1d0z15w2_p1z2f224r0000gr/T/ipykernel_73105/4055232218.py\", line 9, in <module>\n",
      "    import torch\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.2\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "721a7f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì SSL verification disabled\n"
     ]
    }
   ],
   "source": [
    "# SSL workaround for HuggingFace downloads\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = ''\n",
    "\n",
    "import httpx\n",
    "original_client_init = httpx.Client.__init__\n",
    "def patched_client_init(self, *args, **kwargs):\n",
    "    kwargs['verify'] = False\n",
    "    return original_client_init(self, *args, **kwargs)\n",
    "httpx.Client.__init__ = patched_client_init\n",
    "\n",
    "print(\"‚úì SSL verification disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20959177",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afd47127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  model_name: ../outputs/stage1_sft/model\n",
      "  max_length: 512\n",
      "  train_split: 0.9\n",
      "  use_alpaca: True\n",
      "  alpaca_subset: 200\n",
      "  batch_size: 2\n",
      "  gradient_accumulation_steps: 4\n",
      "  num_epochs: 2\n",
      "  learning_rate: 1e-05\n",
      "  warmup_ratio: 0.1\n",
      "  weight_decay: 0.01\n",
      "  randomize_templates: True\n",
      "  output_dir: ../outputs/stage2_instruction\n"
     ]
    }
   ],
   "source": [
    "# Configuration for Stage 2\n",
    "CONFIG = {\n",
    "    # Model - Load Stage 1 GPT-2 checkpoint\n",
    "    \"model_name\": \"../outputs/stage1_sft/model\",  # Load Stage 1 checkpoint\n",
    "    \n",
    "    # Data\n",
    "    \"max_length\": 512,  # Match Stage 1\n",
    "    \"train_split\": 0.9,\n",
    "    \"use_alpaca\": True,  # Use Alpaca dataset from HuggingFace\n",
    "    \"alpaca_subset\": 200,  # Smaller subset for faster training (GPT-2 on CPU)\n",
    "    \n",
    "    # Training\n",
    "    \"batch_size\": 2,  # Smaller for CPU\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"num_epochs\": 2,  # Fewer epochs for demo\n",
    "    \"learning_rate\": 1e-5,  # Lower LR since continuing from Stage 1\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \n",
    "    # Template randomization\n",
    "    \"randomize_templates\": True,\n",
    "    \n",
    "    # Output\n",
    "    \"output_dir\": \"../outputs/stage2_instruction\",\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c94ac9",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Define Instruction Templates\n",
    "\n",
    "üîë **Key to Stage 2 success**: Use multiple prompt templates to prevent instruction overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b63f0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 5 instruction templates\n",
      "Defined 5 system message variations\n"
     ]
    }
   ],
   "source": [
    "# Multiple instruction templates for robustness\n",
    "INSTRUCTION_TEMPLATES = [\n",
    "    # Template 1: Alpaca-style\n",
    "    \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{response}\"\"\",\n",
    "\n",
    "    # Template 2: Simple\n",
    "    \"\"\"Instruction: {instruction}\n",
    "{input}\n",
    "\n",
    "Response: {response}\"\"\",\n",
    "\n",
    "    # Template 3: ChatML-style\n",
    "    \"\"\"<|im_start|>system\n",
    "{system_message}\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "{instruction}\n",
    "{input}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{response}\n",
    "<|im_end|>\"\"\",\n",
    "\n",
    "    # Template 4: Conversational\n",
    "    \"\"\"User: {instruction}\n",
    "{input}\n",
    "\n",
    "Assistant: {response}\"\"\",\n",
    "\n",
    "    # Template 5: Task-focused\n",
    "    \"\"\"Task: {instruction}\n",
    "Context: {input}\n",
    "\n",
    "Output: {response}\"\"\",\n",
    "]\n",
    "\n",
    "# System message variations (for ChatML template)\n",
    "SYSTEM_MESSAGES = [\n",
    "    \"You are a helpful assistant.\",\n",
    "    \"You are an AI assistant that follows instructions carefully.\",\n",
    "    \"You are a knowledgeable and helpful AI.\",\n",
    "    \"You are an assistant designed to help users with their tasks.\",\n",
    "    \"You are a friendly AI that provides accurate and helpful responses.\",\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(INSTRUCTION_TEMPLATES)} instruction templates\")\n",
    "print(f\"Defined {len(SYSTEM_MESSAGES)} system message variations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d41a82a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample formatted with different templates:\n",
      "============================================================\n",
      "\n",
      "--- Template 1 ---\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Translate the following text to French.\n",
      "\n",
      "### Input:\n",
      "Hello, how are you?\n",
      "\n",
      "### Response:\n",
      "Bonjour, comment allez-vous?\n",
      "\n",
      "\n",
      "--- Template 2 ---\n",
      "Instruction: Translate the following text to French.\n",
      "Hello, how are you?\n",
      "\n",
      "Response: Bonjour, comment allez-vous?\n",
      "\n",
      "\n",
      "--- Template 3 ---\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Translate the following text to French.\n",
      "Hello, how are you?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Bonjour, comment allez-vous?\n",
      "<|im_end|>\n",
      "\n",
      "\n",
      "--- Template 4 ---\n",
      "User: Translate the following text to French.\n",
      "Hello, how are you?\n",
      "\n",
      "Assistant: Bonjour, comment allez-vous?\n",
      "\n",
      "\n",
      "--- Template 5 ---\n",
      "Task: Translate the following text to French.\n",
      "Context: Hello, how are you?\n",
      "\n",
      "Output: Bonjour, comment allez-vous?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def format_instruction_sample(sample, template_idx=None, randomize=True):\n",
    "    \"\"\"\n",
    "    Format a sample with instruction templates.\n",
    "    Optionally randomize template for robustness.\n",
    "    \"\"\"\n",
    "    instruction = sample.get(\"instruction\", \"\")\n",
    "    input_text = sample.get(\"input\", \"\")\n",
    "    response = sample.get(\"response\", sample.get(\"output\", \"\"))\n",
    "    \n",
    "    # Select template\n",
    "    if randomize and template_idx is None:\n",
    "        template_idx = random.randint(0, len(INSTRUCTION_TEMPLATES) - 1)\n",
    "    elif template_idx is None:\n",
    "        template_idx = 0\n",
    "    \n",
    "    template = INSTRUCTION_TEMPLATES[template_idx]\n",
    "    \n",
    "    # For ChatML template, add system message\n",
    "    if \"system_message\" in template:\n",
    "        system_msg = random.choice(SYSTEM_MESSAGES) if randomize else SYSTEM_MESSAGES[0]\n",
    "        return template.format(\n",
    "            system_message=system_msg,\n",
    "            instruction=instruction,\n",
    "            input=input_text if input_text else \"\",\n",
    "            response=response,\n",
    "        )\n",
    "    else:\n",
    "        return template.format(\n",
    "            instruction=instruction,\n",
    "            input=input_text if input_text else \"\",\n",
    "            response=response,\n",
    "        )\n",
    "\n",
    "# Test formatting\n",
    "test_sample = {\n",
    "    \"instruction\": \"Translate the following text to French.\",\n",
    "    \"input\": \"Hello, how are you?\",\n",
    "    \"response\": \"Bonjour, comment allez-vous?\",\n",
    "}\n",
    "\n",
    "print(\"Sample formatted with different templates:\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(len(INSTRUCTION_TEMPLATES)):\n",
    "    print(f\"\\n--- Template {i+1} ---\")\n",
    "    print(format_instruction_sample(test_sample, template_idx=i, randomize=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bd5a75",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "671ac6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from: ../outputs/stage1_sft/model\n",
      "Vocab size: 50257\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer from Stage 1 model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Tokenizer loaded from: {CONFIG['model_name']}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8de8e8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Stage 1 model from: ../outputs/stage1_sft/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Warning #191: Forking a process while a parallel region is active is potentially unsafe.\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 148/148 [00:01<00:00, 112.66it/s, Materializing param=transformer.wte.weight]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. Parameters: 124,439,808\n",
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load model from Stage 1 checkpoint\n",
    "model_path = CONFIG[\"model_name\"]\n",
    "\n",
    "print(f\"Loading Stage 1 model from: {model_path}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float32,  # Use float32 for CPU\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded. Parameters: {model.num_parameters():,}\")\n",
    "print(f\"Device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629b11d2",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Prepare Training Data\n",
    "\n",
    "We'll use the **Alpaca dataset** (cleaned version) for instruction tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f84fe4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Alpaca dataset from HuggingFace...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51760/51760 [00:00<00:00, 106711.29 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 samples from Alpaca dataset\n"
     ]
    }
   ],
   "source": [
    "# Load Alpaca dataset\n",
    "if CONFIG[\"use_alpaca\"]:\n",
    "    print(\"Loading Alpaca dataset from HuggingFace...\")\n",
    "    alpaca_dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "    \n",
    "    # Convert to list of dicts\n",
    "    data = []\n",
    "    for item in alpaca_dataset:\n",
    "        data.append({\n",
    "            \"instruction\": item[\"instruction\"],\n",
    "            \"input\": item[\"input\"],\n",
    "            \"response\": item[\"output\"],\n",
    "        })\n",
    "    \n",
    "    # Use subset if specified\n",
    "    if CONFIG[\"alpaca_subset\"]:\n",
    "        random.seed(42)\n",
    "        random.shuffle(data)\n",
    "        data = data[:CONFIG[\"alpaca_subset\"]]\n",
    "    \n",
    "    print(f\"Loaded {len(data)} samples from Alpaca dataset\")\n",
    "else:\n",
    "    # Use manual data\n",
    "    data = [\n",
    "        {\"instruction\": \"Translate to French.\", \"input\": \"Hello, how are you?\", \"response\": \"Bonjour, comment allez-vous?\"},\n",
    "        {\"instruction\": \"Summarize the text.\", \"input\": \"Machine learning is a subset of AI...\", \"response\": \"ML is an AI subset.\"},\n",
    "        # Add more samples...\n",
    "    ]\n",
    "    print(f\"Using manual data: {len(data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5fe6dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training entries:\n",
      "============================================================\n",
      "\n",
      "--- Sample 1 ---\n",
      "Instruction: Describe a traditional wedding ceremony you attended\n",
      "Input: \n",
      "Response: As an AI, I do not have physical presence and therefore do not attend events suc...\n",
      "\n",
      "--- Sample 2 ---\n",
      "Instruction: What do you understand by AI explainability?\n",
      "Input: \n",
      "Response: AI explainability refers to the ability to understand and interpret the reasonin...\n",
      "\n",
      "--- Sample 3 ---\n",
      "Instruction: Generate a dialogue between a customer and a salesperson in a department store.\n",
      "Input: \n",
      "Response: Customer: Excuse me, could you help me find the men's section?\n",
      "\n",
      "Salesperson: Of ...\n"
     ]
    }
   ],
   "source": [
    "# Show sample data\n",
    "print(\"Sample training entries:\")\n",
    "print(\"=\" * 60)\n",
    "for i, sample in enumerate(data[:3]):\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(f\"Instruction: {sample['instruction'][:80]}...\" if len(sample['instruction']) > 80 else f\"Instruction: {sample['instruction']}\")\n",
    "    print(f\"Input: {sample['input'][:50]}...\" if len(sample['input']) > 50 else f\"Input: {sample['input']}\")\n",
    "    print(f\"Response: {sample['response'][:80]}...\" if len(sample['response']) > 80 else f\"Response: {sample['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74768e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted 200 samples\n",
      "\n",
      "Template distribution:\n",
      "  Template 1: 44 samples (22.0%)\n",
      "  Template 2: 48 samples (24.0%)\n",
      "  Template 3: 37 samples (18.5%)\n",
      "  Template 4: 32 samples (16.0%)\n",
      "  Template 5: 39 samples (19.5%)\n"
     ]
    }
   ],
   "source": [
    "# Format data with template randomization\n",
    "random.seed(42)\n",
    "\n",
    "formatted_texts = []\n",
    "template_distribution = {i: 0 for i in range(len(INSTRUCTION_TEMPLATES))}\n",
    "\n",
    "for sample in data:\n",
    "    if CONFIG[\"randomize_templates\"]:\n",
    "        template_idx = random.randint(0, len(INSTRUCTION_TEMPLATES) - 1)\n",
    "    else:\n",
    "        template_idx = 0\n",
    "    \n",
    "    formatted = format_instruction_sample(sample, template_idx=template_idx, randomize=True)\n",
    "    formatted_texts.append(formatted)\n",
    "    template_distribution[template_idx] += 1\n",
    "\n",
    "print(f\"Formatted {len(formatted_texts)} samples\")\n",
    "print(f\"\\nTemplate distribution:\")\n",
    "for idx, count in template_distribution.items():\n",
    "    print(f\"  Template {idx+1}: {count} samples ({100*count/len(formatted_texts):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da4dd2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 180\n",
      "Eval samples: 20\n"
     ]
    }
   ],
   "source": [
    "# Split into train/eval\n",
    "random.shuffle(formatted_texts)\n",
    "\n",
    "split_idx = int(len(formatted_texts) * CONFIG[\"train_split\"])\n",
    "train_texts = formatted_texts[:split_idx]\n",
    "eval_texts = formatted_texts[split_idx:]\n",
    "\n",
    "print(f\"Train samples: {len(train_texts)}\")\n",
    "print(f\"Eval samples: {len(eval_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e35d08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input shape: torch.Size([180, 512])\n",
      "Eval input shape: torch.Size([20, 512])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "def tokenize_texts(texts, tokenizer, max_length):\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "\n",
    "train_tokenized = tokenize_texts(train_texts, tokenizer, CONFIG[\"max_length\"])\n",
    "eval_tokenized = tokenize_texts(eval_texts, tokenizer, CONFIG[\"max_length\"])\n",
    "\n",
    "print(f\"Train input shape: {train_tokenized['input_ids'].shape}\")\n",
    "print(f\"Eval input shape: {eval_tokenized['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc0c56fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 180\n",
      "})\n",
      "Eval dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 20\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset objects\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": train_tokenized[\"input_ids\"].tolist(),\n",
    "    \"attention_mask\": train_tokenized[\"attention_mask\"].tolist(),\n",
    "    \"labels\": train_tokenized[\"labels\"].tolist(),\n",
    "})\n",
    "\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": eval_tokenized[\"input_ids\"].tolist(),\n",
    "    \"attention_mask\": eval_tokenized[\"attention_mask\"].tolist(),\n",
    "    \"labels\": eval_tokenized[\"labels\"].tolist(),\n",
    "})\n",
    "\n",
    "print(f\"Train dataset: {train_dataset}\")\n",
    "print(f\"Eval dataset: {eval_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f393fe9d",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "126d95b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured.\n",
      "Training on: CPU\n"
     ]
    }
   ],
   "source": [
    "# Enable gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    \n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    \n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # CPU training\n",
    "    use_cpu=True,\n",
    "    # bf16=True,  # Uncomment for GPU\n",
    "    \n",
    "    optim=\"adamw_torch\",\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured.\")\n",
    "print(f\"Training on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15e35713",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Data collator and trainer\u001b[39;00m\n\u001b[32m      2\u001b[39m data_collator = DataCollatorForLanguageModeling(\n\u001b[32m      3\u001b[39m     tokenizer=tokenizer,\n\u001b[32m      4\u001b[39m     mlm=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m      5\u001b[39m )\n\u001b[32m      7\u001b[39m trainer = Trainer(\n\u001b[32m      8\u001b[39m     model=model,\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     args=\u001b[43mtraining_args\u001b[49m,\n\u001b[32m     10\u001b[39m     train_dataset=train_dataset,\n\u001b[32m     11\u001b[39m     eval_dataset=eval_dataset,\n\u001b[32m     12\u001b[39m     data_collator=data_collator,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrainer initialized.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'training_args' is not defined"
     ]
    }
   ],
   "source": [
    "# Data collator and trainer\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0dfe92",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Test Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c79aa918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL RESPONSES (Before Stage 2 Training)\n",
      "============================================================\n",
      "\n",
      "--- Test 1 ---\n",
      "Prompt preview: Below is an instruction that describes a task. Write a respo...\n",
      "Response: What is the capital of France?\n",
      "\n",
      "### Response:\n",
      "\n",
      "What is the capital of France?\n",
      "\n",
      "### Response:\n",
      "\n",
      "What is the capital of France?\n",
      "\n",
      "### Response:\n",
      "\n",
      "What is t...\n",
      "\n",
      "--- Test 2 ---\n",
      "Prompt preview: Instruction: Translate the following text to Spanish.\n",
      "Hello,...\n",
      "Response: Hello, I am an educator. I am studying Spanish for my PhD and I am doing research on Spanish for a year.\n",
      "\n",
      "I am currently studying Spanish for my PhD a...\n",
      "\n",
      "--- Test 3 ---\n",
      "Prompt preview: <|im_start|>system\n",
      "You are a helpful assistant.\n",
      "<|im_end|>\n",
      "<...\n",
      "Response: Write a haiku about the computer.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant_user\n",
      "Write a haiku about the user.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant_user_user\n",
      "<|im_...\n"
     ]
    }
   ],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_new_tokens=128):\n",
    "    \"\"\"Generate response from model.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Instruction-format test prompts\n",
    "test_prompts = [\n",
    "    \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "What is the capital of France?\n",
    "\n",
    "### Response:\"\"\",\n",
    "\n",
    "    \"\"\"Instruction: Translate the following text to Spanish.\n",
    "Hello, how are you?\n",
    "\n",
    "Response:\"\"\",\n",
    "\n",
    "    \"\"\"<|im_start|>system\n",
    "You are a helpful assistant.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "Write a haiku about technology.\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL RESPONSES (Before Stage 2 Training)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "before_responses = {}\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\n--- Test {i+1} ---\")\n",
    "    print(f\"Prompt preview: {prompt[:60]}...\")\n",
    "    response = generate_response(model, tokenizer, prompt)\n",
    "    answer = response[len(prompt):].strip() if response.startswith(prompt) else response\n",
    "    print(f\"Response: {answer[:150]}...\" if len(answer) > 150 else f\"Response: {answer}\")\n",
    "    before_responses[i] = answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d210c107",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Train the Model\n",
    "\n",
    "üöÄ **Stage 2 Training - Instruction Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39360b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"Starting Stage 2 training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Training complete!\")\n",
    "print(f\"Total steps: {train_result.global_step}\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70589ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Eval loss: {eval_results['eval_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f96c471",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Test After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e15d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL RESPONSES (After Stage 2 Training)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "after_responses = {}\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\n--- Test {i+1} ---\")\n",
    "    print(f\"Prompt preview: {prompt[:60]}...\")\n",
    "    response = generate_response(model, tokenizer, prompt)\n",
    "    answer = response[len(prompt):].strip() if response.startswith(prompt) else response\n",
    "    print(f\"Response: {answer[:150]}...\" if len(answer) > 150 else f\"Response: {answer}\")\n",
    "    after_responses[i] = answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586bdaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare before and after\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARISON: Before vs After Stage 2\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(len(test_prompts)):\n",
    "    print(f\"\\n--- Test {i+1} ---\")\n",
    "    print(f\"Before: {before_responses[i][:100]}...\" if len(before_responses[i]) > 100 else f\"Before: {before_responses[i]}\")\n",
    "    print(f\"After:  {after_responses[i][:100]}...\" if len(after_responses[i]) > 100 else f\"After:  {after_responses[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb5421",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Paraphrase Robustness Test\n",
    "\n",
    "üîë **This is the key test for Stage 2**: Can the model handle the same instruction phrased differently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158482b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paraphrase robustness test\n",
    "paraphrase_tests = [\n",
    "    # Test 1: Capital question\n",
    "    [\n",
    "        \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "What is the capital of France?\n",
    "\n",
    "### Response:\"\"\",\n",
    "        \"\"\"Instruction: Tell me the capital city of France.\n",
    "\n",
    "Response:\"\"\",\n",
    "        \"\"\"User: Can you tell me which city is the capital of France?\n",
    "\n",
    "Assistant:\"\"\",\n",
    "    ],\n",
    "    \n",
    "    # Test 2: Translation\n",
    "    [\n",
    "        \"\"\"### Instruction:\n",
    "Translate 'Hello' to Spanish.\n",
    "\n",
    "### Response:\"\"\",\n",
    "        \"\"\"Instruction: How do you say 'Hello' in Spanish?\n",
    "\n",
    "Response:\"\"\",\n",
    "        \"\"\"User: Give me the Spanish word for 'Hello'.\n",
    "\n",
    "Assistant:\"\"\",\n",
    "    ],\n",
    "    \n",
    "    # Test 3: Math\n",
    "    [\n",
    "        \"\"\"### Instruction:\n",
    "Calculate 15 + 27.\n",
    "\n",
    "### Response:\"\"\",\n",
    "        \"\"\"Instruction: What is the sum of 15 and 27?\n",
    "\n",
    "Response:\"\"\",\n",
    "        \"\"\"User: Add 15 to 27 and tell me the result.\n",
    "\n",
    "Assistant:\"\"\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PARAPHRASE ROBUSTNESS TEST\")\n",
    "print(\"(Stage 2 should handle different phrasings of the same question)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for test_idx, variants in enumerate(paraphrase_tests):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Test {test_idx + 1}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    for var_idx, prompt in enumerate(variants):\n",
    "        response = generate_response(model, tokenizer, prompt)\n",
    "        answer = response[len(prompt):].strip() if response.startswith(prompt) else response\n",
    "        \n",
    "        print(f\"\\nVariant {var_idx + 1}: {prompt.split(chr(10))[0][:40]}...\")\n",
    "        print(f\"Response: {answer[:80]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a77049a",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a3b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "output_path = Path(CONFIG[\"output_dir\"]) / \"model\"\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(output_path)\n",
    "tokenizer.save_pretrained(output_path)\n",
    "\n",
    "print(f\"Model saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58450fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save responses for comparison\n",
    "import json\n",
    "\n",
    "responses_path = Path(CONFIG[\"output_dir\"]) / \"responses.json\"\n",
    "responses_data = {\n",
    "    \"stage\": 2,\n",
    "    \"model\": CONFIG[\"model_name\"],\n",
    "    \"before_responses\": before_responses,\n",
    "    \"after_responses\": after_responses,\n",
    "}\n",
    "\n",
    "with open(responses_path, \"w\") as f:\n",
    "    json.dump(responses_data, f, indent=2)\n",
    "\n",
    "print(f\"Responses saved to: {responses_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93caf4c4",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Stage 2 Complete!\n",
    "\n",
    "### What we verified:\n",
    "- ‚úÖ Model follows instruction format\n",
    "- ‚úÖ Same task works across multiple phrasings (paraphrase robustness)\n",
    "- ‚úÖ General instruction-following improved\n",
    "- ‚ö†Ô∏è Model may still hallucinate (expected, needs RLHF for full fix)\n",
    "\n",
    "### Key Observations:\n",
    "- Template randomization helped prevent overfitting to specific formats\n",
    "- Model now generalizes better to unseen instruction styles\n",
    "\n",
    "### Next Step: Stage 3 - LoRA/QLoRA\n",
    "In Stage 3, we'll make training memory-efficient using LoRA adapters.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
