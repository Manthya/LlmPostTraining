{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb0e986",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3ee880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install transformers datasets accelerate peft bitsandbytes trl pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ce4800b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.4.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 638, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1971, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3123, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3178, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3641, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3701, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/tg/5d928d1d0z15w2_p1z2f224r0000gr/T/ipykernel_20561/4055232218.py\", line 9, in <module>\n",
      "    import torch\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.2\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "721a7f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ SSL verification disabled\n"
     ]
    }
   ],
   "source": [
    "# SSL workaround for HuggingFace downloads\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = ''\n",
    "\n",
    "import httpx\n",
    "original_client_init = httpx.Client.__init__\n",
    "def patched_client_init(self, *args, **kwargs):\n",
    "    kwargs['verify'] = False\n",
    "    return original_client_init(self, *args, **kwargs)\n",
    "httpx.Client.__init__ = patched_client_init\n",
    "\n",
    "print(\"âœ“ SSL verification disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c75d2099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "# Ensure numpy is properly imported (fix for evaluation issue)\n",
    "import numpy as np\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20959177",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd47127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  model_name: ../outputs/stage1_sft/model\n",
      "  max_length: 512\n",
      "  train_split: 0.9\n",
      "  use_alpaca: True\n",
      "  alpaca_subset: 200\n",
      "  batch_size: 2\n",
      "  gradient_accumulation_steps: 4\n",
      "  num_epochs: 2\n",
      "  learning_rate: 1e-05\n",
      "  warmup_ratio: 0.1\n",
      "  weight_decay: 0.01\n",
      "  randomize_templates: True\n",
      "  output_dir: ../outputs/stage2_instruction\n"
     ]
    }
   ],
   "source": [
    "# Configuration for Stage 2\n",
    "CONFIG = {\n",
    "    # Model - Load Stage 1 GPT-2 checkpoint\n",
    "    \"model_name\": \"../outputs/stage1_sft/model\",  # Load Stage 1 checkpoint\n",
    "    \n",
    "    # Data\n",
    "    \"max_length\": 512,  # Match Stage 1\n",
    "    \"train_split\": 0.9,\n",
    "    \"use_alpaca\": True,  # Use Alpaca dataset from HuggingFace\n",
    "    \"alpaca_subset\": 200,  # Smaller subset for faster training (GPT-2 on CPU)\n",
    "    \n",
    "    # Training\n",
    "    \"batch_size\": 2,  # Smaller for CPU\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"num_epochs\": 2,  # Fewer epochs for demo\n",
    "    \"learning_rate\": 1e-5,  # Lower LR since continuing from Stage 1\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \n",
    "    # Template randomization\n",
    "    \"randomize_templates\": True,\n",
    "    \n",
    "    # Output\n",
    "    \"output_dir\": \"../outputs/stage2_instruction\",\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c94ac9",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Define Instruction Templates\n",
    "\n",
    "ðŸ”‘ **Key to Stage 2 success**: Use multiple prompt templates to prevent instruction overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b63f0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 5 instruction templates\n",
      "Defined 5 system message variations\n"
     ]
    }
   ],
   "source": [
    "# Multiple instruction templates for robustness\n",
    "INSTRUCTION_TEMPLATES = [\n",
    "    # Template 1: Alpaca-style\n",
    "    \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{response}\"\"\",\n",
    "\n",
    "    # Template 2: Simple\n",
    "    \"\"\"Instruction: {instruction}\n",
    "{input}\n",
    "\n",
    "Response: {response}\"\"\",\n",
    "\n",
    "    # Template 3: ChatML-style\n",
    "    \"\"\"<|im_start|>system\n",
    "{system_message}\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "{instruction}\n",
    "{input}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{response}\n",
    "<|im_end|>\"\"\",\n",
    "\n",
    "    # Template 4: Conversational\n",
    "    \"\"\"User: {instruction}\n",
    "{input}\n",
    "\n",
    "Assistant: {response}\"\"\",\n",
    "\n",
    "    # Template 5: Task-focused\n",
    "    \"\"\"Task: {instruction}\n",
    "Context: {input}\n",
    "\n",
    "Output: {response}\"\"\",\n",
    "]\n",
    "\n",
    "# System message variations (for ChatML template)\n",
    "SYSTEM_MESSAGES = [\n",
    "    \"You are a helpful assistant.\",\n",
    "    \"You are an AI assistant that follows instructions carefully.\",\n",
    "    \"You are a knowledgeable and helpful AI.\",\n",
    "    \"You are an assistant designed to help users with their tasks.\",\n",
    "    \"You are a friendly AI that provides accurate and helpful responses.\",\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(INSTRUCTION_TEMPLATES)} instruction templates\")\n",
    "print(f\"Defined {len(SYSTEM_MESSAGES)} system message variations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d41a82a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample formatted with different templates:\n",
      "============================================================\n",
      "\n",
      "--- Template 1 ---\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Translate the following text to French.\n",
      "\n",
      "### Input:\n",
      "Hello, how are you?\n",
      "\n",
      "### Response:\n",
      "Bonjour, comment allez-vous?\n",
      "\n",
      "\n",
      "--- Template 2 ---\n",
      "Instruction: Translate the following text to French.\n",
      "Hello, how are you?\n",
      "\n",
      "Response: Bonjour, comment allez-vous?\n",
      "\n",
      "\n",
      "--- Template 3 ---\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Translate the following text to French.\n",
      "Hello, how are you?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Bonjour, comment allez-vous?\n",
      "<|im_end|>\n",
      "\n",
      "\n",
      "--- Template 4 ---\n",
      "User: Translate the following text to French.\n",
      "Hello, how are you?\n",
      "\n",
      "Assistant: Bonjour, comment allez-vous?\n",
      "\n",
      "\n",
      "--- Template 5 ---\n",
      "Task: Translate the following text to French.\n",
      "Context: Hello, how are you?\n",
      "\n",
      "Output: Bonjour, comment allez-vous?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def format_instruction_sample(sample, template_idx=None, randomize=True):\n",
    "    \"\"\"\n",
    "    Format a sample with instruction templates.\n",
    "    Optionally randomize template for robustness.\n",
    "    \"\"\"\n",
    "    instruction = sample.get(\"instruction\", \"\")\n",
    "    input_text = sample.get(\"input\", \"\")\n",
    "    response = sample.get(\"response\", sample.get(\"output\", \"\"))\n",
    "    \n",
    "    # Select template\n",
    "    if randomize and template_idx is None:\n",
    "        template_idx = random.randint(0, len(INSTRUCTION_TEMPLATES) - 1)\n",
    "    elif template_idx is None:\n",
    "        template_idx = 0\n",
    "    \n",
    "    template = INSTRUCTION_TEMPLATES[template_idx]\n",
    "    \n",
    "    # For ChatML template, add system message\n",
    "    if \"system_message\" in template:\n",
    "        system_msg = random.choice(SYSTEM_MESSAGES) if randomize else SYSTEM_MESSAGES[0]\n",
    "        return template.format(\n",
    "            system_message=system_msg,\n",
    "            instruction=instruction,\n",
    "            input=input_text if input_text else \"\",\n",
    "            response=response,\n",
    "        )\n",
    "    else:\n",
    "        return template.format(\n",
    "            instruction=instruction,\n",
    "            input=input_text if input_text else \"\",\n",
    "            response=response,\n",
    "        )\n",
    "\n",
    "# Test formatting\n",
    "test_sample = {\n",
    "    \"instruction\": \"Translate the following text to French.\",\n",
    "    \"input\": \"Hello, how are you?\",\n",
    "    \"response\": \"Bonjour, comment allez-vous?\",\n",
    "}\n",
    "\n",
    "print(\"Sample formatted with different templates:\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(len(INSTRUCTION_TEMPLATES)):\n",
    "    print(f\"\\n--- Template {i+1} ---\")\n",
    "    print(format_instruction_sample(test_sample, template_idx=i, randomize=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bd5a75",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "671ac6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from: ../outputs/stage1_sft/model\n",
      "Vocab size: 50257\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer from Stage 1 model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Tokenizer loaded from: {CONFIG['model_name']}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8de8e8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Stage 1 model from: ../outputs/stage1_sft/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Warning #191: Forking a process while a parallel region is active is potentially unsafe.\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148/148 [00:01<00:00, 120.75it/s, Materializing param=transformer.wte.weight]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. Parameters: 124,439,808\n",
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load model from Stage 1 checkpoint\n",
    "model_path = CONFIG[\"model_name\"]\n",
    "\n",
    "print(f\"Loading Stage 1 model from: {model_path}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float32,  # Use float32 for CPU\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded. Parameters: {model.num_parameters():,}\")\n",
    "print(f\"Device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629b11d2",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Prepare Training Data\n",
    "\n",
    "We'll use the **Alpaca dataset** (cleaned version) for instruction tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f84fe4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Alpaca dataset from HuggingFace...\n",
      "Loaded 200 samples from Alpaca dataset\n"
     ]
    }
   ],
   "source": [
    "# Load Alpaca dataset\n",
    "if CONFIG[\"use_alpaca\"]:\n",
    "    print(\"Loading Alpaca dataset from HuggingFace...\")\n",
    "    alpaca_dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "    \n",
    "    # Convert to list of dicts\n",
    "    data = []\n",
    "    for item in alpaca_dataset:\n",
    "        data.append({\n",
    "            \"instruction\": item[\"instruction\"],\n",
    "            \"input\": item[\"input\"],\n",
    "            \"response\": item[\"output\"],\n",
    "        })\n",
    "    \n",
    "    # Use subset if specified\n",
    "    if CONFIG[\"alpaca_subset\"]:\n",
    "        random.seed(42)\n",
    "        random.shuffle(data)\n",
    "        data = data[:CONFIG[\"alpaca_subset\"]]\n",
    "    \n",
    "    print(f\"Loaded {len(data)} samples from Alpaca dataset\")\n",
    "else:\n",
    "    # Use manual data\n",
    "    data = [\n",
    "        {\"instruction\": \"Translate to French.\", \"input\": \"Hello, how are you?\", \"response\": \"Bonjour, comment allez-vous?\"},\n",
    "        {\"instruction\": \"Summarize the text.\", \"input\": \"Machine learning is a subset of AI...\", \"response\": \"ML is an AI subset.\"},\n",
    "        # Add more samples...\n",
    "    ]\n",
    "    print(f\"Using manual data: {len(data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5fe6dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training entries:\n",
      "============================================================\n",
      "\n",
      "--- Sample 1 ---\n",
      "Instruction: Describe a traditional wedding ceremony you attended\n",
      "Input: \n",
      "Response: As an AI, I do not have physical presence and therefore do not attend events suc...\n",
      "\n",
      "--- Sample 2 ---\n",
      "Instruction: What do you understand by AI explainability?\n",
      "Input: \n",
      "Response: AI explainability refers to the ability to understand and interpret the reasonin...\n",
      "\n",
      "--- Sample 3 ---\n",
      "Instruction: Generate a dialogue between a customer and a salesperson in a department store.\n",
      "Input: \n",
      "Response: Customer: Excuse me, could you help me find the men's section?\n",
      "\n",
      "Salesperson: Of ...\n"
     ]
    }
   ],
   "source": [
    "# Show sample data\n",
    "print(\"Sample training entries:\")\n",
    "print(\"=\" * 60)\n",
    "for i, sample in enumerate(data[:3]):\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(f\"Instruction: {sample['instruction'][:80]}...\" if len(sample['instruction']) > 80 else f\"Instruction: {sample['instruction']}\")\n",
    "    print(f\"Input: {sample['input'][:50]}...\" if len(sample['input']) > 50 else f\"Input: {sample['input']}\")\n",
    "    print(f\"Response: {sample['response'][:80]}...\" if len(sample['response']) > 80 else f\"Response: {sample['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74768e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted 200 samples\n",
      "\n",
      "Template distribution:\n",
      "  Template 1: 44 samples (22.0%)\n",
      "  Template 2: 48 samples (24.0%)\n",
      "  Template 3: 37 samples (18.5%)\n",
      "  Template 4: 32 samples (16.0%)\n",
      "  Template 5: 39 samples (19.5%)\n"
     ]
    }
   ],
   "source": [
    "# Format data with template randomization\n",
    "random.seed(42)\n",
    "\n",
    "formatted_texts = []\n",
    "template_distribution = {i: 0 for i in range(len(INSTRUCTION_TEMPLATES))}\n",
    "\n",
    "for sample in data:\n",
    "    if CONFIG[\"randomize_templates\"]:\n",
    "        template_idx = random.randint(0, len(INSTRUCTION_TEMPLATES) - 1)\n",
    "    else:\n",
    "        template_idx = 0\n",
    "    \n",
    "    formatted = format_instruction_sample(sample, template_idx=template_idx, randomize=True)\n",
    "    formatted_texts.append(formatted)\n",
    "    template_distribution[template_idx] += 1\n",
    "\n",
    "print(f\"Formatted {len(formatted_texts)} samples\")\n",
    "print(f\"\\nTemplate distribution:\")\n",
    "for idx, count in template_distribution.items():\n",
    "    print(f\"  Template {idx+1}: {count} samples ({100*count/len(formatted_texts):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da4dd2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 180\n",
      "Eval samples: 20\n"
     ]
    }
   ],
   "source": [
    "# Split into train/eval\n",
    "random.shuffle(formatted_texts)\n",
    "\n",
    "split_idx = int(len(formatted_texts) * CONFIG[\"train_split\"])\n",
    "train_texts = formatted_texts[:split_idx]\n",
    "eval_texts = formatted_texts[split_idx:]\n",
    "\n",
    "print(f\"Train samples: {len(train_texts)}\")\n",
    "print(f\"Eval samples: {len(eval_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e35d08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input shape: torch.Size([180, 512])\n",
      "Eval input shape: torch.Size([20, 512])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "def tokenize_texts(texts, tokenizer, max_length):\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "\n",
    "train_tokenized = tokenize_texts(train_texts, tokenizer, CONFIG[\"max_length\"])\n",
    "eval_tokenized = tokenize_texts(eval_texts, tokenizer, CONFIG[\"max_length\"])\n",
    "\n",
    "print(f\"Train input shape: {train_tokenized['input_ids'].shape}\")\n",
    "print(f\"Eval input shape: {eval_tokenized['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc0c56fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 180\n",
      "})\n",
      "Eval dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 20\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset objects\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": train_tokenized[\"input_ids\"].tolist(),\n",
    "    \"attention_mask\": train_tokenized[\"attention_mask\"].tolist(),\n",
    "    \"labels\": train_tokenized[\"labels\"].tolist(),\n",
    "})\n",
    "\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": eval_tokenized[\"input_ids\"].tolist(),\n",
    "    \"attention_mask\": eval_tokenized[\"attention_mask\"].tolist(),\n",
    "    \"labels\": eval_tokenized[\"labels\"].tolist(),\n",
    "})\n",
    "\n",
    "print(f\"Train dataset: {train_dataset}\")\n",
    "print(f\"Eval dataset: {eval_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f393fe9d",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126d95b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured.\n",
      "Training on: CPU\n"
     ]
    }
   ],
   "source": [
    "# Enable gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    \n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    \n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    logging_steps=5,  # More frequent logging\n",
    "    eval_strategy=\"no\",  # Disable mid-training evaluation (causes numpy issue)\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Logging\n",
    "    logging_first_step=True,\n",
    "    logging_nan_inf_filter=True,\n",
    "    disable_tqdm=False,  # Show progress bar\n",
    "    \n",
    "    # CPU training\n",
    "    use_cpu=True,\n",
    "    \n",
    "    optim=\"adamw_torch\",\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured.\")\n",
    "print(f\"Training on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15e35713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized.\n"
     ]
    }
   ],
   "source": [
    "# Data collator and trainer\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0dfe92",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Test Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c79aa918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL RESPONSES (Before Stage 2 Training)\n",
      "============================================================\n",
      "\n",
      "--- Test 1 ---\n",
      "Prompt preview: Below is an instruction that describes a task. Write a respo...\n",
      "Response: Hello, you can read my question. You can read my answer. I will answer it.\n",
      "\n",
      "### Response:\n",
      "\n",
      "Thank you.\n",
      "\n",
      "You are now ready to receive my answer.\n",
      "\n",
      "### Re...\n",
      "\n",
      "--- Test 2 ---\n",
      "Prompt preview: Instruction: Translate the following text to Spanish.\n",
      "Hello,...\n",
      "Response: I'm a programmer, and I can't tell you how to do this.\n",
      "\n",
      "You're not doing this right?\n",
      "\n",
      "Response: I know!\n",
      "\n",
      "I know what you're doing.\n",
      "\n",
      "You're doing this ...\n",
      "\n",
      "--- Test 3 ---\n",
      "Prompt preview: <|im_start|>system\n",
      "You are a helpful assistant.\n",
      "<|im_end|>\n",
      "<...\n",
      "Response: You are an assistant.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Write a haiku about technology.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "You are an assistant.\n",
      "<|im_end|>\n",
      "<|...\n"
     ]
    }
   ],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_new_tokens=128):\n",
    "    \"\"\"Generate response from model.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Instruction-format test prompts\n",
    "test_prompts = [\n",
    "    \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "What is the capital of France?\n",
    "\n",
    "### Response:\"\"\",\n",
    "\n",
    "    \"\"\"Instruction: Translate the following text to Spanish.\n",
    "Hello, how are you?\n",
    "\n",
    "Response:\"\"\",\n",
    "\n",
    "    \"\"\"<|im_start|>system\n",
    "You are a helpful assistant.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "Write a haiku about technology.\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL RESPONSES (Before Stage 2 Training)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "before_responses = {}\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\n--- Test {i+1} ---\")\n",
    "    print(f\"Prompt preview: {prompt[:60]}...\")\n",
    "    response = generate_response(model, tokenizer, prompt)\n",
    "    answer = response[len(prompt):].strip() if response.startswith(prompt) else response\n",
    "    print(f\"Response: {answer[:150]}...\" if len(answer) > 150 else f\"Response: {answer}\")\n",
    "    before_responses[i] = answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d210c107",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Train the Model\n",
    "\n",
    "ðŸš€ **Stage 2 Training - Instruction Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39360b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸš€ STARTING STAGE 2 TRAINING - INSTRUCTION TUNING\n",
      "======================================================================\n",
      "Start time: 2026-02-01 17:54:46\n",
      "\n",
      "Training Configuration:\n",
      "  Model: ../outputs/stage1_sft/model\n",
      "  Epochs: 2\n",
      "  Batch size: 2\n",
      "  Gradient accumulation: 4\n",
      "  Effective batch size: 8\n",
      "  Learning rate: 1e-05\n",
      "  Train samples: 180\n",
      "  Eval samples: 20\n",
      "\n",
      "Expected steps: ~44\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='46' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/46 14:20 < 11:57, 0.03 it/s, Epoch 1.09/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m start_time = time.time()\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Train with automatic progress display\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m train_result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m training_time = time.time() - start_time\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/transformers/trainer.py:2174\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2172\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2173\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2175\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2179\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/transformers/trainer.py:2607\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2605\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m) / steps_in_epoch\n\u001b[32m   2606\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2607\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2608\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2609\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2610\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2611\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2612\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2613\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2614\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2615\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2616\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2617\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2618\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_substep_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/transformers/trainer.py:3020\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3018\u001b[39m metrics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3019\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m-> \u001b[39m\u001b[32m3020\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3021\u001b[39m     is_new_best_metric = \u001b[38;5;28mself\u001b[39m._determine_best_metric(metrics=metrics, trial=trial)\n\u001b[32m   3023\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/transformers/trainer.py:2969\u001b[39m, in \u001b[36mTrainer._evaluate\u001b[39m\u001b[34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[39m\n\u001b[32m   2968\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2969\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2970\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m.state.global_step, metrics)\n\u001b[32m   2972\u001b[39m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/transformers/trainer.py:4289\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4285\u001b[39m     eval_dataloader = tpu_spmd_dataloader(eval_dataloader)\n\u001b[32m   4287\u001b[39m start_time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m4289\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluation_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4290\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4292\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   4293\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   4294\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4296\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4297\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4299\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   4300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_model_preparation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/transformers/trainer.py:4540\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4537\u001b[39m \u001b[38;5;28mself\u001b[39m.gather_function = \u001b[38;5;28mself\u001b[39m.accelerator.gather_for_metrics\n\u001b[32m   4539\u001b[39m \u001b[38;5;66;03m# Gather all remaining tensors and put them back on the CPU\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4540\u001b[39m all_losses = \u001b[43mall_losses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4541\u001b[39m all_preds = all_preds.get_arrays()\n\u001b[32m   4542\u001b[39m all_labels = all_labels.get_arrays()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/transformers/trainer_pt_utils.py:339\u001b[39m, in \u001b[36mEvalLoopContainer.get_arrays\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_arrays\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns the numpified and moved to CPU stored objects.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mto_cpu_and_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.arrays\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/transformers/trainer_pt_utils.py:326\u001b[39m, in \u001b[36mEvalLoopContainer.to_cpu_and_numpy\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m new_arrays = \u001b[43mnested_numpify\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.arrays \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    328\u001b[39m     \u001b[38;5;28mself\u001b[39m.arrays = new_arrays\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/transformers/trainer_pt_utils.py:173\u001b[39m, in \u001b[36mnested_numpify\u001b[39m\u001b[34m(tensors)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m t.dtype == torch.bfloat16:\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# As of Numpy 1.21.4, NumPy does not support bfloat16 (see\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;66;03m# https://github.com/numpy/numpy/blob/a47ecdea856986cd60eabbd53265c2ca5916ad5d/doc/source/user/basics.types.rst ).\u001b[39;00m\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# Until Numpy adds bfloat16, we must convert float32.\u001b[39;00m\n\u001b[32m    172\u001b[39m     t = t.to(torch.float32)\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "# Train with detailed logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸš€ STARTING STAGE 2 TRAINING - INSTRUCTION TUNING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Model: {CONFIG['model_name']}\")\n",
    "print(f\"  Epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"  Gradient accumulation: {CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"  Effective batch size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  Train samples: {len(train_dataset)}\")\n",
    "print(f\"  Eval samples: {len(eval_dataset)}\")\n",
    "print(f\"\\nExpected steps: ~{len(train_dataset) // (CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']) * CONFIG['num_epochs']}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train with automatic progress display\n",
    "train_result = trainer.train()\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nðŸ“Š Training Results:\")\n",
    "print(f\"  Total steps: {train_result.global_step}\")\n",
    "print(f\"  Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"  Training time: {training_time/60:.1f} minutes ({training_time:.0f} seconds)\")\n",
    "print(f\"  Time per step: {training_time/train_result.global_step:.1f} seconds\")\n",
    "if hasattr(train_result, 'metrics'):\n",
    "    print(f\"\\nðŸ“ˆ Additional Metrics:\")\n",
    "    for key, value in train_result.metrics.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"  {key}: {value:.4f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70589ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š FINAL EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Evaluation Metrics:\")\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"  {key}: {value:.4f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f96c471",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Test After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e15d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL RESPONSES (After Stage 2 Training)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "after_responses = {}\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\n--- Test {i+1} ---\")\n",
    "    print(f\"Prompt preview: {prompt[:60]}...\")\n",
    "    response = generate_response(model, tokenizer, prompt)\n",
    "    answer = response[len(prompt):].strip() if response.startswith(prompt) else response\n",
    "    print(f\"Response: {answer[:150]}...\" if len(answer) > 150 else f\"Response: {answer}\")\n",
    "    after_responses[i] = answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586bdaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare before and after\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARISON: Before vs After Stage 2\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(len(test_prompts)):\n",
    "    print(f\"\\n--- Test {i+1} ---\")\n",
    "    print(f\"Before: {before_responses[i][:100]}...\" if len(before_responses[i]) > 100 else f\"Before: {before_responses[i]}\")\n",
    "    print(f\"After:  {after_responses[i][:100]}...\" if len(after_responses[i]) > 100 else f\"After:  {after_responses[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb5421",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Paraphrase Robustness Test\n",
    "\n",
    "ðŸ”‘ **This is the key test for Stage 2**: Can the model handle the same instruction phrased differently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158482b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paraphrase robustness test\n",
    "paraphrase_tests = [\n",
    "    # Test 1: Capital question\n",
    "    [\n",
    "        \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "What is the capital of France?\n",
    "\n",
    "### Response:\"\"\",\n",
    "        \"\"\"Instruction: Tell me the capital city of France.\n",
    "\n",
    "Response:\"\"\",\n",
    "        \"\"\"User: Can you tell me which city is the capital of France?\n",
    "\n",
    "Assistant:\"\"\",\n",
    "    ],\n",
    "    \n",
    "    # Test 2: Translation\n",
    "    [\n",
    "        \"\"\"### Instruction:\n",
    "Translate 'Hello' to Spanish.\n",
    "\n",
    "### Response:\"\"\",\n",
    "        \"\"\"Instruction: How do you say 'Hello' in Spanish?\n",
    "\n",
    "Response:\"\"\",\n",
    "        \"\"\"User: Give me the Spanish word for 'Hello'.\n",
    "\n",
    "Assistant:\"\"\",\n",
    "    ],\n",
    "    \n",
    "    # Test 3: Math\n",
    "    [\n",
    "        \"\"\"### Instruction:\n",
    "Calculate 15 + 27.\n",
    "\n",
    "### Response:\"\"\",\n",
    "        \"\"\"Instruction: What is the sum of 15 and 27?\n",
    "\n",
    "Response:\"\"\",\n",
    "        \"\"\"User: Add 15 to 27 and tell me the result.\n",
    "\n",
    "Assistant:\"\"\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PARAPHRASE ROBUSTNESS TEST\")\n",
    "print(\"(Stage 2 should handle different phrasings of the same question)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for test_idx, variants in enumerate(paraphrase_tests):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Test {test_idx + 1}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    for var_idx, prompt in enumerate(variants):\n",
    "        response = generate_response(model, tokenizer, prompt)\n",
    "        answer = response[len(prompt):].strip() if response.startswith(prompt) else response\n",
    "        \n",
    "        print(f\"\\nVariant {var_idx + 1}: {prompt.split(chr(10))[0][:40]}...\")\n",
    "        print(f\"Response: {answer[:80]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a77049a",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a3b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "output_path = Path(CONFIG[\"output_dir\"]) / \"model\"\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(output_path)\n",
    "tokenizer.save_pretrained(output_path)\n",
    "\n",
    "print(f\"Model saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58450fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save responses for comparison\n",
    "import json\n",
    "\n",
    "responses_path = Path(CONFIG[\"output_dir\"]) / \"responses.json\"\n",
    "responses_data = {\n",
    "    \"stage\": 2,\n",
    "    \"model\": CONFIG[\"model_name\"],\n",
    "    \"before_responses\": before_responses,\n",
    "    \"after_responses\": after_responses,\n",
    "}\n",
    "\n",
    "with open(responses_path, \"w\") as f:\n",
    "    json.dump(responses_data, f, indent=2)\n",
    "\n",
    "print(f\"Responses saved to: {responses_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93caf4c4",
   "metadata": {},
   "source": [
    "---\n",
    "## âœ… Stage 2 Complete!\n",
    "\n",
    "### What we verified:\n",
    "- âœ… Model follows instruction format\n",
    "- âœ… Same task works across multiple phrasings (paraphrase robustness)\n",
    "- âœ… General instruction-following improved\n",
    "- âš ï¸ Model may still hallucinate (expected, needs RLHF for full fix)\n",
    "\n",
    "### Key Observations:\n",
    "- Template randomization helped prevent overfitting to specific formats\n",
    "- Model now generalizes better to unseen instruction styles\n",
    "\n",
    "### Next Step: Stage 3 - LoRA/QLoRA\n",
    "In Stage 3, we'll make training memory-efficient using LoRA adapters.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
