{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caa04611",
   "metadata": {},
   "source": [
    "# üéØ InstructGPT Step 2 & 3: Reward Model + PPO\n",
    "\n",
    "Based on **\"Training language models to follow instructions with human feedback\"** (Ouyang et al., 2022)\n",
    "- Paper: https://arxiv.org/abs/2203.02155\n",
    "\n",
    "## RLHF Pipeline Overview\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Step 1: Supervised Fine-Tuning (SFT) ‚úÖ COMPLETED                      ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ See notebook 06_improved_training.ipynb                              ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Step 2: Reward Model Training ‚Üê THIS NOTEBOOK                          ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                      ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Load pre-trained reward model OR train from comparison data          ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Dataset: Anthropic HH-RLHF (public alternative to OpenAI data)       ‚îÇ\n",
    "‚îÇ  ‚Ä¢ RM learns: score(prompt, response) ‚Üí scalar reward                   ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Step 3: PPO Fine-Tuning ‚Üê THIS NOTEBOOK                                ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                              ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Use reward model as reward signal                                    ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Fine-tune SFT model with PPO                                         ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Add KL penalty: R = R_œà(x,y) - Œ≤¬∑log(œÄ(y|x)/p(y|x))                  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Mix in pretraining data to prevent forgetting                        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## Datasets Used\n",
    "\n",
    "| Paper Dataset | Public Alternative | Description |\n",
    "|---------------|-------------------|-------------|\n",
    "| OpenAI Comparisons | **Anthropic HH-RLHF** | Human preference pairs |\n",
    "| OpenAI Prompts | **OpenAssistant** | Instruction-following prompts |\n",
    "| - | **Stack Exchange Preferences** | Q&A ranking data |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d869b074",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f9604c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.4.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 638, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1971, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3123, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3178, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3641, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3701, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/tg/5d928d1d0z15w2_p1z2f224r0000gr/T/ipykernel_31729/3644757080.py\", line 13, in <module>\n",
      "    import torch\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.2.2\n",
      "Device: CPU\n",
      "TRL Available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# SSL workaround\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# TRL library for RLHF\n",
    "try:\n",
    "    from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "    from trl import RewardTrainer, RewardConfig\n",
    "    TRL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TRL_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è TRL not installed. Installing...\")\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"TRL Available: {TRL_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5526f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TRL if not available\n",
    "if not TRL_AVAILABLE:\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"trl\", \"-q\"])\n",
    "    from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "    from trl import RewardTrainer, RewardConfig\n",
    "    print(\"‚úÖ TRL installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "121ca953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer loaded: 50257 tokens\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "SFT_MODEL_PATH = \"../outputs/improved_training/merged\"  # Our SFT model from notebook 06\n",
    "BASE_MODEL_PATH = \"../models/gpt2\"\n",
    "OUTPUT_DIR = \"../outputs/rlhf_training\"\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úÖ Tokenizer loaded: {tokenizer.vocab_size} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d02354",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Reward Model Dataset (Anthropic HH-RLHF)\n",
    "\n",
    "The InstructGPT paper used internal OpenAI comparison data. We use the public **Anthropic HH-RLHF** dataset which contains:\n",
    "- Human preference pairs (chosen vs rejected responses)\n",
    "- ~170K training examples\n",
    "- Covers helpfulness and harmlessness\n",
    "\n",
    "Paper quote:\n",
    "> \"We collect a dataset of human-labeled comparisons between two model outputs on the same input.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c87a3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'Anthropic/hh-rlhf' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Anthropic HH-RLHF dataset...\n",
      "(This is the public equivalent of InstructGPT's comparison data)\n",
      "\n",
      "üìä Dataset Statistics:\n",
      "  Total examples: 160,800\n",
      "  Columns: ['chosen', 'rejected']\n"
     ]
    }
   ],
   "source": [
    "# Load Anthropic HH-RLHF dataset (similar to InstructGPT's comparison data)\n",
    "print(\"Loading Anthropic HH-RLHF dataset...\")\n",
    "print(\"(This is the public equivalent of InstructGPT's comparison data)\")\n",
    "\n",
    "# Fix SSL issues\n",
    "import httpx\n",
    "original_init = httpx.Client.__init__\n",
    "def patched_init(self, *args, **kwargs):\n",
    "    kwargs.setdefault('verify', False)\n",
    "    original_init(self, *args, **kwargs)\n",
    "httpx.Client.__init__ = patched_init\n",
    "\n",
    "hh_dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train\", trust_remote_code=True)\n",
    "\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"  Total examples: {len(hh_dataset):,}\")\n",
    "print(f\"  Columns: {hh_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38c61143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Sample Example:\n",
      "============================================================\n",
      "\n",
      "üü¢ CHOSEN (preferred by humans):\n",
      "\n",
      "\n",
      "Human: What are some cuss words in english?\n",
      "\n",
      "Assistant: Here‚Äôs an incomplete list.\n",
      "\n",
      "Ass, dick, bugger, crap, fuck, shit, bitch, turd, shithead, shitbag, scrotum, cunt, whore, fucker, shit-eating, cum, cumbucket, fucknugget, butthole, poop, jackass, cocksucker, asshole, goddamn, piss, sperm, blow, wank, jism, cum-sucking, masturbate, faggot, queer, jizz, jizz-licking, prostitute, slut, cheater, fornicator, floozy, wetback, Mexican, Hispanic, sodomite, midget, mama‚Äôs boy, faggot, pervert, queer,...\n",
      "\n",
      "üî¥ REJECTED:\n",
      "\n",
      "\n",
      "Human: What are some cuss words in english?\n",
      "\n",
      "Assistant: Here‚Äôs an incomplete list.\n",
      "\n",
      "Ass, dick, bugger, crap, fuck, shit, bitch, turd, shithead, shitbag, scrotum, cunt, whore, fucker, shit-eating, cum, cumbucket, fucknugget, butthole, poop, jackass, cocksucker, asshole, goddamn, piss, sperm, blow, wank, jism, cum-sucking, masturbate, faggot, queer, jizz, jizz-licking, prostitute, slut, cheater, fornicator, floozy, wetback, Mexican, Hispanic, sodomite, midget, mama‚Äôs boy, faggot, pervert, queer,...\n"
     ]
    }
   ],
   "source": [
    "# Examine dataset structure\n",
    "print(\"\\nüìù Sample Example:\")\n",
    "print(\"=\"*60)\n",
    "example = hh_dataset[0]\n",
    "print(f\"\\nüü¢ CHOSEN (preferred by humans):\")\n",
    "print(example['chosen'][:500] + \"...\")\n",
    "print(f\"\\nüî¥ REJECTED:\")\n",
    "print(example['rejected'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82aed191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Filtered dataset: 127,561 examples (from 160,800)\n",
      "  Using 5,000 samples for reward model training\n"
     ]
    }
   ],
   "source": [
    "# Filter dataset for GPT-2's capacity (shorter examples)\n",
    "def filter_for_gpt2(example):\n",
    "    \"\"\"Keep only short examples suitable for GPT-2.\"\"\"\n",
    "    chosen = example.get('chosen', '')\n",
    "    rejected = example.get('rejected', '')\n",
    "    \n",
    "    # Max length check (GPT-2 context: 1024)\n",
    "    if len(chosen) > 1500 or len(rejected) > 1500:\n",
    "        return False\n",
    "    \n",
    "    # Min length check\n",
    "    if len(chosen) < 50 or len(rejected) < 50:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "filtered_hh = hh_dataset.filter(filter_for_gpt2)\n",
    "print(f\"\\n‚úÖ Filtered dataset: {len(filtered_hh):,} examples (from {len(hh_dataset):,})\")\n",
    "\n",
    "# Take subset for training (similar to paper's ~33K comparisons)\n",
    "N_REWARD_SAMPLES = min(5000, len(filtered_hh))  # Scaled for GPT-2\n",
    "reward_dataset = filtered_hh.shuffle(seed=42).select(range(N_REWARD_SAMPLES))\n",
    "print(f\"  Using {N_REWARD_SAMPLES:,} samples for reward model training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d69eabd",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Option A: Load Pre-trained Reward Model\n",
    "\n",
    "Several pre-trained reward models are available:\n",
    "1. **OpenAssistant/reward-model-deberta-v3-large-v2** - Best public RM\n",
    "2. **weqweasdas/RM-Gemma-2B** - Gemma-based RM\n",
    "3. **berkeley-nest/Starling-RM-7B-alpha** - High-quality RM\n",
    "\n",
    "For GPT-2's scale, we'll use a smaller model or train our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b40be2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Will train custom reward model based on GPT-2\n",
      "   Following InstructGPT paper: RM initialized from SFT model\n"
     ]
    }
   ],
   "source": [
    "# We will train our own GPT-2 based reward model\n",
    "# This follows the InstructGPT paper more closely - using a model from the same family\n",
    "\n",
    "PRETRAINED_RM_LOADED = False\n",
    "print(\"üìù Will train custom reward model based on GPT-2\")\n",
    "print(\"   Following InstructGPT paper: RM initialized from SFT model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76680084",
   "metadata": {},
   "source": [
    "---\n",
    "## 3B. Option B: Train Custom Reward Model\n",
    "\n",
    "Following InstructGPT methodology:\n",
    "- Start from SFT model\n",
    "- Replace final layer with scalar reward head\n",
    "- Train on comparison pairs using Bradley-Terry model:\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = -\\mathbb{E}_{(x, y_w, y_l) \\sim D} [\\log \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l))]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e5d048f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPT2RewardModel class defined\n"
     ]
    }
   ],
   "source": [
    "class GPT2RewardModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Reward Model based on GPT-2.\n",
    "    Takes (prompt, response) and outputs scalar reward.\n",
    "    \n",
    "    InstructGPT paper:\n",
    "    \"We train a reward model (RM) to predict which model output\n",
    "    a human labeler would prefer.\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model_path):\n",
    "        super().__init__()\n",
    "        self.gpt2 = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_path,\n",
    "            torch_dtype=torch.float32,\n",
    "        )\n",
    "        self.reward_head = nn.Linear(self.gpt2.config.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Get last hidden state\n",
    "        outputs = self.gpt2(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        \n",
    "        # Use last token's hidden state for reward\n",
    "        last_hidden = outputs.hidden_states[-1]\n",
    "        \n",
    "        # Get position of last non-padding token\n",
    "        if attention_mask is not None:\n",
    "            last_idx = attention_mask.sum(dim=1) - 1\n",
    "            batch_indices = torch.arange(last_hidden.size(0))\n",
    "            last_token_hidden = last_hidden[batch_indices, last_idx]\n",
    "        else:\n",
    "            last_token_hidden = last_hidden[:, -1, :]\n",
    "        \n",
    "        # Compute scalar reward\n",
    "        reward = self.reward_head(last_token_hidden)\n",
    "        return reward.squeeze(-1)\n",
    "\n",
    "print(\"‚úÖ GPT2RewardModel class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0fdd053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reward dataset created: 5000 pairs\n"
     ]
    }
   ],
   "source": [
    "class RewardDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for reward model training.\n",
    "    Each example contains (chosen, rejected) pair.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        \n",
    "        # Tokenize chosen response\n",
    "        chosen_enc = self.tokenizer(\n",
    "            example['chosen'],\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Tokenize rejected response\n",
    "        rejected_enc = self.tokenizer(\n",
    "            example['rejected'],\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"chosen_input_ids\": chosen_enc[\"input_ids\"].squeeze(),\n",
    "            \"chosen_attention_mask\": chosen_enc[\"attention_mask\"].squeeze(),\n",
    "            \"rejected_input_ids\": rejected_enc[\"input_ids\"].squeeze(),\n",
    "            \"rejected_attention_mask\": rejected_enc[\"attention_mask\"].squeeze(),\n",
    "        }\n",
    "\n",
    "# Create reward dataset\n",
    "if not PRETRAINED_RM_LOADED:\n",
    "    reward_train_dataset = RewardDataset(reward_dataset, tokenizer, max_length=512)\n",
    "    print(f\"‚úÖ Reward dataset created: {len(reward_train_dataset)} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "949d2ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training function defined\n"
     ]
    }
   ],
   "source": [
    "def train_reward_model(model, dataset, epochs=1, batch_size=4, lr=1e-5):\n",
    "    \"\"\"\n",
    "    Train reward model using Bradley-Terry loss.\n",
    "    \n",
    "    Loss = -log(sigmoid(r(chosen) - r(rejected)))\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    print(f\"\\nüöÄ Training Reward Model\")\n",
    "    print(f\"  Epochs: {epochs}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Learning rate: {lr}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    total_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get rewards for chosen and rejected\n",
    "            chosen_reward = model(\n",
    "                batch[\"chosen_input_ids\"],\n",
    "                batch[\"chosen_attention_mask\"]\n",
    "            )\n",
    "            rejected_reward = model(\n",
    "                batch[\"rejected_input_ids\"],\n",
    "                batch[\"rejected_attention_mask\"]\n",
    "            )\n",
    "            \n",
    "            # Bradley-Terry loss\n",
    "            loss = -F.logsigmoid(chosen_reward - rejected_reward).mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"  Epoch {epoch+1}, Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"\\n  Epoch {epoch+1} complete. Avg Loss: {avg_loss:.4f}\")\n",
    "        total_loss = avg_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "print(\"‚úÖ Training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd9b8fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING CUSTOM REWARD MODEL\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 148/148 [00:00<00:00, 436.70it/s, Materializing param=transformer.wte.weight]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Reward Model Parameters:\n",
      "  Total: 124,440,577\n",
      "\n",
      "üöÄ Training Reward Model\n",
      "  Epochs: 1\n",
      "  Batch size: 4\n",
      "  Learning rate: 1e-05\n",
      "============================================================\n",
      "  Epoch 1, Batch 0: Loss = 1.5184\n",
      "  Epoch 1, Batch 100: Loss = 0.6611\n",
      "  Epoch 1, Batch 200: Loss = 0.5811\n",
      "  Epoch 1, Batch 300: Loss = 1.1322\n",
      "  Epoch 1, Batch 400: Loss = 0.8403\n",
      "  Epoch 1, Batch 500: Loss = 0.5720\n",
      "  Epoch 1, Batch 600: Loss = 0.8596\n",
      "  Epoch 1, Batch 700: Loss = 0.6792\n",
      "  Epoch 1, Batch 800: Loss = 0.5240\n",
      "  Epoch 1, Batch 900: Loss = 0.7671\n",
      "  Epoch 1, Batch 1000: Loss = 0.6315\n",
      "  Epoch 1, Batch 1100: Loss = 0.8084\n",
      "  Epoch 1, Batch 1200: Loss = 0.5980\n",
      "\n",
      "  Epoch 1 complete. Avg Loss: 0.7138\n",
      "\n",
      "‚úÖ Reward Model Training Complete!\n",
      "  Time: 490.7 minutes\n",
      "  Final Loss: 0.7138\n",
      "  Saved to: ../outputs/rlhf_training/reward_model\n"
     ]
    }
   ],
   "source": [
    "# Train or load reward model\n",
    "if not PRETRAINED_RM_LOADED:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING CUSTOM REWARD MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize reward model from SFT model\n",
    "    reward_model = GPT2RewardModel(SFT_MODEL_PATH)\n",
    "    print(f\"\\nüìä Reward Model Parameters:\")\n",
    "    print(f\"  Total: {sum(p.numel() for p in reward_model.parameters()):,}\")\n",
    "    \n",
    "    # Train\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    final_loss = train_reward_model(\n",
    "        reward_model,\n",
    "        reward_train_dataset,\n",
    "        epochs=1,\n",
    "        batch_size=4,\n",
    "        lr=1e-5\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ Reward Model Training Complete!\")\n",
    "    print(f\"  Time: {elapsed/60:.1f} minutes\")\n",
    "    print(f\"  Final Loss: {final_loss:.4f}\")\n",
    "    \n",
    "    # Save reward model\n",
    "    rm_path = f\"{OUTPUT_DIR}/reward_model\"\n",
    "    Path(rm_path).mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(reward_model.state_dict(), f\"{rm_path}/reward_model.pt\")\n",
    "    print(f\"  Saved to: {rm_path}\")\n",
    "else:\n",
    "    print(\"‚úÖ Using pre-trained reward model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9935e599",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. PPO Training (Step 3)\n",
    "\n",
    "InstructGPT paper methodology:\n",
    "1. Use SFT model as starting point\n",
    "2. Generate responses to prompts\n",
    "3. Score with reward model\n",
    "4. Update policy with PPO\n",
    "5. Add KL penalty to prevent divergence\n",
    "\n",
    "$$R(x, y) = r_\\theta(x, y) - \\beta \\cdot \\text{KL}(\\pi_{\\text{RL}} || \\pi_{\\text{SFT}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68b04f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SFT model for PPO training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 148/148 [00:00<00:00, 393.32it/s, Materializing param=transformer.wte.weight]             \n",
      "/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/trl/experimental/ppo/modeling_value_head.py:273: FutureWarning: The `AutoModelForCausalLMWithValueHead` is now located in `trl.experimental`. Please update your imports to `from trl.experimental.ppo import AutoModelForCausalLMWithValueHead`. The current import path will be removed and no longer supported in TRL 0.29. For more information, see https://github.com/huggingface/trl/issues/4223.\n",
      "  model = cls(pretrained_model, **multi_adapter_args, **trl_model_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PPO model loaded with value head\n",
      "  Parameters: 124,440,577\n"
     ]
    }
   ],
   "source": [
    "# Load SFT model for PPO\n",
    "print(\"Loading SFT model for PPO training...\")\n",
    "\n",
    "# Using TRL's AutoModelForCausalLMWithValueHead\n",
    "# This adds a value head for PPO advantage estimation\n",
    "try:\n",
    "    from trl import AutoModelForCausalLMWithValueHead\n",
    "    \n",
    "    ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "        SFT_MODEL_PATH,\n",
    "        torch_dtype=torch.float32,\n",
    "    )\n",
    "    print(f\"‚úÖ PPO model loaded with value head\")\n",
    "    print(f\"  Parameters: {sum(p.numel() for p in ppo_model.parameters()):,}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è TRL model loading failed: {e}\")\n",
    "    print(\"  Will use standard model with custom value head\")\n",
    "    ppo_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efd8969d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RL training prompts...\n",
      "\n",
      "‚úÖ Extracted 4815 unique prompts for RL training\n",
      "\n",
      "üìù Sample Prompts:\n",
      "  1. Find the world record for the longest book ever written....\n",
      "  2. Is it hard to refinance your mortgage?...\n",
      "  3. What vegetables have the most iron?...\n"
     ]
    }
   ],
   "source": [
    "# Load prompts for RL training\n",
    "# Using a subset of prompts similar to InstructGPT's prompt distribution\n",
    "\n",
    "print(\"Loading RL training prompts...\")\n",
    "\n",
    "# Option 1: Extract prompts from HH-RLHF dataset\n",
    "def extract_prompt(text):\n",
    "    \"\"\"Extract the human prompt from conversation.\"\"\"\n",
    "    if \"Human:\" in text:\n",
    "        parts = text.split(\"Human:\")\n",
    "        if len(parts) > 1:\n",
    "            prompt = parts[1].split(\"Assistant:\")[0].strip()\n",
    "            return prompt\n",
    "    return None\n",
    "\n",
    "rl_prompts = []\n",
    "for example in reward_dataset:\n",
    "    prompt = extract_prompt(example['chosen'])\n",
    "    if prompt and len(prompt) > 10 and len(prompt) < 500:\n",
    "        rl_prompts.append(prompt)\n",
    "\n",
    "# Deduplicate\n",
    "rl_prompts = list(set(rl_prompts))\n",
    "print(f\"\\n‚úÖ Extracted {len(rl_prompts)} unique prompts for RL training\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nüìù Sample Prompts:\")\n",
    "for i, p in enumerate(rl_prompts[:3]):\n",
    "    print(f\"  {i+1}. {p[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7f8e922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PPO Configuration:\n",
      "  Learning rate: 1.5e-05\n",
      "  Batch size: 2\n",
      "  KL coefficient: 0.2\n",
      "  PPO epochs: 4\n",
      "  Use CPU: True\n"
     ]
    }
   ],
   "source": [
    "# PPO Configuration (InstructGPT-aligned)\n",
    "# Updated for TRL 0.27+ API which has a new PPOConfig structure\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    # Model paths (required in new TRL API)\n",
    "    sft_model_path=SFT_MODEL_PATH,\n",
    "    reward_model_path=SFT_MODEL_PATH,  # We use custom RM, but path is required\n",
    "    \n",
    "    # PPO hyperparameters (from paper)\n",
    "    learning_rate=1.5e-5,           # Paper: 1.5e-5\n",
    "    \n",
    "    # Batch sizes\n",
    "    per_device_train_batch_size=2,  # Scaled for CPU\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_mini_batches=1,\n",
    "    \n",
    "    # KL penalty (critical for InstructGPT)\n",
    "    kl_coef=0.2,                    # Paper: Œ≤ = 0.02, but we use higher for GPT-2\n",
    "    \n",
    "    # PPO specific\n",
    "    num_ppo_epochs=4,               # Paper: 4\n",
    "    cliprange=0.2,                  # Standard PPO\n",
    "    cliprange_value=0.2,\n",
    "    vf_coef=0.1,                    # Value function coefficient\n",
    "    \n",
    "    # Generation\n",
    "    response_length=100,\n",
    "    temperature=0.7,\n",
    "    \n",
    "    # CPU Training settings\n",
    "    use_cpu=True,                   # Required for CPU training\n",
    "    bf16=False,                     # Disable bf16 for CPU\n",
    "    fp16=False,                     # Disable fp16 for CPU\n",
    "    gradient_checkpointing=False,   # Disable for CPU\n",
    "    \n",
    "    # Training\n",
    "    seed=42,\n",
    "    report_to=\"none\",               # Disable wandb\n",
    ")\n",
    "\n",
    "print(\"‚úÖ PPO Configuration:\")\n",
    "print(f\"  Learning rate: {ppo_config.learning_rate}\")\n",
    "print(f\"  Batch size: {ppo_config.per_device_train_batch_size}\")\n",
    "print(f\"  KL coefficient: {ppo_config.kl_coef}\")\n",
    "print(f\"  PPO epochs: {ppo_config.num_ppo_epochs}\")\n",
    "print(f\"  Use CPU: {ppo_config.use_cpu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92ea97d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Reward Model Test:\n",
      "  Good response reward: -1.0287\n",
      "  Bad response reward:  -1.2017\n",
      "  Difference: 0.1730 (should be positive)\n"
     ]
    }
   ],
   "source": [
    "def get_reward(reward_model, tokenizer, prompt, response):\n",
    "    \"\"\"\n",
    "    Get reward score for a (prompt, response) pair.\n",
    "    \"\"\"\n",
    "    text = f\"Human: {prompt}\\n\\nAssistant: {response}\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if isinstance(reward_model, GPT2RewardModel):\n",
    "            reward = reward_model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "        else:\n",
    "            # Pre-trained RM (e.g., OpenAssistant)\n",
    "            outputs = reward_model(**inputs)\n",
    "            reward = outputs.logits.squeeze()\n",
    "    \n",
    "    return reward.item()\n",
    "\n",
    "# Test reward function\n",
    "if reward_model is not None:\n",
    "    test_prompt = \"What is the capital of France?\"\n",
    "    test_response_good = \"The capital of France is Paris.\"\n",
    "    test_response_bad = \"I don't know anything about geography.\"\n",
    "    \n",
    "    reward_good = get_reward(reward_model, tokenizer, test_prompt, test_response_good)\n",
    "    reward_bad = get_reward(reward_model, tokenizer, test_prompt, test_response_bad)\n",
    "    \n",
    "    print(f\"\\nüìä Reward Model Test:\")\n",
    "    print(f\"  Good response reward: {reward_good:.4f}\")\n",
    "    print(f\"  Bad response reward:  {reward_bad:.4f}\")\n",
    "    print(f\"  Difference: {reward_good - reward_bad:.4f} (should be positive)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98dd3884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Improved PPO training function defined\n"
     ]
    }
   ],
   "source": [
    "# Improved PPO training with better stability\n",
    "# Key fixes: Higher KL penalty, reward normalization, advantage normalization\n",
    "\n",
    "def run_improved_ppo_training(model, reward_model, tokenizer, prompts, n_steps=100, lr=5e-6, kl_coef=0.5):\n",
    "    \"\"\"\n",
    "    Improved PPO training with better stability mechanisms.\n",
    "    \n",
    "    Key improvements:\n",
    "    1. Higher KL coefficient (0.5) to stay closer to SFT\n",
    "    2. Reward normalization using running statistics\n",
    "    3. Advantage normalization\n",
    "    4. Lower learning rate for stability\n",
    "    5. Early stopping if KL diverges too much\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    \n",
    "    # Keep reference model for KL calculation\n",
    "    ref_model = copy.deepcopy(model)\n",
    "    ref_model.eval()\n",
    "    for param in ref_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    \n",
    "    # Learning rate scheduler for stability\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_steps)\n",
    "    \n",
    "    print(f\"\\nüöÄ Starting IMPROVED PPO Training\")\n",
    "    print(f\"  Steps: {n_steps}\")\n",
    "    print(f\"  Learning rate: {lr}\")\n",
    "    print(f\"  KL coefficient: {kl_coef} (higher = more conservative)\")\n",
    "    print(f\"  Using reward normalization + advantage normalization\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    generation_kwargs = {\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    }\n",
    "    \n",
    "    stats_history = []\n",
    "    batch_size = 2\n",
    "    \n",
    "    # Running statistics for reward normalization\n",
    "    reward_mean = 0.0\n",
    "    reward_std = 1.0\n",
    "    reward_count = 0\n",
    "    \n",
    "    # KL threshold for early stopping\n",
    "    max_kl = 10.0\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        # Sample batch of prompts\n",
    "        batch_prompts = random.sample(prompts, min(batch_size, len(prompts)))\n",
    "        \n",
    "        batch_rewards = []\n",
    "        batch_kls = []\n",
    "        batch_losses = []\n",
    "        \n",
    "        for prompt in batch_prompts:\n",
    "            # Format prompt\n",
    "            formatted = f\"Human: {prompt}\\n\\nAssistant:\"\n",
    "            inputs = tokenizer(formatted, return_tensors=\"pt\")\n",
    "            \n",
    "            # Generate with policy model\n",
    "            with torch.no_grad():\n",
    "                output = model.generate(**inputs, **generation_kwargs)\n",
    "            \n",
    "            response_ids = output[0][inputs[\"input_ids\"].shape[1]:]\n",
    "            response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # Skip if response is too short (degenerate)\n",
    "            if len(response.split()) < 3:\n",
    "                continue\n",
    "            \n",
    "            # Get reward\n",
    "            reward = get_reward(reward_model, tokenizer, prompt, response)\n",
    "            batch_rewards.append(reward)\n",
    "            \n",
    "            # Update running reward statistics\n",
    "            reward_count += 1\n",
    "            delta = reward - reward_mean\n",
    "            reward_mean += delta / reward_count\n",
    "            if reward_count > 1:\n",
    "                reward_std = max(0.1, (reward_std * (reward_count - 1) + abs(delta)) / reward_count)\n",
    "            \n",
    "            # Normalize reward\n",
    "            normalized_reward = (reward - reward_mean) / reward_std\n",
    "            \n",
    "            # Compute KL divergence\n",
    "            with torch.no_grad():\n",
    "                policy_outputs = model(output, labels=output)\n",
    "                policy_logits = policy_outputs.logits\n",
    "                \n",
    "                ref_outputs = ref_model(output, labels=output)\n",
    "                ref_logits = ref_outputs.logits\n",
    "                \n",
    "                # KL divergence (more stable computation)\n",
    "                policy_log_probs = F.log_softmax(policy_logits, dim=-1)\n",
    "                ref_probs = F.softmax(ref_logits, dim=-1)\n",
    "                kl = (ref_probs * (F.log_softmax(ref_logits, dim=-1) - policy_log_probs)).sum(dim=-1).mean()\n",
    "                batch_kls.append(kl.item())\n",
    "            \n",
    "            # Check KL threshold\n",
    "            if kl.item() > max_kl:\n",
    "                print(f\"  ‚ö†Ô∏è Step {step}: KL too high ({kl.item():.2f}), skipping update\")\n",
    "                continue\n",
    "            \n",
    "            # Policy gradient with advantage normalization\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(output, labels=output)\n",
    "            \n",
    "            # Advantage = normalized_reward (already normalized)\n",
    "            advantage = normalized_reward\n",
    "            \n",
    "            # Clipped policy loss (PPO-style)\n",
    "            policy_loss = -advantage * outputs.loss\n",
    "            kl_loss = kl_coef * kl\n",
    "            \n",
    "            loss = policy_loss + kl_loss\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_losses.append(loss.item())\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if batch_rewards:\n",
    "            avg_reward = sum(batch_rewards) / len(batch_rewards)\n",
    "            avg_kl = sum(batch_kls) / len(batch_kls) if batch_kls else 0\n",
    "            avg_loss = sum(batch_losses) / len(batch_losses) if batch_losses else 0\n",
    "            \n",
    "            stats_history.append({\n",
    "                \"reward\": avg_reward,\n",
    "                \"kl\": avg_kl,\n",
    "                \"loss\": avg_loss,\n",
    "                \"lr\": scheduler.get_last_lr()[0]\n",
    "            })\n",
    "            \n",
    "            if step % 10 == 0:\n",
    "                print(f\"  Step {step}: Reward = {avg_reward:.4f}, KL = {avg_kl:.4f}, Loss = {avg_loss:.4f}, LR = {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    return model, stats_history\n",
    "\n",
    "print(\"‚úÖ Improved PPO training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0a02123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING IMPROVED PPO TRAINING\n",
      "============================================================\n",
      "\n",
      "Key Optimizations:\n",
      "  1. Higher KL coefficient (0.5) - prevents divergence from SFT\n",
      "  2. Lower learning rate (5e-6) - more stable updates\n",
      "  3. Reward normalization - better advantage estimation\n",
      "  4. Cosine LR schedule - gradual learning rate decay\n",
      "  5. Gradient clipping (0.5) - prevents exploding gradients\n",
      "  6. KL threshold - skips updates if diverging too much\n",
      "\n",
      "Loading SFT model for PPO optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 148/148 [00:00<00:00, 416.83it/s, Materializing param=transformer.wte.weight]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä PPO Training Configuration:\n",
      "  Steps: 150\n",
      "  Prompts: 300\n",
      "  Learning rate: 5e-06\n",
      "  KL coefficient: 0.5\n",
      "  Estimated time: ~22.5 minutes on CPU\n",
      "\n",
      "üöÄ Starting IMPROVED PPO Training\n",
      "  Steps: 150\n",
      "  Learning rate: 5e-06\n",
      "  KL coefficient: 0.5 (higher = more conservative)\n",
      "  Using reward normalization + advantage normalization\n",
      "============================================================\n",
      "  Step 0: Reward = -1.2878, KL = 0.2636, Loss = -0.4337, LR = 5.00e-06\n",
      "  Step 10: Reward = -0.3024, KL = 0.1422, Loss = -4.3597, LR = 4.93e-06\n",
      "  Step 20: Reward = -0.8776, KL = 0.2216, Loss = -1.9391, LR = 4.76e-06\n",
      "  Step 30: Reward = -1.3873, KL = 0.1778, Loss = 3.5287, LR = 4.49e-06\n",
      "  Step 40: Reward = -0.9343, KL = 0.4016, Loss = -0.2161, LR = 4.13e-06\n",
      "  Step 50: Reward = -1.4858, KL = 0.3644, Loss = 4.3716, LR = 3.70e-06\n",
      "  Step 60: Reward = -0.9514, KL = 0.2045, Loss = -0.4131, LR = 3.22e-06\n",
      "  Step 70: Reward = -1.4224, KL = 0.3009, Loss = 3.4870, LR = 2.71e-06\n",
      "  Step 80: Reward = -0.9027, KL = 0.2007, Loss = -0.1229, LR = 2.19e-06\n",
      "  Step 90: Reward = -0.8551, KL = 0.2295, Loss = -1.0858, LR = 1.68e-06\n",
      "  Step 100: Reward = -1.3699, KL = 0.2491, Loss = 2.6147, LR = 1.20e-06\n",
      "  Step 110: Reward = -1.3507, KL = 0.3715, Loss = 2.6423, LR = 7.89e-07\n",
      "  Step 120: Reward = -0.9084, KL = 0.3580, Loss = -0.9910, LR = 4.47e-07\n",
      "  Step 130: Reward = -1.3792, KL = 0.2235, Loss = 3.0209, LR = 1.95e-07\n",
      "  Step 140: Reward = -0.7012, KL = 0.4573, Loss = -1.9067, LR = 4.43e-08\n",
      "\n",
      "‚úÖ Improved PPO Training Complete!\n",
      "  Time: 18.4 minutes\n",
      "  Steps: 150\n",
      "\n",
      "üìà Training Statistics:\n",
      "  Average Reward: -0.9874\n",
      "  First 10 steps avg: -1.0663\n",
      "  Last 10 steps avg:  -0.8862\n",
      "  Reward Improvement: +0.1802\n",
      "  Average KL: 0.2752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved to: ../outputs/rlhf_training/ppo_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run IMPROVED PPO training with better stability\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "if reward_model is not None and len(rl_prompts) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STARTING IMPROVED PPO TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\"\"\n",
    "Key Optimizations:\n",
    "  1. Higher KL coefficient (0.5) - prevents divergence from SFT\n",
    "  2. Lower learning rate (5e-6) - more stable updates\n",
    "  3. Reward normalization - better advantage estimation\n",
    "  4. Cosine LR schedule - gradual learning rate decay\n",
    "  5. Gradient clipping (0.5) - prevents exploding gradients\n",
    "  6. KL threshold - skips updates if diverging too much\n",
    "\"\"\")\n",
    "    \n",
    "    # Load fresh SFT model for PPO training\n",
    "    print(\"Loading SFT model for PPO optimization...\")\n",
    "    ppo_train_model = AutoModelForCausalLM.from_pretrained(SFT_MODEL_PATH, torch_dtype=torch.float32)\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Improved PPO configuration\n",
    "    N_PPO_STEPS = 150  # More steps for better convergence\n",
    "    N_PROMPTS = min(300, len(rl_prompts))  # More diverse prompts\n",
    "    KL_COEF = 0.5  # Higher KL penalty to stay close to SFT\n",
    "    LR = 5e-6  # Lower learning rate for stability\n",
    "    \n",
    "    print(f\"\\nüìä PPO Training Configuration:\")\n",
    "    print(f\"  Steps: {N_PPO_STEPS}\")\n",
    "    print(f\"  Prompts: {N_PROMPTS}\")\n",
    "    print(f\"  Learning rate: {LR}\")\n",
    "    print(f\"  KL coefficient: {KL_COEF}\")\n",
    "    print(f\"  Estimated time: ~{N_PPO_STEPS * 0.15:.1f} minutes on CPU\")\n",
    "    \n",
    "    ppo_trained_model, stats = run_improved_ppo_training(\n",
    "        ppo_train_model,\n",
    "        reward_model,\n",
    "        tokenizer,\n",
    "        rl_prompts[:N_PROMPTS],\n",
    "        n_steps=N_PPO_STEPS,\n",
    "        lr=LR,\n",
    "        kl_coef=KL_COEF\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ Improved PPO Training Complete!\")\n",
    "    print(f\"  Time: {elapsed/60:.1f} minutes\")\n",
    "    print(f\"  Steps: {N_PPO_STEPS}\")\n",
    "    \n",
    "    # Calculate improvements\n",
    "    if stats:\n",
    "        avg_reward = sum(s['reward'] for s in stats) / len(stats)\n",
    "        first_10_reward = sum(s['reward'] for s in stats[:10]) / min(10, len(stats))\n",
    "        last_10_reward = sum(s['reward'] for s in stats[-10:]) / min(10, len(stats))\n",
    "        avg_kl = sum(s['kl'] for s in stats) / len(stats)\n",
    "        \n",
    "        print(f\"\\nüìà Training Statistics:\")\n",
    "        print(f\"  Average Reward: {avg_reward:.4f}\")\n",
    "        print(f\"  First 10 steps avg: {first_10_reward:.4f}\")\n",
    "        print(f\"  Last 10 steps avg:  {last_10_reward:.4f}\")\n",
    "        print(f\"  Reward Improvement: {last_10_reward - first_10_reward:+.4f}\")\n",
    "        print(f\"  Average KL: {avg_kl:.4f}\")\n",
    "    \n",
    "    # Save PPO model\n",
    "    ppo_path = f\"{OUTPUT_DIR}/ppo_model\"\n",
    "    Path(ppo_path).mkdir(parents=True, exist_ok=True)\n",
    "    ppo_trained_model.save_pretrained(ppo_path)\n",
    "    tokenizer.save_pretrained(ppo_path)\n",
    "    print(f\"  Saved to: {ppo_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping PPO training (models not ready)\")\n",
    "    print(\"  Check that reward model is loaded and prompts are available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3957b5f2",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Evaluation: GPT-2 Base vs SFT vs SFT+PPO\n",
    "\n",
    "Following InstructGPT paper evaluation methodology:\n",
    "- Compare all three stages of the pipeline\n",
    "- Measure reward scores from RM\n",
    "- Check for quality improvements/degradation\n",
    "\n",
    "| Model | Description |\n",
    "|-------|-------------|\n",
    "| **GPT-2 Base** | Original pretrained model (no fine-tuning) |\n",
    "| **InstructGPT SFT** | After Step 1: Supervised Fine-Tuning |\n",
    "| **InstructGPT SFT+PPO** | After Step 3: Full RLHF pipeline |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6cb25538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models for comprehensive evaluation...\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ Loading GPT-2 Base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 148/148 [00:00<00:00, 236.62it/s, Materializing param=transformer.wte.weight]             \n",
      "GPT2LMHeadModel LOAD REPORT from: ../models/gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Loaded from: ../models/gpt2\n",
      "\n",
      "2Ô∏è‚É£ Loading InstructGPT SFT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 148/148 [00:00<00:00, 238.34it/s, Materializing param=transformer.wte.weight]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Loaded from: ../outputs/improved_training/merged\n",
      "\n",
      "3Ô∏è‚É£ Loading InstructGPT SFT+PPO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 148/148 [00:00<00:00, 269.71it/s, Materializing param=transformer.wte.weight]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Loaded from: ../outputs/rlhf_training/ppo_model\n",
      "\n",
      "============================================================\n",
      "All models loaded successfully!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Load all three models for comparison\n",
    "print(\"Loading models for comprehensive evaluation...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. GPT-2 Base (original pretrained)\n",
    "print(\"\\n1Ô∏è‚É£ Loading GPT-2 Base...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_PATH, torch_dtype=torch.float32)\n",
    "base_model.eval()\n",
    "print(f\"   ‚úÖ Loaded from: {BASE_MODEL_PATH}\")\n",
    "\n",
    "# 2. InstructGPT SFT (after supervised fine-tuning)\n",
    "print(\"\\n2Ô∏è‚É£ Loading InstructGPT SFT...\")\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(SFT_MODEL_PATH, torch_dtype=torch.float32)\n",
    "sft_model.eval()\n",
    "print(f\"   ‚úÖ Loaded from: {SFT_MODEL_PATH}\")\n",
    "\n",
    "# 3. InstructGPT SFT+PPO (full RLHF)\n",
    "print(\"\\n3Ô∏è‚É£ Loading InstructGPT SFT+PPO...\")\n",
    "ppo_eval_model = None\n",
    "ppo_path = f\"{OUTPUT_DIR}/ppo_model\"\n",
    "if Path(ppo_path).exists():\n",
    "    ppo_eval_model = AutoModelForCausalLM.from_pretrained(ppo_path, torch_dtype=torch.float32)\n",
    "    ppo_eval_model.eval()\n",
    "    print(f\"   ‚úÖ Loaded from: {ppo_path}\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è PPO model not found at {ppo_path}\")\n",
    "    print(\"   Will compare Base vs SFT only\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All models loaded successfully!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b8b522b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üî¨ COMPREHENSIVE MODEL COMPARISON\n",
      "   GPT-2 Base vs InstructGPT SFT vs InstructGPT SFT+PPO\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìù PROMPT: What is the capital of France?\n",
      "======================================================================\n",
      "\n",
      "üî¥ GPT-2 Base (reward: -0.8110):\n",
      "   The French are really like an Italian family. They have a lot in common with their German neighbours, but they also share some similarities to ours as...\n",
      "\n",
      "üîµ InstructGPT SFT (reward: -0.9713):\n",
      "   It's French. That means it has a high-tech, industrial base and very sophisticated services sector.\"\n",
      "\n",
      "üü¢ InstructGPT SFT+PPO (reward: -0.6560):\n",
      "   It's a great place for French language arts, art and culture. The area has its own market economy where it can get into some good places such as Lyon ...\n",
      "\n",
      "   üìä Reward Improvements:\n",
      "      SFT vs Base: -0.1603 ‚ö†Ô∏è\n",
      "      PPO vs SFT:  +0.3153 ‚úÖ\n",
      "      PPO vs Base: +0.1550 ‚úÖ\n",
      "\n",
      "======================================================================\n",
      "üìù PROMPT: Explain why the sky is blue.\n",
      "======================================================================\n",
      "\n",
      "üî¥ GPT-2 Base (reward: -0.5153):\n",
      "   The reason I'm on this planet, and how it feels to be here today [at] home (or in your room). This will probably not happen for a very long time! But ...\n",
      "\n",
      "üîµ InstructGPT SFT (reward: -0.5430):\n",
      "   It's not that much more green than it used to be, but when you get a look at how we've done things over there lately I think they're going in exactly ...\n",
      "\n",
      "üü¢ InstructGPT SFT+PPO (reward: -1.1556):\n",
      "   It's a sign of life!\n",
      "\n",
      "   üìä Reward Improvements:\n",
      "      SFT vs Base: -0.0277 ‚ö†Ô∏è\n",
      "      PPO vs SFT:  -0.6126 ‚ö†Ô∏è\n",
      "      PPO vs Base: -0.6403 ‚ö†Ô∏è\n",
      "\n",
      "======================================================================\n",
      "üìù PROMPT: How do I make a cup of tea?\n",
      "======================================================================\n",
      "\n",
      "üî¥ GPT-2 Base (reward: -1.1695):\n",
      "   You can get it in the morning. It's usually good for you, but if we're not getting any more coffee or water then what are our options going to be with...\n",
      "\n",
      "üîµ InstructGPT SFT (reward: -0.5756):\n",
      "   A couple cups. (laughs) Then you're ready to eat!\n",
      "\n",
      "üü¢ InstructGPT SFT+PPO (reward: -0.8180):\n",
      "   It's just one thing, but it is important to get your mind off the subject. In my book \"The Secret World of Happiness,\" we talk about how you can have ...\n",
      "\n",
      "   üìä Reward Improvements:\n",
      "      SFT vs Base: +0.5939 ‚úÖ\n",
      "      PPO vs SFT:  -0.2424 ‚ö†Ô∏è\n",
      "      PPO vs Base: +0.3515 ‚úÖ\n",
      "\n",
      "======================================================================\n",
      "üìù PROMPT: What are the benefits of exercise?\n",
      "======================================================================\n",
      "\n",
      "üî¥ GPT-2 Base (reward: -0.1888):\n",
      "   The most important benefit is that you lose weight faster. If your metabolism slows down, then it's likely to be due for some time or another; if not ...\n",
      "\n",
      "üîµ InstructGPT SFT (reward: -1.0541):\n",
      "   I'd love to see a list like this.\n",
      "\n",
      "üü¢ InstructGPT SFT+PPO (reward: -1.2316):\n",
      "   Exercise has a lot to do with how you feel, your body's response to it. You can't really go out and get yourself back up in one moment without exercis...\n",
      "\n",
      "   üìä Reward Improvements:\n",
      "      SFT vs Base: -0.8653 ‚ö†Ô∏è\n",
      "      PPO vs SFT:  -0.1775 ‚ö†Ô∏è\n",
      "      PPO vs Base: -1.0428 ‚ö†Ô∏è\n",
      "\n",
      "======================================================================\n",
      "üìù PROMPT: Tell me about climate change.\n",
      "======================================================================\n",
      "\n",
      "üî¥ GPT-2 Base (reward: -0.7848):\n",
      "   I'm not going to tell you what the IPCC's saying, but they're making a very good case for it by their own standards and from an international perspect...\n",
      "\n",
      "üîµ InstructGPT SFT (reward: -0.8855):\n",
      "   The question of whether we can get more people to go on an expedition is a different one from the questions, 'What will happen if it's not going well?...\n",
      "\n",
      "üü¢ InstructGPT SFT+PPO (reward: -0.8549):\n",
      "   We're talking to the UN Security Council, and we have a very special envoy from France who's working with us on this issue of global warming because i...\n",
      "\n",
      "   üìä Reward Improvements:\n",
      "      SFT vs Base: -0.1007 ‚ö†Ô∏è\n",
      "      PPO vs SFT:  +0.0306 ‚úÖ\n",
      "      PPO vs Base: -0.0701 ‚ö†Ô∏è\n",
      "\n",
      "======================================================================\n",
      "üìù PROMPT: Who wrote Romeo and Juliet?\n",
      "======================================================================\n",
      "\n",
      "üî¥ GPT-2 Base (reward: -1.5466):\n",
      "   Well, I think it's a pretty good read. It was written in the year 1839 by Edward Scaife (1908‚Äì1881), who had been born at Lubbock on January 1st‚Äîthe d...\n",
      "\n",
      "üîµ InstructGPT SFT (reward: -1.3333):\n",
      "   Robert Kranz. I remember reading about this book by the late William F. Buckley, which was written in 1974 when it appeared on Harper's Weekly ‚Äî that ...\n",
      "\n",
      "üü¢ InstructGPT SFT+PPO (reward: -0.9066):\n",
      "   I'm not sure, but it's a pretty good novel. It has some very well-written characters that are still being written as they go along - one of the great ...\n",
      "\n",
      "   üìä Reward Improvements:\n",
      "      SFT vs Base: +0.2133 ‚úÖ\n",
      "      PPO vs SFT:  +0.4267 ‚úÖ\n",
      "      PPO vs Base: +0.6400 ‚úÖ\n",
      "\n",
      "======================================================================\n",
      "üìù PROMPT: What is 2 + 2?\n",
      "======================================================================\n",
      "\n",
      "üî¥ GPT-2 Base (reward: -1.5059):\n",
      "   I think it's a combination of things. The one that you see in this video, the two numbers were different but they are similar to each other and we're ...\n",
      "\n",
      "üîµ InstructGPT SFT (reward: -0.9747):\n",
      "   Why are you telling me this. I am your best friend in the world, and we've been living together for some time now. You want to know what it means that...\n",
      "\n",
      "üü¢ InstructGPT SFT+PPO (reward: -0.8596):\n",
      "   I'm thinking of a game called \"Hobbit\", and it's about the two robots, both trying to learn how they can beat each other in order for them all get thr...\n",
      "\n",
      "   üìä Reward Improvements:\n",
      "      SFT vs Base: +0.5312 ‚úÖ\n",
      "      PPO vs SFT:  +0.1151 ‚úÖ\n",
      "      PPO vs Base: +0.6464 ‚úÖ\n",
      "\n",
      "======================================================================\n",
      "üìù PROMPT: Name a fruit that is red.\n",
      "======================================================================\n",
      "\n",
      "üî¥ GPT-2 Base (reward: -0.4422):\n",
      "   You can find it here in the garden, right? It's also called \"Green Bamboo\". And I don't know if you've ever seen green bamboo before‚Ä¶ (sigh) That was ...\n",
      "\n",
      "üîµ InstructGPT SFT (reward: -1.0616):\n",
      "   \"My name was Mary, but I didn't know it.\"\n",
      "\n",
      "üü¢ InstructGPT SFT+PPO (reward: -1.4123):\n",
      "   I'll be back soon!\n",
      "\n",
      "   üìä Reward Improvements:\n",
      "      SFT vs Base: -0.6194 ‚ö†Ô∏è\n",
      "      PPO vs SFT:  -0.3507 ‚ö†Ô∏è\n",
      "      PPO vs Base: -0.9701 ‚ö†Ô∏è\n"
     ]
    }
   ],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_new_tokens=100):\n",
    "    \"\"\"Generate response from model.\"\"\"\n",
    "    formatted = f\"Human: {prompt}\\n\\nAssistant:\"\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            repetition_penalty=1.2,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"Assistant:\" in response:\n",
    "        response = response.split(\"Assistant:\")[1].strip()\n",
    "    return response\n",
    "\n",
    "# Evaluation prompts (same as paper evaluation style)\n",
    "eval_prompts = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain why the sky is blue.\",\n",
    "    \"How do I make a cup of tea?\",\n",
    "    \"What are the benefits of exercise?\",\n",
    "    \"Tell me about climate change.\",\n",
    "    \"Who wrote Romeo and Juliet?\",\n",
    "    \"What is 2 + 2?\",\n",
    "    \"Name a fruit that is red.\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"   GPT-2 Base vs InstructGPT SFT vs InstructGPT SFT+PPO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Store results for summary table\n",
    "all_results = {\n",
    "    \"base\": {\"responses\": [], \"rewards\": []},\n",
    "    \"sft\": {\"responses\": [], \"rewards\": []},\n",
    "    \"ppo\": {\"responses\": [], \"rewards\": []},\n",
    "}\n",
    "\n",
    "for prompt in eval_prompts:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìù PROMPT: {prompt}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # 1. GPT-2 Base response\n",
    "    base_response = generate_response(base_model, tokenizer, prompt)\n",
    "    base_reward = get_reward(reward_model, tokenizer, prompt, base_response) if reward_model else 0\n",
    "    all_results[\"base\"][\"responses\"].append(base_response)\n",
    "    all_results[\"base\"][\"rewards\"].append(base_reward)\n",
    "    \n",
    "    print(f\"\\nüî¥ GPT-2 Base (reward: {base_reward:.4f}):\")\n",
    "    print(f\"   {base_response[:150]}{'...' if len(base_response) > 150 else ''}\")\n",
    "    \n",
    "    # 2. InstructGPT SFT response\n",
    "    sft_response = generate_response(sft_model, tokenizer, prompt)\n",
    "    sft_reward = get_reward(reward_model, tokenizer, prompt, sft_response) if reward_model else 0\n",
    "    all_results[\"sft\"][\"responses\"].append(sft_response)\n",
    "    all_results[\"sft\"][\"rewards\"].append(sft_reward)\n",
    "    \n",
    "    print(f\"\\nüîµ InstructGPT SFT (reward: {sft_reward:.4f}):\")\n",
    "    print(f\"   {sft_response[:150]}{'...' if len(sft_response) > 150 else ''}\")\n",
    "    \n",
    "    # 3. InstructGPT SFT+PPO response\n",
    "    if ppo_eval_model:\n",
    "        ppo_response = generate_response(ppo_eval_model, tokenizer, prompt)\n",
    "        ppo_reward = get_reward(reward_model, tokenizer, prompt, ppo_response) if reward_model else 0\n",
    "        all_results[\"ppo\"][\"responses\"].append(ppo_response)\n",
    "        all_results[\"ppo\"][\"rewards\"].append(ppo_reward)\n",
    "        \n",
    "        print(f\"\\nüü¢ InstructGPT SFT+PPO (reward: {ppo_reward:.4f}):\")\n",
    "        print(f\"   {ppo_response[:150]}{'...' if len(ppo_response) > 150 else ''}\")\n",
    "        \n",
    "        # Improvement indicators\n",
    "        sft_vs_base = sft_reward - base_reward\n",
    "        ppo_vs_sft = ppo_reward - sft_reward\n",
    "        ppo_vs_base = ppo_reward - base_reward\n",
    "        \n",
    "        print(f\"\\n   üìä Reward Improvements:\")\n",
    "        print(f\"      SFT vs Base: {sft_vs_base:+.4f} {'‚úÖ' if sft_vs_base > 0 else '‚ö†Ô∏è'}\")\n",
    "        print(f\"      PPO vs SFT:  {ppo_vs_sft:+.4f} {'‚úÖ' if ppo_vs_sft > 0 else '‚ö†Ô∏è'}\")\n",
    "        print(f\"      PPO vs Base: {ppo_vs_base:+.4f} {'‚úÖ' if ppo_vs_base > 0 else '‚ö†Ô∏è'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44886f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä SUMMARY: AVERAGE REWARD SCORES\n",
      "======================================================================\n",
      "\n",
      "Model                               Avg Reward      vs Base       vs SFT\n",
      "----------------------------------------------------------------------\n",
      "üî¥ GPT-2 Base                           -0.8705            -            -\n",
      "üîµ InstructGPT SFT                      -0.9249      -0.0544            -\n",
      "üü¢ InstructGPT SFT+PPO                  -0.9868      -0.1163      -0.0619\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üèÜ WINNER: GPT-2 Base (avg reward: -0.8705)\n",
      "\n",
      "üìà Improvement Analysis:\n",
      "   SFT improves over Base by: -6.2%\n",
      "   PPO improves over SFT by:  -6.7%\n",
      "   PPO improves over Base by: -13.4%\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary Statistics Table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä SUMMARY: AVERAGE REWARD SCORES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "avg_base = sum(all_results[\"base\"][\"rewards\"]) / len(all_results[\"base\"][\"rewards\"])\n",
    "avg_sft = sum(all_results[\"sft\"][\"rewards\"]) / len(all_results[\"sft\"][\"rewards\"])\n",
    "\n",
    "print(f\"\\n{'Model':<30} {'Avg Reward':>15} {'vs Base':>12} {'vs SFT':>12}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'üî¥ GPT-2 Base':<30} {avg_base:>15.4f} {'-':>12} {'-':>12}\")\n",
    "print(f\"{'üîµ InstructGPT SFT':<30} {avg_sft:>15.4f} {avg_sft-avg_base:>+12.4f} {'-':>12}\")\n",
    "\n",
    "if ppo_eval_model and all_results[\"ppo\"][\"rewards\"]:\n",
    "    avg_ppo = sum(all_results[\"ppo\"][\"rewards\"]) / len(all_results[\"ppo\"][\"rewards\"])\n",
    "    print(f\"{'üü¢ InstructGPT SFT+PPO':<30} {avg_ppo:>15.4f} {avg_ppo-avg_base:>+12.4f} {avg_ppo-avg_sft:>+12.4f}\")\n",
    "    \n",
    "    # Winner determination\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    models = [(\"GPT-2 Base\", avg_base), (\"InstructGPT SFT\", avg_sft), (\"InstructGPT SFT+PPO\", avg_ppo)]\n",
    "    winner = max(models, key=lambda x: x[1])\n",
    "    print(f\"\\nüèÜ WINNER: {winner[0]} (avg reward: {winner[1]:.4f})\")\n",
    "else:\n",
    "    models = [(\"GPT-2 Base\", avg_base), (\"InstructGPT SFT\", avg_sft)]\n",
    "    winner = max(models, key=lambda x: x[1])\n",
    "    print(f\"\\nüèÜ Best Model (without PPO): {winner[0]} (avg reward: {winner[1]:.4f})\")\n",
    "\n",
    "# Improvement percentages\n",
    "print(\"\\nüìà Improvement Analysis:\")\n",
    "print(f\"   SFT improves over Base by: {((avg_sft - avg_base) / abs(avg_base) * 100) if avg_base != 0 else 0:.1f}%\")\n",
    "if ppo_eval_model and all_results[\"ppo\"][\"rewards\"]:\n",
    "    print(f\"   PPO improves over SFT by:  {((avg_ppo - avg_sft) / abs(avg_sft) * 100) if avg_sft != 0 else 0:.1f}%\")\n",
    "    print(f\"   PPO improves over Base by: {((avg_ppo - avg_base) / abs(avg_base) * 100) if avg_base != 0 else 0:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94c0aede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä INSTRUCTGPT-STYLE METRICS COMPARISON\n",
      "======================================================================\n",
      "\n",
      "Evaluating GPT-2 Base...\n",
      "Evaluating InstructGPT SFT...\n",
      "Evaluating InstructGPT SFT+PPO...\n",
      "\n",
      "======================================================================\n",
      "Metric                      GPT-2 Base          SFT      SFT+PPO\n",
      "----------------------------------------------------------------------\n",
      "instruction_following            100%        100%         88%\n",
      "factual_accuracy                  38%         50%         38%\n",
      "coherence                        100%        100%         88%\n",
      "appropriate_length               100%        100%         88%\n",
      "no_hallucination                 100%        100%        100%\n",
      "----------------------------------------------------------------------\n",
      "OVERALL                           88%         90%         80%\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üèÜ FINAL RANKING (by InstructGPT metrics):\n",
      "   ü•á #1: üîµ InstructGPT SFT - 90%\n",
      "   ü•à #2: üî¥ GPT-2 Base - 88%\n",
      "   ü•â #3: üü¢ InstructGPT SFT+PPO - 80%\n"
     ]
    }
   ],
   "source": [
    "# InstructGPT-Style Evaluation Metrics (from paper)\n",
    "def instructgpt_style_evaluation(model, tokenizer, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate model using InstructGPT paper metrics:\n",
    "    1. Instruction Following\n",
    "    2. Factual Accuracy (TruthfulQA-style)\n",
    "    3. Coherence\n",
    "    4. Appropriate Length\n",
    "    5. No Hallucination\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"instruction_following\": [],\n",
    "        \"factual_accuracy\": [],\n",
    "        \"coherence\": [],\n",
    "        \"appropriate_length\": [],\n",
    "        \"no_hallucination\": [],\n",
    "    }\n",
    "    \n",
    "    # Factual test cases\n",
    "    factual_tests = [\n",
    "        {\"q\": \"What is the capital of France?\", \"keywords\": [\"paris\"]},\n",
    "        {\"q\": \"What is 2 + 2?\", \"keywords\": [\"4\", \"four\"]},\n",
    "        {\"q\": \"What color is the sky?\", \"keywords\": [\"blue\"]},\n",
    "        {\"q\": \"Who wrote Romeo and Juliet?\", \"keywords\": [\"shakespeare\", \"william\"]},\n",
    "        {\"q\": \"What is the largest planet?\", \"keywords\": [\"jupiter\"]},\n",
    "        {\"q\": \"What language is spoken in Spain?\", \"keywords\": [\"spanish\"]},\n",
    "        {\"q\": \"How many days in a week?\", \"keywords\": [\"7\", \"seven\"]},\n",
    "        {\"q\": \"What is the boiling point of water?\", \"keywords\": [\"100\", \"212\", \"celsius\", \"fahrenheit\"]},\n",
    "    ]\n",
    "    \n",
    "    for test in factual_tests:\n",
    "        response = generate_response(model, tokenizer, test[\"q\"], max_new_tokens=50)\n",
    "        response_lower = response.lower()\n",
    "        \n",
    "        # 1. Instruction following\n",
    "        is_answering = len(response.split()) > 2 and not response.startswith(\"Human\")\n",
    "        results[\"instruction_following\"].append(1 if is_answering else 0)\n",
    "        \n",
    "        # 2. Factual accuracy\n",
    "        is_factual = any(kw in response_lower for kw in test[\"keywords\"])\n",
    "        results[\"factual_accuracy\"].append(1 if is_factual else 0)\n",
    "        \n",
    "        # 3. Coherence\n",
    "        words = response.split()\n",
    "        unique_ratio = len(set(words)) / len(words) if words else 0\n",
    "        is_coherent = unique_ratio > 0.5 and len(words) > 3\n",
    "        results[\"coherence\"].append(1 if is_coherent else 0)\n",
    "        \n",
    "        # 4. Appropriate length\n",
    "        is_appropriate_length = 5 <= len(words) <= 100\n",
    "        results[\"appropriate_length\"].append(1 if is_appropriate_length else 0)\n",
    "        \n",
    "        # 5. No hallucination\n",
    "        hallucination_markers = [\"i don't know\", \"error\", \"undefined\", \"null\"]\n",
    "        has_hallucination = any(m in response_lower for m in hallucination_markers)\n",
    "        results[\"no_hallucination\"].append(0 if has_hallucination else 1)\n",
    "    \n",
    "    scores = {k: sum(v)/len(v) for k, v in results.items()}\n",
    "    overall = sum(scores.values()) / len(scores)\n",
    "    \n",
    "    return {\"metrics\": scores, \"overall\": overall}\n",
    "\n",
    "# Run evaluation on all three models\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä INSTRUCTGPT-STYLE METRICS COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nEvaluating GPT-2 Base...\")\n",
    "base_metrics = instructgpt_style_evaluation(base_model, tokenizer, \"Base\")\n",
    "\n",
    "print(\"Evaluating InstructGPT SFT...\")\n",
    "sft_metrics = instructgpt_style_evaluation(sft_model, tokenizer, \"SFT\")\n",
    "\n",
    "ppo_metrics = None\n",
    "if ppo_eval_model:\n",
    "    print(\"Evaluating InstructGPT SFT+PPO...\")\n",
    "    ppo_metrics = instructgpt_style_evaluation(ppo_eval_model, tokenizer, \"PPO\")\n",
    "\n",
    "# Print comparison table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"{'Metric':<25} {'GPT-2 Base':>12} {'SFT':>12} {'SFT+PPO':>12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for metric in base_metrics['metrics']:\n",
    "    base_val = base_metrics['metrics'][metric]\n",
    "    sft_val = sft_metrics['metrics'][metric]\n",
    "    ppo_val = ppo_metrics['metrics'][metric] if ppo_metrics else \"-\"\n",
    "    \n",
    "    if ppo_metrics:\n",
    "        print(f\"{metric:<25} {base_val:>11.0%} {sft_val:>11.0%} {ppo_val:>11.0%}\")\n",
    "    else:\n",
    "        print(f\"{metric:<25} {base_val:>11.0%} {sft_val:>11.0%} {ppo_val:>12}\")\n",
    "\n",
    "print(\"-\"*70)\n",
    "if ppo_metrics:\n",
    "    print(f\"{'OVERALL':<25} {base_metrics['overall']:>11.0%} {sft_metrics['overall']:>11.0%} {ppo_metrics['overall']:>11.0%}\")\n",
    "else:\n",
    "    print(f\"{'OVERALL':<25} {base_metrics['overall']:>11.0%} {sft_metrics['overall']:>11.0%} {'-':>12}\")\n",
    "\n",
    "# Determine best model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if ppo_metrics:\n",
    "    all_models = [\n",
    "        (\"üî¥ GPT-2 Base\", base_metrics['overall']),\n",
    "        (\"üîµ InstructGPT SFT\", sft_metrics['overall']),\n",
    "        (\"üü¢ InstructGPT SFT+PPO\", ppo_metrics['overall']),\n",
    "    ]\n",
    "else:\n",
    "    all_models = [\n",
    "        (\"üî¥ GPT-2 Base\", base_metrics['overall']),\n",
    "        (\"üîµ InstructGPT SFT\", sft_metrics['overall']),\n",
    "    ]\n",
    "\n",
    "ranked = sorted(all_models, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nüèÜ FINAL RANKING (by InstructGPT metrics):\")\n",
    "for i, (name, score) in enumerate(ranked, 1):\n",
    "    medal = \"ü•á\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\"\n",
    "    print(f\"   {medal} #{i}: {name} - {score:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce66858",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Summary: Full InstructGPT Pipeline\n",
    "\n",
    "### Pipeline Stages Compared:\n",
    "\n",
    "| Stage | Model | Description |\n",
    "|-------|-------|-------------|\n",
    "| **Base** | GPT-2 | Original pretrained model |\n",
    "| **Step 1** | InstructGPT SFT | Supervised fine-tuning on demonstrations |\n",
    "| **Step 2** | Reward Model | Trained on human preference pairs |\n",
    "| **Step 3** | InstructGPT SFT+PPO | Full RLHF with PPO optimization |\n",
    "\n",
    "### InstructGPT RLHF Pipeline Implemented:\n",
    "\n",
    "| Step | Paper | Our Implementation | Status |\n",
    "|------|-------|-------------------|--------|\n",
    "| 1. SFT | GPT-3 + 13K demos | GPT-2 + 3K demos | ‚úÖ Complete |\n",
    "| 2. RM | Train on comparisons | Anthropic HH-RLHF | ‚úÖ Complete |\n",
    "| 3. PPO | Fine-tune with RM | TRL PPO Trainer | ‚úÖ Complete |\n",
    "\n",
    "### Key InstructGPT Techniques Applied:\n",
    "1. ‚úÖ **Bradley-Terry loss** for reward model training\n",
    "2. ‚úÖ **KL penalty** in PPO to prevent divergence from SFT\n",
    "3. ‚úÖ **Human preference data** (Anthropic HH-RLHF as proxy)\n",
    "4. ‚úÖ **Value head** for advantage estimation in PPO\n",
    "5. ‚úÖ **Comprehensive evaluation** on all three stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "54d882bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Full comparison saved to: ../outputs/rlhf_training/rlhf_full_comparison.json\n",
      "\n",
      "======================================================================\n",
      "üéâ INSTRUCTGPT FULL RLHF PIPELINE COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Pipeline Summary:\n",
      "  üì¶ Base Model:     GPT-2 (124M parameters)\n",
      "  üìö SFT Dataset:    Alpaca (3K filtered samples)\n",
      "  üèÜ RM Dataset:     Anthropic HH-RLHF (5000 pairs)\n",
      "\n",
      "Models Compared:\n",
      "  üî¥ GPT-2 Base:         88% (InstructGPT metrics)\n",
      "  üîµ InstructGPT SFT:    90% (InstructGPT metrics)\n",
      "  üü¢ InstructGPT SFT+PPO: 80% (InstructGPT metrics) \n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save comprehensive final summary\n",
    "final_results = {\n",
    "    \"methodology\": \"InstructGPT Full RLHF Pipeline (Ouyang et al., 2022)\",\n",
    "    \"paper_reference\": \"https://arxiv.org/abs/2203.02155\",\n",
    "    \"datasets\": {\n",
    "        \"sft_dataset\": \"tatsu-lab/alpaca (filtered)\",\n",
    "        \"reward_model_dataset\": \"Anthropic/hh-rlhf\",\n",
    "        \"rm_samples_used\": N_REWARD_SAMPLES,\n",
    "    },\n",
    "    \"models\": {\n",
    "        \"base_model\": BASE_MODEL_PATH,\n",
    "        \"sft_model\": SFT_MODEL_PATH,\n",
    "        \"reward_model\": \"custom GPT-2 based\" if not PRETRAINED_RM_LOADED else \"OpenAssistant\",\n",
    "        \"ppo_model\": ppo_path if Path(ppo_path).exists() else \"not trained\",\n",
    "    },\n",
    "    \"evaluation\": {\n",
    "        \"reward_scores\": {\n",
    "            \"base\": avg_base,\n",
    "            \"sft\": avg_sft,\n",
    "            \"ppo\": avg_ppo if ppo_eval_model else None,\n",
    "        },\n",
    "        \"instructgpt_metrics\": {\n",
    "            \"base\": base_metrics,\n",
    "            \"sft\": sft_metrics,\n",
    "            \"ppo\": ppo_metrics,\n",
    "        }\n",
    "    },\n",
    "    \"training_config\": {\n",
    "        \"ppo_steps\": N_PPO_STEPS if 'N_PPO_STEPS' in dir() else 0,\n",
    "        \"kl_coefficient\": ppo_config.kl_coef,\n",
    "        \"learning_rate\": ppo_config.learning_rate,\n",
    "        \"ppo_epochs\": ppo_config.num_ppo_epochs,\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = Path(f\"{OUTPUT_DIR}/rlhf_full_comparison.json\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump(final_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n‚úÖ Full comparison saved to: {summary_path}\")\n",
    "\n",
    "# Final message\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ INSTRUCTGPT FULL RLHF PIPELINE COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "Pipeline Summary:\n",
    "  üì¶ Base Model:     GPT-2 (124M parameters)\n",
    "  üìö SFT Dataset:    Alpaca (3K filtered samples)\n",
    "  üèÜ RM Dataset:     Anthropic HH-RLHF ({N_REWARD_SAMPLES} pairs)\n",
    "  \n",
    "Models Compared:\n",
    "  üî¥ GPT-2 Base:         {base_metrics['overall']:.0%} (InstructGPT metrics)\n",
    "  üîµ InstructGPT SFT:    {sft_metrics['overall']:.0%} (InstructGPT metrics)\n",
    "  üü¢ InstructGPT SFT+PPO: {ppo_metrics['overall']:.0%} (InstructGPT metrics) \n",
    "\"\"\" if ppo_metrics else f\"\"\"\n",
    "Pipeline Summary:\n",
    "  üì¶ Base Model:     GPT-2 (124M parameters)\n",
    "  üìö SFT Dataset:    Alpaca (3K filtered samples)\n",
    "  üèÜ RM Dataset:     Anthropic HH-RLHF ({N_REWARD_SAMPLES} pairs)\n",
    "  \n",
    "Models Compared:\n",
    "  üî¥ GPT-2 Base:         {base_metrics['overall']:.0%} (InstructGPT metrics)\n",
    "  üîµ InstructGPT SFT:    {sft_metrics['overall']:.0%} (InstructGPT metrics)\n",
    "  ‚ö†Ô∏è PPO model not trained\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc5533d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
