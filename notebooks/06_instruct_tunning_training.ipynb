{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f40e69",
   "metadata": {},
   "source": [
    "# ðŸš€ InstructGPT-Style SFT for GPT-2\n",
    "\n",
    "Based on **\"Training language models to follow instructions with human feedback\"** (Ouyang et al., 2022)\n",
    "- Paper: https://arxiv.org/abs/2203.02155\n",
    "\n",
    "## InstructGPT Methodology Summary\n",
    "\n",
    "### Paper Key Findings:\n",
    "1. **SFT Data**: ~13,000 labeler demonstrations of desired behavior\n",
    "2. **Training**: Supervised fine-tuning on GPT-3 (we adapt for GPT-2)\n",
    "3. **Pretraining Data Mix**: Mix small fraction of pretraining data during fine-tuning\n",
    "4. **KL Penalty**: Keep model close to original to prevent \"alignment tax\"\n",
    "5. **Learning Rate**: Cosine schedule with low LR to preserve capabilities\n",
    "6. **Epochs**: 16 epochs on small high-quality dataset (paper) â†’ we use 2-3 for GPT-2\n",
    "\n",
    "### Our Adaptation for GPT-2 (124M):\n",
    "\n",
    "| InstructGPT Paper | Our GPT-2 Adaptation | Rationale |\n",
    "|-------------------|----------------------|-----------|\n",
    "| 13,000 demonstrations | 3,000 filtered samples | Scale by model size ratio (~1/10) |\n",
    "| GPT-3 (175B) | GPT-2 (124M) | 1400x smaller model |\n",
    "| 16 epochs | 2 epochs | Smaller model overfits faster |\n",
    "| Cosine LR schedule | Cosine with LR=2e-5 | Standard for small models |\n",
    "| Pretraining mix | KL regularization | Simpler but same effect |\n",
    "| RLHF (Step 2-3) | SFT only | Focus on Step 1 |\n",
    "\n",
    "### Key Improvements Over Previous Attempts:\n",
    "\n",
    "| Issue | Previous | InstructGPT-Style |\n",
    "|-------|----------|-------------------|\n",
    "| Learning Rate | 1e-5 to 1e-4 | 2e-5 with cosine decay |\n",
    "| Samples | 22-200 | 3,000 curated |\n",
    "| Templates | 5 different | 1 consistent |\n",
    "| Epochs | 2-3 | 2 (with early stopping) |\n",
    "| Regularization | None | KL penalty (Î²=0.1) + dropout |\n",
    "| Data Quality | Random | Filtered for task complexity |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5a73e5",
   "metadata": {},
   "source": [
    "## InstructGPT Paper: 3-Step RLHF Pipeline\n",
    "\n",
    "The paper describes a 3-step process (we implement Step 1):\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Step 1: Supervised Fine-Tuning (SFT) â† WE ARE HERE                     â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚\n",
    "â”‚  â€¢ Collect labeler demonstrations of desired behavior                   â”‚\n",
    "â”‚  â€¢ Fine-tune pretrained model on these demonstrations                   â”‚\n",
    "â”‚  â€¢ Use cosine learning rate schedule                                    â”‚\n",
    "â”‚  â€¢ 16 epochs on ~13K samples (paper)                                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Step 2: Reward Model Training (Not Implemented)                        â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚\n",
    "â”‚  â€¢ Collect comparison data (labelers rank outputs)                      â”‚\n",
    "â”‚  â€¢ Train reward model to predict human preferences                      â”‚\n",
    "â”‚  â€¢ RM learns: score(prompt, response) â†’ scalar reward                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Step 3: PPO Fine-Tuning (Not Implemented)                              â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚\n",
    "â”‚  â€¢ Use reward model as reward signal                                    â”‚\n",
    "â”‚  â€¢ Fine-tune SFT model with PPO                                         â”‚\n",
    "â”‚  â€¢ Add KL penalty: R = R_Ïˆ(x,y) - Î²Â·log(Ï€(y|x)/p(y|x))                  â”‚\n",
    "â”‚  â€¢ Mix in pretraining data to prevent forgetting                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Paper Quote on Pretraining Data Mix:\n",
    "> \"During RL fine-tuning we mix in a small fraction of the original data used to train GPT-3, and train on this data using the normal log likelihood maximization. This roughly maintains performance on safety and human preferences, while mitigating performance decreases on academic tasks.\"\n",
    "\n",
    "We simulate this effect using **KL regularization** which penalizes divergence from original weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da60510",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5572408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.4.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 638, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1971, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3123, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3178, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3641, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3701, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/tg/5d928d1d0z15w2_p1z2f224r0000gr/T/ipykernel_88006/2131112418.py\", line 14, in <module>\n",
      "    import torch\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.2.2\n",
      "Device: CPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# SSL workaround\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "826b3a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Improved Configuration:\n",
      "{\n",
      "  \"data\": {\n",
      "    \"source\": \"alpaca-cleaned (filtered)\",\n",
      "    \"n_samples\": 3000,\n",
      "    \"max_length\": 256,\n",
      "    \"template\": \"Question: {instruction}\\nAnswer:\",\n",
      "    \"filter_criteria\": {\n",
      "      \"max_response_tokens\": 100,\n",
      "      \"task_types\": [\n",
      "        \"qa\",\n",
      "        \"classification\",\n",
      "        \"short_generation\"\n",
      "      ],\n",
      "      \"exclude_complex\": true\n",
      "    }\n",
      "  },\n",
      "  \"training\": {\n",
      "    \"learning_rate\": 1e-06,\n",
      "    \"batch_size\": 8,\n",
      "    \"gradient_accumulation\": 2,\n",
      "    \"epochs\": 1,\n",
      "    \"warmup_steps\": 100,\n",
      "    \"weight_decay\": 0.1,\n",
      "    \"max_grad_norm\": 0.5,\n",
      "    \"lr_scheduler\": \"linear\"\n",
      "  },\n",
      "  \"regularization\": {\n",
      "    \"dropout\": 0.1,\n",
      "    \"data_mixing_ratio\": 0.3,\n",
      "    \"kl_penalty\": 0.1\n",
      "  },\n",
      "  \"lora\": {\n",
      "    \"r\": 4,\n",
      "    \"alpha\": 8,\n",
      "    \"dropout\": 0.2,\n",
      "    \"target_modules\": [\n",
      "      \"c_attn\",\n",
      "      \"c_proj\",\n",
      "      \"mlp.c_fc\",\n",
      "      \"mlp.c_proj\"\n",
      "    ],\n",
      "    \"learning_rate\": 5e-05\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load improved configuration\n",
    "config_path = Path(\"../configs/improved_gpt2_config.json\")\n",
    "with open(config_path) as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "print(\"Loaded Improved Configuration:\")\n",
    "print(json.dumps(CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad601f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenizer loaded: 50257 tokens\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "BASE_MODEL_PATH = \"../models/gpt2\"\n",
    "OUTPUT_DIR = \"../outputs/improved_training\"\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load tokenizer and base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"âœ… Tokenizer loaded: {tokenizer.vocab_size} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dda365",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Dataset Curation (Filtered for GPT-2)\n",
    "\n",
    "Key improvements:\n",
    "- Filter for **simple tasks** (Q&A, classification)\n",
    "- **Short responses** (max 100 tokens)\n",
    "- **Single template** (no gradient conflicts)\n",
    "- **2000+ samples** for adequate learning signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "615f338f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Filter function defined\n"
     ]
    }
   ],
   "source": [
    "def filter_for_gpt2(example: Dict) -> bool:\n",
    "    \"\"\"\n",
    "    Filter dataset for GPT-2's limited capacity.\n",
    "    Only keep simple, short examples.\n",
    "    \"\"\"\n",
    "    instruction = example.get(\"instruction\", \"\")\n",
    "    output = example.get(\"output\", \"\")\n",
    "    inp = example.get(\"input\", \"\")\n",
    "    \n",
    "    # Skip if empty\n",
    "    if not instruction or not output:\n",
    "        return False\n",
    "    \n",
    "    # Skip very long responses (>100 tokens ~400 chars)\n",
    "    if len(output) > 400:\n",
    "        return False\n",
    "    \n",
    "    # Skip very short responses (likely need context)\n",
    "    if len(output) < 10:\n",
    "        return False\n",
    "    \n",
    "    # Skip code-related tasks\n",
    "    code_keywords = [\"code\", \"python\", \"javascript\", \"function\", \"class\", \n",
    "                    \"def \", \"import \", \"```\", \"programming\"]\n",
    "    text = (instruction + output).lower()\n",
    "    if any(kw in text for kw in code_keywords):\n",
    "        return False\n",
    "    \n",
    "    # Skip complex reasoning tasks\n",
    "    complex_keywords = [\"step by step\", \"explain how\", \"analyze\", \"compare and contrast\",\n",
    "                       \"multiple steps\", \"calculate\", \"solve\"]\n",
    "    if any(kw in text for kw in complex_keywords):\n",
    "        return False\n",
    "    \n",
    "    # Skip tasks requiring external context\n",
    "    if len(inp) > 200:  # Large input context\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "print(\"âœ… Filter function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a34f820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'tatsu-lab/alpaca' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Alpaca dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52002/52002 [00:00<00:00, 162529.66 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 52002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52002/52002 [00:01<00:00, 37657.75 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset size: 30476\n",
      "\n",
      "âœ… Using 3000 curated samples\n"
     ]
    }
   ],
   "source": [
    "# Fix SSL and httpx issues\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import httpx\n",
    "httpx._client.Client.__init__.__globals__['DEFAULT_TIMEOUT_CONFIG'] = httpx.Timeout(30.0)\n",
    "\n",
    "# Monkey-patch httpx to handle SSL\n",
    "original_init = httpx.Client.__init__\n",
    "def patched_init(self, *args, **kwargs):\n",
    "    kwargs.setdefault('verify', False)\n",
    "    original_init(self, *args, **kwargs)\n",
    "httpx.Client.__init__ = patched_init\n",
    "\n",
    "# Load and filter dataset\n",
    "print(\"Loading Alpaca dataset...\")\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", trust_remote_code=True)\n",
    "print(f\"Original dataset size: {len(dataset)}\")\n",
    "\n",
    "# Apply filter\n",
    "filtered_dataset = dataset.filter(filter_for_gpt2)\n",
    "print(f\"Filtered dataset size: {len(filtered_dataset)}\")\n",
    "\n",
    "# Take subset for training (2000-3000 samples as recommended)\n",
    "N_SAMPLES = min(3000, len(filtered_dataset))\n",
    "filtered_dataset = filtered_dataset.shuffle(seed=42).select(range(N_SAMPLES))\n",
    "print(f\"\\nâœ… Using {N_SAMPLES} curated samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3068626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Response Length Statistics:\n",
      "  Min: 10 chars\n",
      "  Max: 400 chars\n",
      "  Mean: 143 chars\n",
      "\n",
      "ðŸ“ Sample Examples:\n",
      "\n",
      "--- Example 1 ---\n",
      "Instruction: Edit this sentence for grammar, syntax, and style â€œIt can incredibly difficult to decideâ€...\n",
      "Output: It can be incredibly difficult to decide....\n",
      "\n",
      "--- Example 2 ---\n",
      "Instruction: Which is the best way to learn a new language?...\n",
      "Output: The best way to learn a new language is through total immersion. This can be accomplished by traveli...\n",
      "\n",
      "--- Example 3 ---\n",
      "Instruction: List 5 strategies for better organization and time management....\n",
      "Output: 1. Set realistic goals and prioritize tasks.\n",
      "2. Use a calendar to track meetings and deadlines.\n",
      "3. B...\n"
     ]
    }
   ],
   "source": [
    "# Show sample distribution\n",
    "response_lengths = [len(ex[\"output\"]) for ex in filtered_dataset]\n",
    "print(f\"\\nðŸ“Š Response Length Statistics:\")\n",
    "print(f\"  Min: {min(response_lengths)} chars\")\n",
    "print(f\"  Max: {max(response_lengths)} chars\")\n",
    "print(f\"  Mean: {sum(response_lengths)/len(response_lengths):.0f} chars\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nðŸ“ Sample Examples:\")\n",
    "for i in range(3):\n",
    "    ex = filtered_dataset[i]\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Instruction: {ex['instruction'][:100]}...\")\n",
    "    print(f\"Output: {ex['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6b5a36",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Single Template Formatting\n",
    "\n",
    "**Critical improvement**: Use ONE consistent template to avoid gradient conflicts.\n",
    "\n",
    "```\n",
    "Question: {instruction}\n",
    "Answer: {output}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0826053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template Preview:\n",
      "==================================================\n",
      "Question: Edit this sentence for grammar, syntax, and style â€œIt can incredibly difficult to decideâ€\n",
      "Answer: It can be incredibly difficult to decide.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Simple, consistent template\n",
    "TEMPLATE = \"\"\"Question: {instruction}\n",
    "Answer: {output}\"\"\"\n",
    "\n",
    "def format_example(example: Dict) -> str:\n",
    "    \"\"\"Format example with single consistent template.\"\"\"\n",
    "    instruction = example[\"instruction\"]\n",
    "    \n",
    "    # Append input if present\n",
    "    if example.get(\"input\"):\n",
    "        instruction = f\"{instruction}\\nContext: {example['input']}\"\n",
    "    \n",
    "    return TEMPLATE.format(\n",
    "        instruction=instruction,\n",
    "        output=example[\"output\"]\n",
    "    )\n",
    "\n",
    "# Test formatting\n",
    "print(\"Template Preview:\")\n",
    "print(\"=\" * 50)\n",
    "print(format_example(filtered_dataset[0]))\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "827da9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training dataset created: 3000 samples\n"
     ]
    }
   ],
   "source": [
    "class GPT2Dataset(Dataset):\n",
    "    \"\"\"Custom dataset for GPT-2 fine-tuning.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length=256):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = format_example(self.data[idx])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding[\"input_ids\"].squeeze()\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": input_ids.clone()\n",
    "        }\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = GPT2Dataset(filtered_dataset, tokenizer, max_length=256)\n",
    "print(f\"âœ… Training dataset created: {len(train_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e974b6b",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. KL Regularization Implementation\n",
    "\n",
    "To prevent catastrophic forgetting, we add a KL divergence penalty:\n",
    "\n",
    "$$\\mathcal{L}_{total} = \\mathcal{L}_{task} + \\beta \\cdot KL(P_{new} || P_{original})$$\n",
    "\n",
    "This keeps the new model's output distribution close to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7042021d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… KLRegularizedTrainer defined\n"
     ]
    }
   ],
   "source": [
    "class KLRegularizedTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom trainer with KL divergence regularization.\n",
    "    Keeps model close to original distribution to prevent forgetting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, reference_model=None, kl_weight=0.1, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.reference_model = reference_model\n",
    "        self.kl_weight = kl_weight\n",
    "        \n",
    "        # Freeze reference model\n",
    "        if self.reference_model is not None:\n",
    "            self.reference_model.eval()\n",
    "            for param in self.reference_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(f\"âœ… Reference model frozen for KL regularization (Î²={kl_weight})\")\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Standard forward pass\n",
    "        outputs = model(**inputs)\n",
    "        task_loss = outputs.loss\n",
    "        \n",
    "        # Add KL regularization if reference model exists\n",
    "        if self.reference_model is not None and self.kl_weight > 0:\n",
    "            with torch.no_grad():\n",
    "                ref_outputs = self.reference_model(**inputs)\n",
    "                ref_logits = ref_outputs.logits\n",
    "            \n",
    "            # Compute KL divergence\n",
    "            new_probs = F.log_softmax(outputs.logits, dim=-1)\n",
    "            ref_probs = F.softmax(ref_logits, dim=-1)\n",
    "            \n",
    "            kl_loss = F.kl_div(new_probs, ref_probs, reduction=\"batchmean\")\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = task_loss + self.kl_weight * kl_loss\n",
    "        else:\n",
    "            total_loss = task_loss\n",
    "        \n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n",
    "\n",
    "print(\"âœ… KLRegularizedTrainer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52a36f9",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Setup with Improved LoRA\n",
    "\n",
    "LoRA improvements:\n",
    "- **Lower rank** (r=4 vs 16) - less overfitting\n",
    "- **More target modules** - include MLP layers\n",
    "- **Higher dropout** - better regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efced59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148/148 [00:00<00:00, 450.26it/s, Materializing param=transformer.wte.weight]             \n",
      "GPT2LMHeadModel LOAD REPORT from: ../models/gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148/148 [00:00<00:00, 419.76it/s, Materializing param=transformer.wte.weight]             \n",
      "GPT2LMHeadModel LOAD REPORT from: ../models/gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Models loaded\n",
      "  Trainable model params: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "# Load base model (for training)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    torch_dtype=torch.float32,\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Load reference model (frozen, for KL regularization)\n",
    "reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    torch_dtype=torch.float32,\n",
    ")\n",
    "\n",
    "print(f\"âœ… Models loaded\")\n",
    "print(f\"  Trainable model params: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f530952e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Parameter Summary:\n",
      "  Total params: 125,029,632\n",
      "  Trainable params: 589,824\n",
      "  Trainable %: 0.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Improved LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=4,                    # Lower rank (was 16)\n",
    "    lora_alpha=8,           # Lower alpha (was 32)\n",
    "    lora_dropout=0.2,       # Higher dropout (was 0.1)\n",
    "    target_modules=[        # Include MLP layers\n",
    "        \"c_attn\",\n",
    "        \"c_proj\",\n",
    "        \"c_fc\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nðŸ“Š Parameter Summary:\")\n",
    "print(f\"  Total params: {total_params:,}\")\n",
    "print(f\"  Trainable params: {trainable_params:,}\")\n",
    "print(f\"  Trainable %: {100 * trainable_params / total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035ea128",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. InstructGPT-Style Training Configuration\n",
    "\n",
    "### Paper-Aligned Changes:\n",
    "- **Cosine LR Schedule**: Paper uses cosine annealing (vs linear decay)\n",
    "- **LR: 2e-5**: Standard for small models, with cosine decay\n",
    "- **2 Epochs**: Paper used 16 on 13K samples; we use 2 on 3K (similar exposure)\n",
    "- **Warmup**: 10% of steps (paper methodology)\n",
    "- **Weight Decay**: 0.01 (standard regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43349952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… InstructGPT-style training arguments configured\n",
      "  Learning rate: 2e-05 (with cosine decay)\n",
      "  Epochs: 2\n",
      "  LR Scheduler: SchedulerType.COSINE\n",
      "  Warmup ratio: 0.1\n",
      "  Effective batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# InstructGPT-style training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # InstructGPT-aligned parameters\n",
    "    learning_rate=2e-5,          # Standard SFT LR\n",
    "    num_train_epochs=2,          # Paper: 16 epochs on 13K â†’ we: 2 on 3K\n",
    "    weight_decay=0.01,           # Standard regularization\n",
    "    max_grad_norm=1.0,           # Standard gradient clipping\n",
    "    \n",
    "    # Batch settings\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Effective batch = 16\n",
    "    \n",
    "    # Cosine LR schedule (paper methodology)\n",
    "    warmup_ratio=0.1,            # 10% warmup steps\n",
    "    lr_scheduler_type=\"cosine\",  # Paper uses cosine schedule\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    \n",
    "    # Disable evaluation (CPU training)\n",
    "    eval_strategy=\"no\",\n",
    "    \n",
    "    # CPU settings\n",
    "    use_cpu=True,\n",
    "    fp16=False,\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    ")\n",
    "\n",
    "print(\"âœ… InstructGPT-style training arguments configured\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate} (with cosine decay)\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  LR Scheduler: {training_args.lr_scheduler_type}\")\n",
    "print(f\"  Warmup ratio: {training_args.warmup_ratio}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54cbb45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Training Estimates:\n",
      "  Samples: 3000\n",
      "  Steps per epoch: 187\n",
      "  Total steps: 374\n",
      "  Estimated time: ~1 minutes\n"
     ]
    }
   ],
   "source": [
    "# Calculate training estimates\n",
    "n_samples = len(train_dataset)\n",
    "batch_size = training_args.per_device_train_batch_size\n",
    "grad_accum = training_args.gradient_accumulation_steps\n",
    "epochs = training_args.num_train_epochs\n",
    "\n",
    "steps_per_epoch = n_samples // (batch_size * grad_accum)\n",
    "total_steps = steps_per_epoch * epochs\n",
    "\n",
    "# Estimate time (based on previous runs: ~0.15s per step on CPU)\n",
    "est_time_min = total_steps * 0.15 / 60\n",
    "\n",
    "print(f\"\\nðŸ“Š Training Estimates:\")\n",
    "print(f\"  Samples: {n_samples}\")\n",
    "print(f\"  Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Estimated time: ~{est_time_min:.0f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a02788",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Training with KL Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "426eb1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Reference model frozen for KL regularization (Î²=0.1)\n",
      "\n",
      "============================================================\n",
      "ðŸš€ STARTING IMPROVED TRAINING\n",
      "============================================================\n",
      "\n",
      "Improvements Applied:\n",
      "  âœ“ Curated dataset: 3000 samples\n",
      "  âœ“ Single template (no gradient conflicts)\n",
      "  âœ“ Low LR: 2e-05\n",
      "  âœ“ 1 epoch (prevent overfitting)\n",
      "  âœ“ KL regularization: Î²=0.1\n",
      "  âœ“ LoRA r=4 (lower rank)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create KL-regularized trainer\n",
    "trainer = KLRegularizedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    reference_model=reference_model,\n",
    "    kl_weight=CONFIG[\"regularization\"][\"kl_penalty\"],  # 0.1 from config\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸš€ STARTING IMPROVED TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nImprovements Applied:\")\n",
    "print(f\"  âœ“ Curated dataset: {len(train_dataset)} samples\")\n",
    "print(f\"  âœ“ Single template (no gradient conflicts)\")\n",
    "print(f\"  âœ“ Low LR: {training_args.learning_rate}\")\n",
    "print(f\"  âœ“ 1 epoch (prevent overfitting)\")\n",
    "print(f\"  âœ“ KL regularization: Î²={CONFIG['regularization']['kl_penalty']}\")\n",
    "print(f\"  âœ“ LoRA r=4 (lower rank)\")\n",
    "print(f\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1cc258e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='376' max='376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [376/376 3:16:35, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>55.203252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>45.706738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>37.629385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>31.912324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>29.950393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>28.735977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>28.463452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "âœ… TRAINING COMPLETE!\n",
      "============================================================\n",
      "  Time: 197.2 minutes\n",
      "  Final loss: 36.1814\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"âœ… TRAINING COMPLETE!\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"  Time: {elapsed/60:.1f} minutes\")\n",
    "print(f\"  Final loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9699f3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Adapter saved to: ../outputs/improved_training/adapter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Merged model saved to: ../outputs/improved_training/merged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "adapter_path = f\"{OUTPUT_DIR}/adapter\"\n",
    "model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "print(f\"âœ… Adapter saved to: {adapter_path}\")\n",
    "\n",
    "# Save merged model\n",
    "merged_path = f\"{OUTPUT_DIR}/merged\"\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(merged_path)\n",
    "tokenizer.save_pretrained(merged_path)\n",
    "print(f\"âœ… Merged model saved to: {merged_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c62703",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ec4290d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148/148 [00:00<00:00, 292.38it/s, Materializing param=transformer.wte.weight]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded for evaluation\n"
     ]
    }
   ],
   "source": [
    "# Load improved model for evaluation\n",
    "eval_model = AutoModelForCausalLM.from_pretrained(\n",
    "    merged_path,\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "eval_model.eval()\n",
    "\n",
    "print(\"âœ… Model loaded for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "178e3b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generation function defined\n"
     ]
    }
   ],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_new_tokens=100):\n",
    "    \"\"\"Generate response with same format as training.\"\"\"\n",
    "    # Format prompt like training data\n",
    "    formatted = f\"Question: {prompt}\\nAnswer:\"\n",
    "    \n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract answer part\n",
    "    if \"Answer:\" in response:\n",
    "        response = response.split(\"Answer:\")[1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"âœ… Generation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7440714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "IMPROVED MODEL RESPONSES\n",
      "============================================================\n",
      "\n",
      "Q: What is the capital of France?\n",
      "A: The French government has an internal budget that can be divided into three categories. Each category, from GDP to foreign exchange reserves, should have a minimum amount needed for its upkeep by fisc...\n",
      "----------------------------------------\n",
      "\n",
      "Q: Who wrote Romeo and Juliet?\n",
      "A: The author of the first two books, William Shakespeare.\n",
      "----------------------------------------\n",
      "\n",
      "Q: What is 2 + 2?\n",
      "A: A 3+2 = 1.\n",
      "\n",
      "Â  Â (I think that the first two numbers are not really needed, but I've been thinking about it for a while now and figured out how to make them work well).\n",
      "----------------------------------------\n",
      "\n",
      "Q: What color is the sky?\n",
      "A: The Milky Way.\n",
      "----------------------------------------\n",
      "\n",
      "Q: Name a fruit that is red.\n",
      "A: Red grapes are found in many countries, but they tend to be the most common type of grapefruit eaten by people who want their favorite flavor and color more than other types of fruits like apples or t...\n",
      "----------------------------------------\n",
      "\n",
      "Q: What is the largest planet in our solar system?\n",
      "A: The nearest known dwarf star, Europa (Jupiters 9) lies at 740 light-years from Earth.\n",
      "----------------------------------------\n",
      "\n",
      "Q: What language do people speak in Spain?\n",
      "A: Spanish is the second most spoken of languages on earth. It's a major part of our culture and we have this wonderful tradition about that, which goes back to when there were no formal traditions like ...\n",
      "----------------------------------------\n",
      "\n",
      "Q: Is water wet?\n",
      "A: Water is always damp, and can cause a headache if it's not washed regularly. But the idea of dry soil may be better than nothing at all!\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test queries (same as evaluation notebook)\n",
    "test_queries = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Who wrote Romeo and Juliet?\",\n",
    "    \"What is 2 + 2?\",\n",
    "    \"What color is the sky?\",\n",
    "    \"Name a fruit that is red.\",\n",
    "    \"What is the largest planet in our solar system?\",\n",
    "    \"What language do people speak in Spain?\",\n",
    "    \"Is water wet?\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPROVED MODEL RESPONSES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "improved_results = []\n",
    "for query in test_queries:\n",
    "    response = generate_response(eval_model, tokenizer, query)\n",
    "    improved_results.append({\"query\": query, \"response\": response})\n",
    "    \n",
    "    print(f\"\\nQ: {query}\")\n",
    "    print(f\"A: {response[:200]}{'...' if len(response) > 200 else ''}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "917e02d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMPARISON WITH PREVIOUS STAGES\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148/148 [00:00<00:00, 408.99it/s, Materializing param=transformer.wte.weight]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Query: What is the capital of France?\n",
      "==================================================\n",
      "\n",
      "ðŸ“› Stage 2 (before): The French Republic consists mostly in cities and towns, but it has several other important territories as well. It was founded by Charles II (1608â€“17...\n",
      "\n",
      "âœ… Improved (after): The French government has an internal budget that can be divided into three categories. Each category, from GDP to foreign exchange reserves, should h...\n",
      "\n",
      "==================================================\n",
      "Query: Who wrote Romeo and Juliet?\n",
      "==================================================\n",
      "\n",
      "ðŸ“› Stage 2 (before): The author of the first two novels, Philip Marlowe and Thomas Hardy. He was born in 1715 to a wealthy family on Long Island. His father died when he r...\n",
      "\n",
      "âœ… Improved (after): The author of the first two books, William Shakespeare....\n",
      "\n",
      "==================================================\n",
      "Query: What is 2 + 2?\n",
      "==================================================\n",
      "\n",
      "ðŸ“› Stage 2 (before): It's a shorthand for \"3 plus 1\" or 3 minus 0.1 (0.2) in English, which means that two equal numbers are equivalent to one and the same number of negat...\n",
      "\n",
      "âœ… Improved (after): A 3+2 = 1.\n",
      "\n",
      "Â  Â (I think that the first two numbers are not really needed, but I've been thinking about it for a while now and figured out how to make ...\n"
     ]
    }
   ],
   "source": [
    "# Compare with previous stages\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON WITH PREVIOUS STAGES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load Stage 2 (worst performer) for comparison\n",
    "stage2_path = \"../outputs/stage2_instruction/model\"\n",
    "if Path(stage2_path).exists():\n",
    "    stage2_model = AutoModelForCausalLM.from_pretrained(stage2_path, torch_dtype=torch.float32)\n",
    "    stage2_model.eval()\n",
    "    \n",
    "    # Compare on same queries\n",
    "    for i, query in enumerate(test_queries[:3]):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Stage 2 response\n",
    "        s2_resp = generate_response(stage2_model, tokenizer, query)\n",
    "        print(f\"\\nðŸ“› Stage 2 (before): {s2_resp[:150]}...\")\n",
    "        \n",
    "        # Improved response\n",
    "        print(f\"\\nâœ… Improved (after): {improved_results[i]['response'][:150]}...\")\n",
    "    \n",
    "    del stage2_model\n",
    "else:\n",
    "    print(\"Stage 2 model not found for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb2dcbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMPARISON WITH BASE GPT-2\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148/148 [00:00<00:00, 519.98it/s, Materializing param=transformer.wte.weight]             \n",
      "GPT2LMHeadModel LOAD REPORT from: ../models/gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Side-by-Side Comparison:\n",
      "\n",
      "============================================================\n",
      "Q: What is the capital of France?\n",
      "============================================================\n",
      "\n",
      "ðŸ”µ Base GPT-2:      The French government has about $3 billion in assets. That's a lot to spend on infrastructure, and there are more than 1...\n",
      "\n",
      "ðŸŸ¢ InstructGPT-SFT: The French government has an internal budget that can be divided into three categories. Each category, from GDP to forei...\n",
      "\n",
      "============================================================\n",
      "Q: Who wrote Romeo and Juliet?\n",
      "============================================================\n",
      "\n",
      "ðŸ”µ Base GPT-2:      The author of the novel, William Shakespeare. He is a prolific writer in his own right who has been writing for several ...\n",
      "\n",
      "ðŸŸ¢ InstructGPT-SFT: The author of the first two books, William Shakespeare....\n",
      "\n",
      "============================================================\n",
      "Q: What is 2 + 2?\n",
      "============================================================\n",
      "\n",
      "ðŸ”µ Base GPT-2:      A 1-3 digit number represents the \"number of years.\" The decimal value indicates a year. One can only use it to represen...\n",
      "\n",
      "ðŸŸ¢ InstructGPT-SFT: A 3+2 = 1.\n",
      "\n",
      "Â  Â (I think that the first two numbers are not really needed, but I've been thinking about it for a while no...\n",
      "\n",
      "============================================================\n",
      "Q: What color is the sky?\n",
      "============================================================\n",
      "\n",
      "ðŸ”µ Base GPT-2:      The colors of Earth and Mars are represented by red, blue or green. Moon's brightness in sunlight can be colored using a...\n",
      "\n",
      "ðŸŸ¢ InstructGPT-SFT: The Milky Way....\n",
      "\n",
      "============================================================\n",
      "Q: Name a fruit that is red.\n",
      "============================================================\n",
      "\n",
      "ðŸ”µ Base GPT-2:      Yes, it does! You can choose any of the three flavors and then you'll be able to decide which one should taste better wi...\n",
      "\n",
      "ðŸŸ¢ InstructGPT-SFT: Red grapes are found in many countries, but they tend to be the most common type of grapefruit eaten by people who want ...\n",
      "\n",
      "============================================================\n",
      "Q: What is the largest planet in our solar system?\n",
      "============================================================\n",
      "\n",
      "ðŸ”µ Base GPT-2:      The biggest, and probably most important one. Jupiter has a diameter of 2/3 Earth's radius; its mass at this distance me...\n",
      "\n",
      "ðŸŸ¢ InstructGPT-SFT: The nearest known dwarf star, Europa (Jupiters 9) lies at 740 light-years from Earth....\n",
      "\n",
      "============================================================\n",
      "Q: What language do people speak in Spain?\n",
      "============================================================\n",
      "\n",
      "ðŸ”µ Base GPT-2:      It is not hard to understand what it means. People who have a Spanish education are able to learn about the history of E...\n",
      "\n",
      "ðŸŸ¢ InstructGPT-SFT: Spanish is the second most spoken of languages on earth. It's a major part of our culture and we have this wonderful tra...\n",
      "\n",
      "============================================================\n",
      "Q: Is water wet?\n",
      "============================================================\n",
      "\n",
      "ðŸ”µ Base GPT-2:      Yes, but it is not dry. It will become damp if the surface of your body does NOT move in such a way that you do need to ...\n",
      "\n",
      "ðŸŸ¢ InstructGPT-SFT: Water is always damp, and can cause a headache if it's not washed regularly. But the idea of dry soil may be better than...\n",
      "\n",
      "\n",
      "============================================================\n",
      "BASE GPT-2 INSTRUCTGPT-STYLE METRICS\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š InstructGPT-Style Evaluation\n",
      "============================================================\n",
      "\n",
      "ðŸ“ˆ InstructGPT-Style Metrics:\n",
      "  Instruction Following: 100%\n",
      "  Factual Accuracy:      50%\n",
      "  Coherence:             100%\n",
      "  Appropriate Length:    100%\n",
      "  No Hallucination:      88%\n",
      "\n",
      "  ðŸŽ¯ Overall Score:       88%\n",
      "\n",
      "\n",
      "ðŸ“ˆ METRICS COMPARISON TABLE:\n",
      "============================================================\n",
      "Metric                         Base GPT-2 InstructGPT-SFT\n",
      "------------------------------------------------------------\n",
      "instruction_following               100%           100% =\n",
      "factual_accuracy                     50%            25% â†“\n",
      "coherence                           100%            88% â†“\n",
      "appropriate_length                  100%            75% â†“\n",
      "no_hallucination                     88%           100% â†‘\n",
      "------------------------------------------------------------\n",
      "OVERALL                              88%            78%\n"
     ]
    }
   ],
   "source": [
    "# Compare with Base GPT-2 model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON WITH BASE GPT-2\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load base GPT-2 model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_PATH, torch_dtype=torch.float32)\n",
    "base_model.eval()\n",
    "\n",
    "base_results = []\n",
    "for query in test_queries:\n",
    "    base_resp = generate_response(base_model, tokenizer, query)\n",
    "    base_results.append({\"query\": query, \"response\": base_resp})\n",
    "\n",
    "# Side-by-side comparison\n",
    "print(\"\\nðŸ“Š Side-by-Side Comparison:\")\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Q: {query}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nðŸ”µ Base GPT-2:      {base_results[i]['response'][:120]}...\")\n",
    "    print(f\"\\nðŸŸ¢ InstructGPT-SFT: {improved_results[i]['response'][:120]}...\")\n",
    "\n",
    "# Run InstructGPT-style evaluation on base model\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"BASE GPT-2 INSTRUCTGPT-STYLE METRICS\")\n",
    "print(\"=\"*60)\n",
    "base_metrics = instructgpt_style_evaluation(base_model, tokenizer, test_queries)\n",
    "\n",
    "# Comparison table\n",
    "print(\"\\n\\nðŸ“ˆ METRICS COMPARISON TABLE:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<25} {'Base GPT-2':>15} {'InstructGPT-SFT':>15}\")\n",
    "print(\"-\"*60)\n",
    "for metric in instructgpt_metrics['metrics']:\n",
    "    base_val = base_metrics['metrics'][metric]\n",
    "    improved_val = instructgpt_metrics['metrics'][metric]\n",
    "    diff = improved_val - base_val\n",
    "    arrow = \"â†‘\" if diff > 0 else \"â†“\" if diff < 0 else \"=\"\n",
    "    print(f\"{metric:<25} {base_val:>14.0%} {improved_val:>14.0%} {arrow}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'OVERALL':<25} {base_metrics['overall']:>14.0%} {instructgpt_metrics['overall']:>14.0%}\")\n",
    "\n",
    "del base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13bd3b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "QUALITY METRICS\n",
      "============================================================\n",
      "\n",
      "Average Quality Score: 0.91\n",
      "  (1.0 = perfect, 0.1 = degenerate)\n",
      "\n",
      "Per-query scores:\n",
      "  1. âœ… Score: 1.00 - What is the capital of France?\n",
      "  2. âœ… Score: 0.89 - Who wrote Romeo and Juliet?\n",
      "  3. âœ… Score: 0.97 - What is 2 + 2?\n",
      "  4. âš ï¸ Score: 0.50 - What color is the sky?\n",
      "  5. âœ… Score: 0.98 - Name a fruit that is red.\n",
      "  6. âœ… Score: 1.00 - What is the largest planet in our solar \n",
      "  7. âœ… Score: 0.98 - What language do people speak in Spain?\n",
      "  8. âœ… Score: 1.00 - Is water wet?\n"
     ]
    }
   ],
   "source": [
    "# Quality metrics\n",
    "def simple_quality_check(response):\n",
    "    \"\"\"Basic quality heuristics.\"\"\"\n",
    "    # Check for repetition (degenerate output)\n",
    "    words = response.lower().split()\n",
    "    if len(words) > 5:\n",
    "        unique_ratio = len(set(words)) / len(words)\n",
    "    else:\n",
    "        unique_ratio = 1.0\n",
    "    \n",
    "    # Check response length (too short = bad, too long = bad)\n",
    "    length_ok = 5 < len(words) < 150\n",
    "    \n",
    "    # Check for common degenerate patterns\n",
    "    degenerate_patterns = [\n",
    "        \"the the the\",\n",
    "        \"is is is\",\n",
    "        \"and and and\",\n",
    "        \"\\n\\n\\n\\n\",\n",
    "    ]\n",
    "    has_degenerate = any(p in response.lower() for p in degenerate_patterns)\n",
    "    \n",
    "    return {\n",
    "        \"unique_ratio\": unique_ratio,\n",
    "        \"length_ok\": length_ok,\n",
    "        \"no_degenerate\": not has_degenerate,\n",
    "        \"quality_score\": unique_ratio * (1 if length_ok else 0.5) * (1 if not has_degenerate else 0.1)\n",
    "    }\n",
    "\n",
    "# Evaluate all responses\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUALITY METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "scores = []\n",
    "for result in improved_results:\n",
    "    metrics = simple_quality_check(result[\"response\"])\n",
    "    scores.append(metrics[\"quality_score\"])\n",
    "    \n",
    "avg_score = sum(scores) / len(scores)\n",
    "print(f\"\\nAverage Quality Score: {avg_score:.2f}\")\n",
    "print(f\"  (1.0 = perfect, 0.1 = degenerate)\")\n",
    "print(f\"\\nPer-query scores:\")\n",
    "for i, (result, score) in enumerate(zip(improved_results, scores)):\n",
    "    status = \"âœ…\" if score > 0.5 else \"âš ï¸\" if score > 0.2 else \"âŒ\"\n",
    "    print(f\"  {i+1}. {status} Score: {score:.2f} - {result['query'][:40]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a2168c",
   "metadata": {},
   "source": [
    "---\n",
    "## 8.1 InstructGPT-Style Evaluation Metrics\n",
    "\n",
    "The paper evaluates on several axes:\n",
    "1. **Helpfulness**: Does it follow instructions and provide useful information?\n",
    "2. **Truthfulness**: Does it avoid generating false statements? (TruthfulQA)\n",
    "3. **Harmlessness**: Does it avoid toxic/harmful content? (RealToxicityPrompts)\n",
    "4. **Hallucination Rate**: Does it make things up?\n",
    "\n",
    "We adapt these for GPT-2's capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4a46807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š InstructGPT-Style Evaluation\n",
      "============================================================\n",
      "\n",
      "ðŸ“ˆ InstructGPT-Style Metrics:\n",
      "  Instruction Following: 100%\n",
      "  Factual Accuracy:      25%\n",
      "  Coherence:             88%\n",
      "  Appropriate Length:    75%\n",
      "  No Hallucination:      100%\n",
      "\n",
      "  ðŸŽ¯ Overall Score:       78%\n",
      "\n",
      "âœ… InstructGPT-style evaluation complete\n"
     ]
    }
   ],
   "source": [
    "def instructgpt_style_evaluation(model, tokenizer, test_samples):\n",
    "    \"\"\"\n",
    "    InstructGPT-inspired evaluation metrics adapted for GPT-2.\n",
    "    \n",
    "    Paper evaluation axes:\n",
    "    1. Helpfulness - Does it follow instructions?\n",
    "    2. Truthfulness - Does it avoid false statements?  \n",
    "    3. Coherence - Is the output well-formed?\n",
    "    4. Appropriate length - Not too short, not rambling\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {\n",
    "        \"instruction_following\": [],\n",
    "        \"factual_accuracy\": [],\n",
    "        \"coherence\": [],\n",
    "        \"appropriate_length\": [],\n",
    "        \"no_hallucination\": [],\n",
    "    }\n",
    "    \n",
    "    # Test samples with known answers (TruthfulQA-style)\n",
    "    factual_tests = [\n",
    "        {\"q\": \"What is the capital of France?\", \"keywords\": [\"paris\"]},\n",
    "        {\"q\": \"What is 2 + 2?\", \"keywords\": [\"4\", \"four\"]},\n",
    "        {\"q\": \"What color is the sky?\", \"keywords\": [\"blue\"]},\n",
    "        {\"q\": \"Who wrote Romeo and Juliet?\", \"keywords\": [\"shakespeare\", \"william\"]},\n",
    "        {\"q\": \"What is the largest planet?\", \"keywords\": [\"jupiter\"]},\n",
    "        {\"q\": \"What language is spoken in Spain?\", \"keywords\": [\"spanish\"]},\n",
    "        {\"q\": \"How many days in a week?\", \"keywords\": [\"7\", \"seven\"]},\n",
    "        {\"q\": \"What is the boiling point of water?\", \"keywords\": [\"100\", \"212\", \"celsius\", \"fahrenheit\"]},\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nðŸ“Š InstructGPT-Style Evaluation\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for test in factual_tests:\n",
    "        response = generate_response(model, tokenizer, test[\"q\"], max_new_tokens=50)\n",
    "        response_lower = response.lower()\n",
    "        \n",
    "        # 1. Instruction following (did it attempt to answer?)\n",
    "        is_answering = len(response.split()) > 2 and not response.startswith(\"Question\")\n",
    "        results[\"instruction_following\"].append(1 if is_answering else 0)\n",
    "        \n",
    "        # 2. Factual accuracy (contains expected keywords?)\n",
    "        is_factual = any(kw in response_lower for kw in test[\"keywords\"])\n",
    "        results[\"factual_accuracy\"].append(1 if is_factual else 0)\n",
    "        \n",
    "        # 3. Coherence (no repetition, complete sentences)\n",
    "        words = response.split()\n",
    "        unique_ratio = len(set(words)) / len(words) if words else 0\n",
    "        is_coherent = unique_ratio > 0.5 and len(words) > 3\n",
    "        results[\"coherence\"].append(1 if is_coherent else 0)\n",
    "        \n",
    "        # 4. Appropriate length\n",
    "        is_appropriate_length = 5 <= len(words) <= 100\n",
    "        results[\"appropriate_length\"].append(1 if is_appropriate_length else 0)\n",
    "        \n",
    "        # 5. No obvious hallucination (check for contradictions/nonsense)\n",
    "        hallucination_markers = [\"i don't know\", \"error\", \"undefined\", \"null\"]\n",
    "        has_hallucination = any(m in response_lower for m in hallucination_markers)\n",
    "        results[\"no_hallucination\"].append(0 if has_hallucination else 1)\n",
    "    \n",
    "    # Calculate scores\n",
    "    scores = {k: sum(v)/len(v) for k, v in results.items()}\n",
    "    overall = sum(scores.values()) / len(scores)\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ InstructGPT-Style Metrics:\")\n",
    "    print(f\"  Instruction Following: {scores['instruction_following']:.0%}\")\n",
    "    print(f\"  Factual Accuracy:      {scores['factual_accuracy']:.0%}\")\n",
    "    print(f\"  Coherence:             {scores['coherence']:.0%}\")\n",
    "    print(f\"  Appropriate Length:    {scores['appropriate_length']:.0%}\")\n",
    "    print(f\"  No Hallucination:      {scores['no_hallucination']:.0%}\")\n",
    "    print(f\"\\n  ðŸŽ¯ Overall Score:       {overall:.0%}\")\n",
    "    \n",
    "    return {\"metrics\": scores, \"overall\": overall}\n",
    "\n",
    "# Run InstructGPT-style evaluation\n",
    "instructgpt_metrics = instructgpt_style_evaluation(eval_model, tokenizer, test_queries)\n",
    "print(f\"\\nâœ… InstructGPT-style evaluation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd8717b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… InstructGPT-style results saved to: ../outputs/improved_training/instructgpt_style_results.json\n"
     ]
    }
   ],
   "source": [
    "# Save results with InstructGPT-style metrics\n",
    "results_summary = {\n",
    "    \"methodology\": \"InstructGPT-Style SFT (Ouyang et al., 2022)\",\n",
    "    \"paper_reference\": \"https://arxiv.org/abs/2203.02155\",\n",
    "    \"training\": {\n",
    "        \"samples\": len(train_dataset),\n",
    "        \"epochs\": training_args.num_train_epochs,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"lr_scheduler\": \"cosine\",\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"lora_rank\": 4,\n",
    "        \"kl_weight\": CONFIG[\"regularization\"][\"kl_penalty\"],\n",
    "        \"final_loss\": train_result.training_loss,\n",
    "        \"training_time_minutes\": elapsed / 60,\n",
    "    },\n",
    "    \"evaluation\": {\n",
    "        \"simple_quality_score\": avg_score,\n",
    "        \"instructgpt_metrics\": instructgpt_metrics,\n",
    "        \"test_results\": improved_results,\n",
    "    },\n",
    "    \"paper_alignment\": {\n",
    "        \"cosine_lr_schedule\": True,\n",
    "        \"kl_regularization\": True,\n",
    "        \"curated_dataset\": True,\n",
    "        \"single_template\": True,\n",
    "        \"warmup_steps\": True,\n",
    "    },\n",
    "    \"improvements_applied\": [\n",
    "        \"InstructGPT paper methodology (SFT step)\",\n",
    "        \"Cosine learning rate schedule\",\n",
    "        \"10% warmup ratio\",\n",
    "        \"Curated dataset (3K filtered samples)\",\n",
    "        \"Single template (no gradient conflicts)\",\n",
    "        \"KL regularization (simulates pretraining mix)\",\n",
    "        \"Lower LoRA rank (r=4)\",\n",
    "        \"Higher dropout (0.2)\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_path = Path(f\"{OUTPUT_DIR}/instructgpt_style_results.json\")\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… InstructGPT-style results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a5165c",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Summary: InstructGPT-Style SFT for GPT-2\n",
    "\n",
    "### Paper-Aligned Methodology:\n",
    "Based on **Ouyang et al., 2022** - \"Training language models to follow instructions with human feedback\"\n",
    "\n",
    "| Paper Method | Our Implementation |\n",
    "|--------------|-------------------|\n",
    "| 13K demonstrations | 3K curated samples (scaled for model size) |\n",
    "| GPT-3 (175B) | GPT-2 (124M) |\n",
    "| Cosine LR schedule | âœ… Cosine with 10% warmup |\n",
    "| 16 epochs | 2 epochs (proportional to data size) |\n",
    "| Pretraining data mix | KL regularization (simpler, same effect) |\n",
    "| RLHF (3 steps) | SFT only (Step 1 of 3) |\n",
    "\n",
    "### Key InstructGPT Insights Applied:\n",
    "1. âœ… **Quality over Quantity**: Paper used only 13K samples (we use 3K filtered)\n",
    "2. âœ… **Cosine LR Schedule**: Smooth decay prevents sudden changes\n",
    "3. âœ… **KL Regularization**: Prevents \"alignment tax\" (capability loss)\n",
    "4. âœ… **Consistent Template**: Single format reduces gradient conflicts\n",
    "5. âœ… **LoRA Efficiency**: Paper notes RLHF uses <2% of pretraining compute\n",
    "\n",
    "### Evaluation Axes (from Paper):\n",
    "| Metric | Description | Our Implementation |\n",
    "|--------|-------------|-------------------|\n",
    "| Helpfulness | Follows instructions | Instruction following score |\n",
    "| Truthfulness | Factually accurate | Keyword matching (TruthfulQA-style) |\n",
    "| Harmlessness | Avoids toxicity | Not evaluated (GPT-2 limitation) |\n",
    "| Hallucination | Doesn't make things up | Hallucination detection |\n",
    "\n",
    "### Future Work (Full RLHF):\n",
    "To implement the complete InstructGPT pipeline:\n",
    "1. **Reward Model**: Train a model to predict human preferences\n",
    "2. **PPO Fine-tuning**: Optimize policy using RM as reward signal\n",
    "3. **Human Evaluation**: Collect preference data from labelers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0289a369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸŽ‰ INSTRUCTGPT-STYLE SFT COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Based on: \"Training language models to follow instructions with human feedback\"\n",
      "Paper: https://arxiv.org/abs/2203.02155\n",
      "\n",
      "Training Summary:\n",
      "  - Samples: 3000 (paper: 13K â†’ scaled for GPT-2)\n",
      "  - Epochs: 2 (paper: 16)\n",
      "  - LR Schedule: SchedulerType.COSINE with 10% warmup\n",
      "  - Final loss: 36.1814\n",
      "\n",
      "Evaluation Summary:\n",
      "  - Simple quality score: 0.91\n",
      "  - InstructGPT metrics:\n",
      "    â€¢ Instruction Following: 100%\n",
      "    â€¢ Factual Accuracy: 25%\n",
      "    â€¢ Overall: 78%\n",
      "\n",
      "Model saved to: ../outputs/improved_training/merged\n",
      "\n",
      "Key InstructGPT Insights Applied:\n",
      "  âœ“ Cosine LR schedule (paper methodology)\n",
      "  âœ“ KL regularization (simulates pretraining data mix)\n",
      "  âœ“ Quality-filtered dataset (paper emphasizes data quality)\n",
      "  âœ“ Single template (consistent format)\n",
      "\n",
      "Next Steps (Full RLHF - not implemented):\n",
      "  1. Train Reward Model on human preferences\n",
      "  2. Fine-tune with PPO using RM as reward\n",
      "  3. Mix pretraining data during RL\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Get warmup info (handle both warmup_ratio and warmup_steps)\n",
    "warmup_info = f\"{training_args.warmup_ratio:.0%}\" if training_args.warmup_ratio else f\"{training_args.warmup_steps} steps\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ‰ INSTRUCTGPT-STYLE SFT COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "Based on: \"Training language models to follow instructions with human feedback\"\n",
    "Paper: https://arxiv.org/abs/2203.02155\n",
    "\n",
    "Training Summary:\n",
    "  - Samples: {len(train_dataset)} (paper: 13K â†’ scaled for GPT-2)\n",
    "  - Epochs: {training_args.num_train_epochs} (paper: 16)\n",
    "  - LR Schedule: {training_args.lr_scheduler_type} with {warmup_info} warmup\n",
    "  - Final loss: {train_result.training_loss:.4f}\n",
    "\n",
    "Evaluation Summary:\n",
    "  - Simple quality score: {avg_score:.2f}\n",
    "  - InstructGPT metrics:\n",
    "    â€¢ Instruction Following: {instructgpt_metrics['metrics']['instruction_following']:.0%}\n",
    "    â€¢ Factual Accuracy: {instructgpt_metrics['metrics']['factual_accuracy']:.0%}\n",
    "    â€¢ Overall: {instructgpt_metrics['overall']:.0%}\n",
    "\n",
    "Model saved to: {merged_path}\n",
    "\n",
    "Key InstructGPT Insights Applied:\n",
    "  âœ“ Cosine LR schedule (paper methodology)\n",
    "  âœ“ KL regularization (simulates pretraining data mix)\n",
    "  âœ“ Quality-filtered dataset (paper emphasizes data quality)\n",
    "  âœ“ Single template (consistent format)\n",
    "  \n",
    "Next Steps (Full RLHF - not implemented):\n",
    "  1. Train Reward Model on human preferences\n",
    "  2. Fine-tune with PPO using RM as reward\n",
    "  3. Mix pretraining data during RL\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
