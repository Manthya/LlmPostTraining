{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f40e69",
   "metadata": {},
   "source": [
    "# üöÄ Improved GPT-2 Fine-Tuning\n",
    "\n",
    "This notebook implements all the improvements identified in our analysis:\n",
    "\n",
    "| Issue | Previous | Improved |\n",
    "|-------|----------|----------|\n",
    "| Learning Rate | 1e-5 to 1e-4 | 1e-6 |\n",
    "| Samples | 22-200 | 2000+ |\n",
    "| Templates | 5 different | 1 consistent |\n",
    "| Epochs | 2-3 | 1 |\n",
    "| Regularization | None | KL penalty + data mixing |\n",
    "| LoRA rank | 16 | 4 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da60510",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5572408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.4.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 638, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1971, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3123, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3178, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3641, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3701, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/tg/5d928d1d0z15w2_p1z2f224r0000gr/T/ipykernel_88006/2131112418.py\", line 14, in <module>\n",
      "    import torch\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.2.2\n",
      "Device: CPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# SSL workaround\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "826b3a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Improved Configuration:\n",
      "{\n",
      "  \"data\": {\n",
      "    \"source\": \"alpaca-cleaned (filtered)\",\n",
      "    \"n_samples\": 3000,\n",
      "    \"max_length\": 256,\n",
      "    \"template\": \"Question: {instruction}\\nAnswer:\",\n",
      "    \"filter_criteria\": {\n",
      "      \"max_response_tokens\": 100,\n",
      "      \"task_types\": [\n",
      "        \"qa\",\n",
      "        \"classification\",\n",
      "        \"short_generation\"\n",
      "      ],\n",
      "      \"exclude_complex\": true\n",
      "    }\n",
      "  },\n",
      "  \"training\": {\n",
      "    \"learning_rate\": 1e-06,\n",
      "    \"batch_size\": 8,\n",
      "    \"gradient_accumulation\": 2,\n",
      "    \"epochs\": 1,\n",
      "    \"warmup_steps\": 100,\n",
      "    \"weight_decay\": 0.1,\n",
      "    \"max_grad_norm\": 0.5,\n",
      "    \"lr_scheduler\": \"linear\"\n",
      "  },\n",
      "  \"regularization\": {\n",
      "    \"dropout\": 0.1,\n",
      "    \"data_mixing_ratio\": 0.3,\n",
      "    \"kl_penalty\": 0.1\n",
      "  },\n",
      "  \"lora\": {\n",
      "    \"r\": 4,\n",
      "    \"alpha\": 8,\n",
      "    \"dropout\": 0.2,\n",
      "    \"target_modules\": [\n",
      "      \"c_attn\",\n",
      "      \"c_proj\",\n",
      "      \"mlp.c_fc\",\n",
      "      \"mlp.c_proj\"\n",
      "    ],\n",
      "    \"learning_rate\": 5e-05\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load improved configuration\n",
    "config_path = Path(\"../configs/improved_gpt2_config.json\")\n",
    "with open(config_path) as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "print(\"Loaded Improved Configuration:\")\n",
    "print(json.dumps(CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad601f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer loaded: 50257 tokens\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "BASE_MODEL_PATH = \"../models/gpt2\"\n",
    "OUTPUT_DIR = \"../outputs/improved_training\"\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load tokenizer and base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úÖ Tokenizer loaded: {tokenizer.vocab_size} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dda365",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Dataset Curation (Filtered for GPT-2)\n",
    "\n",
    "Key improvements:\n",
    "- Filter for **simple tasks** (Q&A, classification)\n",
    "- **Short responses** (max 100 tokens)\n",
    "- **Single template** (no gradient conflicts)\n",
    "- **2000+ samples** for adequate learning signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "615f338f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Filter function defined\n"
     ]
    }
   ],
   "source": [
    "def filter_for_gpt2(example: Dict) -> bool:\n",
    "    \"\"\"\n",
    "    Filter dataset for GPT-2's limited capacity.\n",
    "    Only keep simple, short examples.\n",
    "    \"\"\"\n",
    "    instruction = example.get(\"instruction\", \"\")\n",
    "    output = example.get(\"output\", \"\")\n",
    "    inp = example.get(\"input\", \"\")\n",
    "    \n",
    "    # Skip if empty\n",
    "    if not instruction or not output:\n",
    "        return False\n",
    "    \n",
    "    # Skip very long responses (>100 tokens ~400 chars)\n",
    "    if len(output) > 400:\n",
    "        return False\n",
    "    \n",
    "    # Skip very short responses (likely need context)\n",
    "    if len(output) < 10:\n",
    "        return False\n",
    "    \n",
    "    # Skip code-related tasks\n",
    "    code_keywords = [\"code\", \"python\", \"javascript\", \"function\", \"class\", \n",
    "                    \"def \", \"import \", \"```\", \"programming\"]\n",
    "    text = (instruction + output).lower()\n",
    "    if any(kw in text for kw in code_keywords):\n",
    "        return False\n",
    "    \n",
    "    # Skip complex reasoning tasks\n",
    "    complex_keywords = [\"step by step\", \"explain how\", \"analyze\", \"compare and contrast\",\n",
    "                       \"multiple steps\", \"calculate\", \"solve\"]\n",
    "    if any(kw in text for kw in complex_keywords):\n",
    "        return False\n",
    "    \n",
    "    # Skip tasks requiring external context\n",
    "    if len(inp) > 200:  # Large input context\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "print(\"‚úÖ Filter function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a34f820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'tatsu-lab/alpaca' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Alpaca dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52002/52002 [00:00<00:00, 162529.66 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 52002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52002/52002 [00:01<00:00, 37657.75 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset size: 30476\n",
      "\n",
      "‚úÖ Using 3000 curated samples\n"
     ]
    }
   ],
   "source": [
    "# Fix SSL and httpx issues\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import httpx\n",
    "httpx._client.Client.__init__.__globals__['DEFAULT_TIMEOUT_CONFIG'] = httpx.Timeout(30.0)\n",
    "\n",
    "# Monkey-patch httpx to handle SSL\n",
    "original_init = httpx.Client.__init__\n",
    "def patched_init(self, *args, **kwargs):\n",
    "    kwargs.setdefault('verify', False)\n",
    "    original_init(self, *args, **kwargs)\n",
    "httpx.Client.__init__ = patched_init\n",
    "\n",
    "# Load and filter dataset\n",
    "print(\"Loading Alpaca dataset...\")\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", trust_remote_code=True)\n",
    "print(f\"Original dataset size: {len(dataset)}\")\n",
    "\n",
    "# Apply filter\n",
    "filtered_dataset = dataset.filter(filter_for_gpt2)\n",
    "print(f\"Filtered dataset size: {len(filtered_dataset)}\")\n",
    "\n",
    "# Take subset for training (2000-3000 samples as recommended)\n",
    "N_SAMPLES = min(3000, len(filtered_dataset))\n",
    "filtered_dataset = filtered_dataset.shuffle(seed=42).select(range(N_SAMPLES))\n",
    "print(f\"\\n‚úÖ Using {N_SAMPLES} curated samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3068626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Response Length Statistics:\n",
      "  Min: 10 chars\n",
      "  Max: 400 chars\n",
      "  Mean: 143 chars\n",
      "\n",
      "üìù Sample Examples:\n",
      "\n",
      "--- Example 1 ---\n",
      "Instruction: Edit this sentence for grammar, syntax, and style ‚ÄúIt can incredibly difficult to decide‚Äù...\n",
      "Output: It can be incredibly difficult to decide....\n",
      "\n",
      "--- Example 2 ---\n",
      "Instruction: Which is the best way to learn a new language?...\n",
      "Output: The best way to learn a new language is through total immersion. This can be accomplished by traveli...\n",
      "\n",
      "--- Example 3 ---\n",
      "Instruction: List 5 strategies for better organization and time management....\n",
      "Output: 1. Set realistic goals and prioritize tasks.\n",
      "2. Use a calendar to track meetings and deadlines.\n",
      "3. B...\n"
     ]
    }
   ],
   "source": [
    "# Show sample distribution\n",
    "response_lengths = [len(ex[\"output\"]) for ex in filtered_dataset]\n",
    "print(f\"\\nüìä Response Length Statistics:\")\n",
    "print(f\"  Min: {min(response_lengths)} chars\")\n",
    "print(f\"  Max: {max(response_lengths)} chars\")\n",
    "print(f\"  Mean: {sum(response_lengths)/len(response_lengths):.0f} chars\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nüìù Sample Examples:\")\n",
    "for i in range(3):\n",
    "    ex = filtered_dataset[i]\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Instruction: {ex['instruction'][:100]}...\")\n",
    "    print(f\"Output: {ex['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6b5a36",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Single Template Formatting\n",
    "\n",
    "**Critical improvement**: Use ONE consistent template to avoid gradient conflicts.\n",
    "\n",
    "```\n",
    "Question: {instruction}\n",
    "Answer: {output}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0826053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template Preview:\n",
      "==================================================\n",
      "Question: Edit this sentence for grammar, syntax, and style ‚ÄúIt can incredibly difficult to decide‚Äù\n",
      "Answer: It can be incredibly difficult to decide.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Simple, consistent template\n",
    "TEMPLATE = \"\"\"Question: {instruction}\n",
    "Answer: {output}\"\"\"\n",
    "\n",
    "def format_example(example: Dict) -> str:\n",
    "    \"\"\"Format example with single consistent template.\"\"\"\n",
    "    instruction = example[\"instruction\"]\n",
    "    \n",
    "    # Append input if present\n",
    "    if example.get(\"input\"):\n",
    "        instruction = f\"{instruction}\\nContext: {example['input']}\"\n",
    "    \n",
    "    return TEMPLATE.format(\n",
    "        instruction=instruction,\n",
    "        output=example[\"output\"]\n",
    "    )\n",
    "\n",
    "# Test formatting\n",
    "print(\"Template Preview:\")\n",
    "print(\"=\" * 50)\n",
    "print(format_example(filtered_dataset[0]))\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "827da9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training dataset created: 3000 samples\n"
     ]
    }
   ],
   "source": [
    "class GPT2Dataset(Dataset):\n",
    "    \"\"\"Custom dataset for GPT-2 fine-tuning.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length=256):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = format_example(self.data[idx])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding[\"input_ids\"].squeeze()\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": input_ids.clone()\n",
    "        }\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = GPT2Dataset(filtered_dataset, tokenizer, max_length=256)\n",
    "print(f\"‚úÖ Training dataset created: {len(train_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e974b6b",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. KL Regularization Implementation\n",
    "\n",
    "To prevent catastrophic forgetting, we add a KL divergence penalty:\n",
    "\n",
    "$$\\mathcal{L}_{total} = \\mathcal{L}_{task} + \\beta \\cdot KL(P_{new} || P_{original})$$\n",
    "\n",
    "This keeps the new model's output distribution close to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7042021d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ KLRegularizedTrainer defined\n"
     ]
    }
   ],
   "source": [
    "class KLRegularizedTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom trainer with KL divergence regularization.\n",
    "    Keeps model close to original distribution to prevent forgetting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, reference_model=None, kl_weight=0.1, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.reference_model = reference_model\n",
    "        self.kl_weight = kl_weight\n",
    "        \n",
    "        # Freeze reference model\n",
    "        if self.reference_model is not None:\n",
    "            self.reference_model.eval()\n",
    "            for param in self.reference_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(f\"‚úÖ Reference model frozen for KL regularization (Œ≤={kl_weight})\")\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Standard forward pass\n",
    "        outputs = model(**inputs)\n",
    "        task_loss = outputs.loss\n",
    "        \n",
    "        # Add KL regularization if reference model exists\n",
    "        if self.reference_model is not None and self.kl_weight > 0:\n",
    "            with torch.no_grad():\n",
    "                ref_outputs = self.reference_model(**inputs)\n",
    "                ref_logits = ref_outputs.logits\n",
    "            \n",
    "            # Compute KL divergence\n",
    "            new_probs = F.log_softmax(outputs.logits, dim=-1)\n",
    "            ref_probs = F.softmax(ref_logits, dim=-1)\n",
    "            \n",
    "            kl_loss = F.kl_div(new_probs, ref_probs, reduction=\"batchmean\")\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = task_loss + self.kl_weight * kl_loss\n",
    "        else:\n",
    "            total_loss = task_loss\n",
    "        \n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n",
    "\n",
    "print(\"‚úÖ KLRegularizedTrainer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52a36f9",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Setup with Improved LoRA\n",
    "\n",
    "LoRA improvements:\n",
    "- **Lower rank** (r=4 vs 16) - less overfitting\n",
    "- **More target modules** - include MLP layers\n",
    "- **Higher dropout** - better regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efced59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 148/148 [00:00<00:00, 190.83it/s, Materializing param=transformer.wte.weight]             \n",
      "GPT2LMHeadModel LOAD REPORT from: ../models/gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 148/148 [00:00<00:00, 325.91it/s, Materializing param=transformer.wte.weight]             \n",
      "GPT2LMHeadModel LOAD REPORT from: ../models/gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Models loaded\n",
      "  Trainable model params: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "# Load base model (for training)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    torch_dtype=torch.float32,\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Load reference model (frozen, for KL regularization)\n",
    "reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    torch_dtype=torch.float32,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Models loaded\")\n",
    "print(f\"  Trainable model params: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f530952e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/Users/manthan-kamble/Documents/GitHub/LlmPostTraining/.venv/lib/python3.12/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "\n",
      "üìä Parameter Summary:\n",
      "  Total params: 125,029,632\n",
      "  Trainable params: 589,824\n",
      "  Trainable %: 0.47%\n"
     ]
    }
   ],
   "source": [
    "# Improved LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=4,                    # Lower rank (was 16)\n",
    "    lora_alpha=8,           # Lower alpha (was 32)\n",
    "    lora_dropout=0.2,       # Higher dropout (was 0.1)\n",
    "    target_modules=[        # Include MLP layers\n",
    "        \"c_attn\",\n",
    "        \"c_proj\",\n",
    "        \"c_fc\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nüìä Parameter Summary:\")\n",
    "print(f\"  Total params: {total_params:,}\")\n",
    "print(f\"  Trainable params: {trainable_params:,}\")\n",
    "print(f\"  Trainable %: {100 * trainable_params / total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035ea128",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Improved Training Configuration\n",
    "\n",
    "Key changes:\n",
    "- **LR: 1e-6** (100x lower than before)\n",
    "- **1 epoch** (single pass to prevent overfitting)\n",
    "- **Higher weight decay** (0.1 for regularization)\n",
    "- **Lower gradient clipping** (0.5 for stability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43349952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training arguments configured\n",
      "  Learning rate: 1e-06\n",
      "  Epochs: 1\n",
      "  Effective batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# Improved training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Key improvements\n",
    "    learning_rate=1e-6,          # 100x lower!\n",
    "    num_train_epochs=1,          # Single epoch\n",
    "    weight_decay=0.1,            # Higher regularization\n",
    "    max_grad_norm=0.5,           # Lower gradient clipping\n",
    "    \n",
    "    # Batch settings\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Effective batch = 16\n",
    "    \n",
    "    # LR schedule\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    \n",
    "    # Disable evaluation (CPU training)\n",
    "    eval_strategy=\"no\",\n",
    "    \n",
    "    # CPU settings\n",
    "    use_cpu=True,\n",
    "    fp16=False,\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54cbb45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Training Estimates:\n",
      "  Samples: 3000\n",
      "  Steps per epoch: 187\n",
      "  Total steps: 187\n",
      "  Estimated time: ~0 minutes\n"
     ]
    }
   ],
   "source": [
    "# Calculate training estimates\n",
    "n_samples = len(train_dataset)\n",
    "batch_size = training_args.per_device_train_batch_size\n",
    "grad_accum = training_args.gradient_accumulation_steps\n",
    "epochs = training_args.num_train_epochs\n",
    "\n",
    "steps_per_epoch = n_samples // (batch_size * grad_accum)\n",
    "total_steps = steps_per_epoch * epochs\n",
    "\n",
    "# Estimate time (based on previous runs: ~0.15s per step on CPU)\n",
    "est_time_min = total_steps * 0.15 / 60\n",
    "\n",
    "print(f\"\\nüìä Training Estimates:\")\n",
    "print(f\"  Samples: {n_samples}\")\n",
    "print(f\"  Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Estimated time: ~{est_time_min:.0f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a02788",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Training with KL Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "426eb1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reference model frozen for KL regularization (Œ≤=0.1)\n",
      "\n",
      "============================================================\n",
      "üöÄ STARTING IMPROVED TRAINING\n",
      "============================================================\n",
      "\n",
      "Improvements Applied:\n",
      "  ‚úì Curated dataset: 3000 samples\n",
      "  ‚úì Single template (no gradient conflicts)\n",
      "  ‚úì Low LR: 1e-06\n",
      "  ‚úì 1 epoch (prevent overfitting)\n",
      "  ‚úì KL regularization: Œ≤=0.1\n",
      "  ‚úì LoRA r=4 (lower rank)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create KL-regularized trainer\n",
    "trainer = KLRegularizedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    reference_model=reference_model,\n",
    "    kl_weight=CONFIG[\"regularization\"][\"kl_penalty\"],  # 0.1 from config\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ STARTING IMPROVED TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nImprovements Applied:\")\n",
    "print(f\"  ‚úì Curated dataset: {len(train_dataset)} samples\")\n",
    "print(f\"  ‚úì Single template (no gradient conflicts)\")\n",
    "print(f\"  ‚úì Low LR: {training_args.learning_rate}\")\n",
    "print(f\"  ‚úì 1 epoch (prevent overfitting)\")\n",
    "print(f\"  ‚úì KL regularization: Œ≤={CONFIG['regularization']['kl_penalty']}\")\n",
    "print(f\"  ‚úì LoRA r=4 (lower rank)\")\n",
    "print(f\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc258e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 17/188 13:42 < 2:36:16, 0.02 it/s, Epoch 0.09/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start training\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"  Time: {elapsed/60:.1f} minutes\")\n",
    "print(f\"  Final loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9699f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "adapter_path = f\"{OUTPUT_DIR}/adapter\"\n",
    "model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "print(f\"‚úÖ Adapter saved to: {adapter_path}\")\n",
    "\n",
    "# Save merged model\n",
    "merged_path = f\"{OUTPUT_DIR}/merged\"\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(merged_path)\n",
    "tokenizer.save_pretrained(merged_path)\n",
    "print(f\"‚úÖ Merged model saved to: {merged_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c62703",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec4290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load improved model for evaluation\n",
    "eval_model = AutoModelForCausalLM.from_pretrained(\n",
    "    merged_path,\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "eval_model.eval()\n",
    "\n",
    "print(\"‚úÖ Model loaded for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178e3b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_new_tokens=100):\n",
    "    \"\"\"Generate response with same format as training.\"\"\"\n",
    "    # Format prompt like training data\n",
    "    formatted = f\"Question: {prompt}\\nAnswer:\"\n",
    "    \n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract answer part\n",
    "    if \"Answer:\" in response:\n",
    "        response = response.split(\"Answer:\")[1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"‚úÖ Generation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7440714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries (same as evaluation notebook)\n",
    "test_queries = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Who wrote Romeo and Juliet?\",\n",
    "    \"What is 2 + 2?\",\n",
    "    \"What color is the sky?\",\n",
    "    \"Name a fruit that is red.\",\n",
    "    \"What is the largest planet in our solar system?\",\n",
    "    \"What language do people speak in Spain?\",\n",
    "    \"Is water wet?\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPROVED MODEL RESPONSES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "improved_results = []\n",
    "for query in test_queries:\n",
    "    response = generate_response(eval_model, tokenizer, query)\n",
    "    improved_results.append({\"query\": query, \"response\": response})\n",
    "    \n",
    "    print(f\"\\nQ: {query}\")\n",
    "    print(f\"A: {response[:200]}{'...' if len(response) > 200 else ''}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917e02d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with previous stages\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON WITH PREVIOUS STAGES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load Stage 2 (worst performer) for comparison\n",
    "stage2_path = \"../outputs/stage2_instruction/model\"\n",
    "if Path(stage2_path).exists():\n",
    "    stage2_model = AutoModelForCausalLM.from_pretrained(stage2_path, torch_dtype=torch.float32)\n",
    "    stage2_model.eval()\n",
    "    \n",
    "    # Compare on same queries\n",
    "    for i, query in enumerate(test_queries[:3]):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Stage 2 response\n",
    "        s2_resp = generate_response(stage2_model, tokenizer, query)\n",
    "        print(f\"\\nüìõ Stage 2 (before): {s2_resp[:150]}...\")\n",
    "        \n",
    "        # Improved response\n",
    "        print(f\"\\n‚úÖ Improved (after): {improved_results[i]['response'][:150]}...\")\n",
    "    \n",
    "    del stage2_model\n",
    "else:\n",
    "    print(\"Stage 2 model not found for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd3b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality metrics\n",
    "def simple_quality_check(response):\n",
    "    \"\"\"Basic quality heuristics.\"\"\"\n",
    "    # Check for repetition (degenerate output)\n",
    "    words = response.lower().split()\n",
    "    if len(words) > 5:\n",
    "        unique_ratio = len(set(words)) / len(words)\n",
    "    else:\n",
    "        unique_ratio = 1.0\n",
    "    \n",
    "    # Check response length (too short = bad, too long = bad)\n",
    "    length_ok = 5 < len(words) < 150\n",
    "    \n",
    "    # Check for common degenerate patterns\n",
    "    degenerate_patterns = [\n",
    "        \"the the the\",\n",
    "        \"is is is\",\n",
    "        \"and and and\",\n",
    "        \"\\n\\n\\n\\n\",\n",
    "    ]\n",
    "    has_degenerate = any(p in response.lower() for p in degenerate_patterns)\n",
    "    \n",
    "    return {\n",
    "        \"unique_ratio\": unique_ratio,\n",
    "        \"length_ok\": length_ok,\n",
    "        \"no_degenerate\": not has_degenerate,\n",
    "        \"quality_score\": unique_ratio * (1 if length_ok else 0.5) * (1 if not has_degenerate else 0.1)\n",
    "    }\n",
    "\n",
    "# Evaluate all responses\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUALITY METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "scores = []\n",
    "for result in improved_results:\n",
    "    metrics = simple_quality_check(result[\"response\"])\n",
    "    scores.append(metrics[\"quality_score\"])\n",
    "    \n",
    "avg_score = sum(scores) / len(scores)\n",
    "print(f\"\\nAverage Quality Score: {avg_score:.2f}\")\n",
    "print(f\"  (1.0 = perfect, 0.1 = degenerate)\")\n",
    "print(f\"\\nPer-query scores:\")\n",
    "for i, (result, score) in enumerate(zip(improved_results, scores)):\n",
    "    status = \"‚úÖ\" if score > 0.5 else \"‚ö†Ô∏è\" if score > 0.2 else \"‚ùå\"\n",
    "    print(f\"  {i+1}. {status} Score: {score:.2f} - {result['query'][:40]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8717b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_summary = {\n",
    "    \"training\": {\n",
    "        \"samples\": len(train_dataset),\n",
    "        \"epochs\": 1,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"lora_rank\": 4,\n",
    "        \"kl_weight\": CONFIG[\"regularization\"][\"kl_penalty\"],\n",
    "        \"final_loss\": train_result.training_loss,\n",
    "        \"training_time_minutes\": elapsed / 60,\n",
    "    },\n",
    "    \"evaluation\": {\n",
    "        \"average_quality_score\": avg_score,\n",
    "        \"results\": improved_results,\n",
    "    },\n",
    "    \"improvements_applied\": [\n",
    "        \"Curated dataset (filtered for simple tasks)\",\n",
    "        \"Single template (no gradient conflicts)\",\n",
    "        \"100x lower learning rate\",\n",
    "        \"Single epoch training\",\n",
    "        \"KL regularization against original model\",\n",
    "        \"Lower LoRA rank (r=4)\",\n",
    "        \"Higher dropout (0.2)\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_path = Path(f\"{OUTPUT_DIR}/improved_results.json\")\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a5165c",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Summary\n",
    "\n",
    "### Improvements Applied:\n",
    "1. ‚úÖ **Curated Dataset**: 2000+ samples filtered for simple tasks\n",
    "2. ‚úÖ **Single Template**: Eliminated gradient conflicts\n",
    "3. ‚úÖ **Low Learning Rate**: 1e-6 (100x lower)\n",
    "4. ‚úÖ **Single Epoch**: Prevent overfitting\n",
    "5. ‚úÖ **KL Regularization**: Keep model close to original\n",
    "6. ‚úÖ **Lower LoRA Rank**: r=4 to reduce overfitting\n",
    "7. ‚úÖ **Higher Dropout**: 0.2 for regularization\n",
    "\n",
    "### Expected vs Achieved:\n",
    "| Metric | Previous Stages | Expected | Achieved |\n",
    "|--------|-----------------|----------|----------|\n",
    "| Coherent responses | ~5% | 40-60% | TBD |\n",
    "| No repetition | ~20% | 80%+ | TBD |\n",
    "| Quality score | ~0.1 | 0.5+ | See above |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0289a369",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ IMPROVED TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "Summary:\n",
    "  - Training samples: {len(train_dataset)}\n",
    "  - Final loss: {train_result.training_loss:.4f}\n",
    "  - Average quality score: {avg_score:.2f}\n",
    "  - Model saved to: {merged_path}\n",
    "\n",
    "Key takeaways:\n",
    "  1. Lower LR + fewer epochs prevents catastrophic forgetting\n",
    "  2. Single template eliminates gradient conflicts\n",
    "  3. KL regularization keeps model grounded\n",
    "  4. GPT-2 works best for SIMPLE tasks\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
