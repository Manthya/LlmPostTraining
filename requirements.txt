# LLM Post-Training Dependencies
# ================================

# Core ML Libraries
torch>=2.0.0
transformers>=4.36.0
datasets>=2.14.0
accelerate>=0.25.0

# Parameter-Efficient Fine-Tuning
peft>=0.7.0

# Quantization
bitsandbytes>=0.41.0

# Training
trl>=0.7.0

# Utilities
pyyaml>=6.0
pandas>=2.0.0
numpy>=1.24.0

# Jupyter
jupyter>=1.0.0
ipykernel>=6.0.0

# Optional: Experiment Tracking
# wandb>=0.15.0

# Optional: Better Progress Bars
tqdm>=4.65.0
