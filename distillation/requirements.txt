# Requirements for Reasoning Distillation Pipeline
# Following DeepSeek-R1-Distill methodology

# Core ML libraries
torch>=2.0.0
transformers>=4.36.0
datasets>=2.16.0
accelerate>=0.25.0

# PEFT for efficient fine-tuning
peft>=0.7.0
bitsandbytes>=0.41.0  # For quantization support

# Training utilities
trl>=0.7.0
sentencepiece>=0.1.99
tokenizers>=0.15.0

# Evaluation
evaluate>=0.4.0
scikit-learn>=1.3.0

# Utilities
tqdm>=4.66.0
numpy>=1.24.0
pandas>=2.0.0

# Logging and config
pyyaml>=6.0
jsonlines>=4.0.0
